{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting pickled `exactly` hits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "EXACTLY_OUT = Path('/share/compling/projects/sanpi/notebooks/exactly_out')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Define~~ Import helper `utils`\n",
    "\n",
    "~~Copied from `./source/analyze/utils/`~~\\\n",
    "~~jupyter won't import them 🤷‍♀️~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.utils.dataframes import (balance_sample, cols_by_str, concat_pkls,\n",
    "                                     make_cats, optimize_hit_df, show_counts,\n",
    "                                     summarize_text_cols, corners)\n",
    "from source.utils.general import find_files, print_iter\n",
    "from source.utils.sample import sample_pickle\n",
    "from source.utils.visualize import heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_files(data_dir: Path, fname_glob: str, verbose: bool = False):\n",
    "    # path_iter = data_dir.rglob(fname_glob)\n",
    "    # if verbose:\n",
    "    #     path_iter = tuple(path_iter)\n",
    "    #     print_iter(\n",
    "    #         [f'../{p.relative_to(data_dir)}' for p in path_iter], bullet='-',\n",
    "    #         header=f'### {len(path_iter)} paths matching {fname_glob} found in {data_dir}')\n",
    "    # return path_iter\n",
    "\n",
    "\n",
    "# def print_iter(iter_obj,\n",
    "    #            bullet: str = '▸',\n",
    "    #            # //    logger: logging.Logger = None,\n",
    "    #            # //    level: int = 20,\n",
    "    #            header: str = ''):\n",
    "\n",
    "    # bullet_str = f'\\n{bullet} '\n",
    "\n",
    "    # iter_str = bullet_str.join(f'{i}' for i in iter_obj)\n",
    "\n",
    "    # msg_str = f'\\n{header}{bullet_str}{iter_str}'\n",
    "    # msg_str = msg_str.replace('\\n\\n', '\\n').strip(f'{bullet} ')\n",
    "\n",
    "    # print(msg_str)\n",
    "\n",
    "\n",
    "# def cols_by_str(df: pd.DataFrame, start_str=None, end_str=None) -> list:\n",
    "    # if end_str:\n",
    "    #     cols = df.columns[df.columns.str.endswith(end_str)]\n",
    "    #     if start_str:\n",
    "    #         cols = cols[cols.str.startswith(start_str)]\n",
    "    # elif start_str:\n",
    "    #     cols = df.columns[df.columns.str.startswith(start_str)]\n",
    "    # else:\n",
    "    #     cols = df.columns\n",
    "\n",
    "    # return cols.to_list()\n",
    "\n",
    "\n",
    "# def make_cats(orig_df: pd.DataFrame, columns: list = None) -> pd.DataFrame:\n",
    "    # df = orig_df.copy()\n",
    "    # if columns is None:\n",
    "    #     cat_suff = (\"code\", \"name\", \"path\", \"stem\")\n",
    "    #     columns = df.columns.str.endswith(cat_suff)  # type: ignore\n",
    "    # df.loc[:, columns] = df.loc[:, columns].astype(\n",
    "    #     'string').fillna('_').astype('category')\n",
    "\n",
    "    # return df\n",
    "\n",
    "\n",
    "# def balance_sample(full_df: pd.DataFrame,\n",
    "    #                column_name: str = 'category',\n",
    "    #                sample_per_value: int = 5,\n",
    "    #                verbose: bool = False) -> tuple:\n",
    "    # '''\n",
    "    # create sample with no more than n rows satisfying each unique value\n",
    "    # of the given column. A value of -1 for `sample_per_value` will limit\n",
    "    # all values' results to the minimum count per value.\n",
    "    # '''\n",
    "    # info_message = ''\n",
    "    # subsamples = []\n",
    "    # for __, col_val_df in full_df.groupby(column_name):\n",
    "    #     # take sample if 1+ and less than length of full dataframe\n",
    "    #     if len(col_val_df) > sample_per_value > 0:\n",
    "    #         subsample_df = col_val_df.sample(sample_per_value)\n",
    "    #         subsamples.append(subsample_df)\n",
    "    #     else:\n",
    "    #         subsamples.append(col_val_df)\n",
    "\n",
    "    # # > trim all \"by column\" sub dfs to length of shortest if -1 given\n",
    "    # if sample_per_value == -1:\n",
    "    #     trim_len = int(min(len(sdf) for sdf in subsamples))\n",
    "    #     subsamples = [sdf.sample(trim_len)\n",
    "    #                   for sdf in subsamples]\n",
    "\n",
    "    # b_sample = pd.concat(subsamples)\n",
    "\n",
    "    # if verbose:\n",
    "    #     subset_info_table = (\n",
    "    #         b_sample\n",
    "    #         .value_counts(subset=column_name)\n",
    "    #         .to_frame(name='count')\n",
    "    #         .assign(percentage=b_sample\n",
    "    #                 .value_counts(column_name, normalize=True)\n",
    "    #                 .round(2) * 100)\n",
    "    #         .to_markdown())\n",
    "    #     label = (full_df.hits_df_pkl[0].stem + ' '\n",
    "    #              if 'hits_df_pkl' in full_df.columns\n",
    "    #              else '')\n",
    "    #     info_message = (f'\\n## {column_name} representation in {label}sample\\n'\n",
    "    #                     + subset_info_table)\n",
    "\n",
    "    # return b_sample, info_message\n",
    "\n",
    "\n",
    "# def concat_pkls(data_dir: Path = Path('/share/compling/data/sanpi/2_hit_tables'),\n",
    "    #             fname_glob: str = '*.pkl.gz',\n",
    "    #             pickles=None,\n",
    "    #             convert_dtypes=False,\n",
    "    #             verbose: bool = True) -> pd.DataFrame:\n",
    "    # if not pickles:\n",
    "    #     pickles = find_files(Path(data_dir), fname_glob, verbose)\n",
    "\n",
    "    # # tested and found that it is faster to assign `corpus` intermittently\n",
    "    # df = pd.concat((pd.read_pickle(p).assign(corpus=p.stem.rsplit('_', 2)[0])\n",
    "    #                 for p in pickles))\n",
    "\n",
    "    # dup_check_cols = cols_by_str(df, end_str=('text', 'id', 'sent'))\n",
    "    # df = (df.loc[~df.duplicated(subset=dup_check_cols), :])\n",
    "    # df = df.convert_dtypes()\n",
    "    # df = make_cats(df, (['corpus'] + cols_by_str(df, start_str=('nr', 'neg', 'adv'),\n",
    "    #                                              end_str=('lemma', 'form'))))\n",
    "\n",
    "    # return df\n",
    "# def _optimize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # # print('Original Dataframe:')\n",
    "    # # df.info(memory_usage='deep')\n",
    "\n",
    "    # # > drop unneeded string columns\n",
    "    # # was:\n",
    "    # #   for c in udf.cols_by_str(df, start_str=('context', 'text', 'sent_text', 'token')):\n",
    "    # for c in cols_by_str(df, start_str=('context', 'token', 'utt')):\n",
    "    #     df.pop(c)\n",
    "\n",
    "    # # > select only non-`object` dtype columns\n",
    "    # relevant_cols = df.columns[~df.dtypes.astype(\n",
    "    #     'string').str.endswith(('object'))]\n",
    "    # # limit df to `relevant_cols`\n",
    "    # df = df[relevant_cols]\n",
    "\n",
    "    # # > create empty dataframe with `relevant_cols` as index/rows\n",
    "    # df_info = pd.DataFrame(index=relevant_cols)\n",
    "\n",
    "    # df_info = df_info.assign(\n",
    "    #     mem0=df.memory_usage(deep=True),\n",
    "    #     dtype0=df.dtypes.astype('string'),\n",
    "    #     defined_values=df.count(),\n",
    "    #     unique_values=df.apply(pd.unique, axis=0).apply(len))\n",
    "    # df_info = df_info.assign(\n",
    "    #     ratio_unique=(df_info.unique_values/df_info.defined_values).round(2))\n",
    "\n",
    "    # cat_candidates = df_info.loc[df_info.ratio_unique < 0.8,\n",
    "    #                              :].loc[df_info.dtype0 != 'category'].index.to_list()\n",
    "    # #was: catted_df = udf.make_cats(df.copy(), cat_candidates)\n",
    "    # catted_df = make_cats(df.copy(), cat_candidates)\n",
    "\n",
    "    # df_info = df_info.assign(dtype1=catted_df.dtypes,\n",
    "    #                          mem1=catted_df.memory_usage(deep=True))\n",
    "    # df_info = df_info.assign(mem_change=df_info.mem1-df_info.mem0)\n",
    "    # print(df_info.sort_values(\n",
    "    #     ['mem_change', 'ratio_unique', 'dtype0']).to_markdown())\n",
    "    # mem_improved = df_info.loc[df_info.mem_change < 0, :].index.to_list()\n",
    "    # for c in df.columns[~df.columns.isin(mem_improved)]:\n",
    "    #     print(c, '\\t', df.loc[:, c].dtype)\n",
    "    # df.loc[:, mem_improved] = catted_df.loc[:, mem_improved]\n",
    "    # print('Category Converted dataframe:')\n",
    "    # df.info(memory_usage='deep')\n",
    "\n",
    "    # return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Newly created~~ Moved to [`source.utils.dataframes`](source/utils/dataframes.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * moved to to utils.dataframes\n",
    "# def show_counts(df, columns):\n",
    "    # return df.value_counts(columns).to_frame().rename(columns={0: 'count'})\n",
    "\n",
    "\n",
    "# def summarize_text_cols(tdf: pd.DataFrame):\n",
    "\n",
    "    # summary = tdf.describe().transpose()\n",
    "    # summary = summary.assign(top_percent=(\n",
    "    #     ((pd.to_numeric(summary.freq) / len(tdf)))*100).round(2))\n",
    "    # summary = summary.rename(columns={'top': 'top_value', 'freq': 'top_freq'})\n",
    "\n",
    "    # return summary.convert_dtypes().sort_values('unique')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 12 paths matching exactly*hits+deps.pkl.gz found in /share/compling/data/sanpi/3_dep_info\n",
      "- ../raised/exactly_apw_neg-raised_hits+deps.pkl.gz\n",
      "- ../raised/exactly_nyt_neg-raised_hits+deps.pkl.gz\n",
      "- ../raised/exactly_puddin_neg-raised_hits+deps.pkl.gz\n",
      "- ../scoped/exactly_apw_with-relay_hits+deps.pkl.gz\n",
      "- ../scoped/exactly_nyt_with-relay_hits+deps.pkl.gz\n",
      "- ../scoped/exactly_puddin_with-relay_hits+deps.pkl.gz\n",
      "- ../contig/exactly_apw_sans-relay_hits+deps.pkl.gz\n",
      "- ../contig/exactly_nyt_sans-relay_hits+deps.pkl.gz\n",
      "- ../contig/exactly_puddin_sans-relay_hits+deps.pkl.gz\n",
      "- ../advadj/exactly_apw_all-RB-JJs_hits+deps.pkl.gz\n",
      "- ../advadj/exactly_nyt_all-RB-JJs_hits+deps.pkl.gz\n",
      "- ../advadj/exactly_puddin_all-RB-JJs_hits+deps.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "ddf = concat_pkls(\n",
    "    data_dir=Path('/share/compling/data/sanpi/3_dep_info'),\n",
    "    fname_glob='exactly*hits+deps.pkl.gz',\n",
    "    convert_dtypes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150437 entries, apw_eng_20030918_0697_20:4-5-8-9 to pcc_eng_09_047.0803_x0745587_19:3-4\n",
      "Data columns (total 43 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   colloc             150437 non-null  string  \n",
      " 1   sent_text          150437 non-null  string  \n",
      " 2   nr_form            457 non-null     category\n",
      " 3   neg_form           51496 non-null   category\n",
      " 4   adv_form           150437 non-null  category\n",
      " 5   adj_form           150437 non-null  string  \n",
      " 6   hit_text           150437 non-null  string  \n",
      " 7   text_window        150437 non-null  string  \n",
      " 8   sent_id            150437 non-null  string  \n",
      " 9   match_id           150437 non-null  string  \n",
      " 10  colloc_id          150437 non-null  string  \n",
      " 11  token_str          150437 non-null  string  \n",
      " 12  lemma_str          150437 non-null  string  \n",
      " 13  context_prev_id    150437 non-null  string  \n",
      " 14  context_prev_sent  150437 non-null  string  \n",
      " 15  context_next_id    150437 non-null  string  \n",
      " 16  context_next_sent  150437 non-null  string  \n",
      " 17  nr_lemma           457 non-null     category\n",
      " 18  neg_lemma          51496 non-null   category\n",
      " 19  adv_lemma          150437 non-null  category\n",
      " 20  adj_lemma          150437 non-null  string  \n",
      " 21  nr_index           457 non-null     UInt16  \n",
      " 22  neg_index          51496 non-null   UInt16  \n",
      " 23  adv_index          150437 non-null  UInt16  \n",
      " 24  adj_index          150437 non-null  UInt16  \n",
      " 25  dep_negraise       457 non-null     object  \n",
      " 26  dep_neg            51496 non-null   object  \n",
      " 27  dep_mod            150437 non-null  object  \n",
      " 28  json_source        150437 non-null  object  \n",
      " 29  utt_len            150437 non-null  UInt16  \n",
      " 30  category           150437 non-null  string  \n",
      " 31  hits_df_path       150437 non-null  object  \n",
      " 32  dep_str            150437 non-null  string  \n",
      " 33  dep_str_ix         150437 non-null  string  \n",
      " 34  dep_str_full       150437 non-null  string  \n",
      " 35  dep_str_rel        150437 non-null  string  \n",
      " 36  dep_str_mask       150437 non-null  string  \n",
      " 37  dep_str_mask_rel   150437 non-null  string  \n",
      " 38  corpus             150437 non-null  category\n",
      " 39  relay_form         2891 non-null    string  \n",
      " 40  relay_lemma        2891 non-null    string  \n",
      " 41  relay_index        2891 non-null    UInt8   \n",
      " 42  dep_relay          2891 non-null    object  \n",
      "dtypes: UInt16(5), UInt8(1), category(7), object(6), string(24)\n",
      "memory usage: 434.7 MB\n",
      "Original Dataframe Minus Superfluous Columns:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150437 entries, apw_eng_20030918_0697_20:4-5-8-9 to pcc_eng_09_047.0803_x0745587_19:3-4\n",
      "Data columns (total 35 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   colloc            150437 non-null  string  \n",
      " 1   nr_form           457 non-null     category\n",
      " 2   neg_form          51496 non-null   category\n",
      " 3   adv_form          150437 non-null  category\n",
      " 4   adj_form          150437 non-null  string  \n",
      " 5   hit_text          150437 non-null  string  \n",
      " 6   sent_id           150437 non-null  string  \n",
      " 7   match_id          150437 non-null  string  \n",
      " 8   colloc_id         150437 non-null  string  \n",
      " 9   lemma_str         150437 non-null  string  \n",
      " 10  nr_lemma          457 non-null     category\n",
      " 11  neg_lemma         51496 non-null   category\n",
      " 12  adv_lemma         150437 non-null  category\n",
      " 13  adj_lemma         150437 non-null  string  \n",
      " 14  nr_index          457 non-null     UInt16  \n",
      " 15  neg_index         51496 non-null   UInt16  \n",
      " 16  adv_index         150437 non-null  UInt16  \n",
      " 17  adj_index         150437 non-null  UInt16  \n",
      " 18  dep_negraise      457 non-null     object  \n",
      " 19  dep_neg           51496 non-null   object  \n",
      " 20  dep_mod           150437 non-null  object  \n",
      " 21  utt_len           150437 non-null  UInt16  \n",
      " 22  category          150437 non-null  string  \n",
      " 23  hits_df_path      150437 non-null  object  \n",
      " 24  dep_str           150437 non-null  string  \n",
      " 25  dep_str_ix        150437 non-null  string  \n",
      " 26  dep_str_full      150437 non-null  string  \n",
      " 27  dep_str_rel       150437 non-null  string  \n",
      " 28  dep_str_mask      150437 non-null  string  \n",
      " 29  dep_str_mask_rel  150437 non-null  string  \n",
      " 30  corpus            150437 non-null  category\n",
      " 31  relay_form        2891 non-null    string  \n",
      " 32  relay_lemma       2891 non-null    string  \n",
      " 33  relay_index       2891 non-null    UInt8   \n",
      " 34  dep_relay         2891 non-null    object  \n",
      "dtypes: UInt16(5), UInt8(1), category(7), object(5), string(17)\n",
      "memory usage: 281.6 MB\n",
      "\n",
      "Memory Usage Comparison\n",
      "\n",
      "|                  |          mem0 | dtype0   |   defined_values |   unique_values |   ratio_unique | dtype1   |          mem1 |     mem_change |\n",
      "|:-----------------|--------------:|:---------|-----------------:|----------------:|---------------:|:---------|--------------:|---------------:|\n",
      "| lemma_str        | 29,759,457.00 | string   |    150,437.00    |    78,190.00    |           0.52 | category | 17,540,971.00 | -12,218,486.00 |\n",
      "| dep_str_mask_rel | 11,599,471.00 | string   |    150,437.00    |          291.00 |           0.00 | category |    337,911.00 | -11,261,560.00 |\n",
      "| dep_str_rel      | 14,388,422.00 | string   |    150,437.00    |    26,315.00    |           0.17 | category |  3,891,252.00 | -10,497,170.00 |\n",
      "| dep_str_mask     |  9,774,606.00 | string   |    150,437.00    |          190.00 |           0.00 | category |    319,579.00 |  -9,455,027.00 |\n",
      "| category         |  9,477,531.00 | string   |    150,437.00    |            4.00 |           0.00 | category |    150,861.00 |  -9,326,670.00 |\n",
      "| dep_str          | 12,563,557.00 | string   |    150,437.00    |    25,541.00    |           0.17 | category |  3,509,565.00 |  -9,053,992.00 |\n",
      "| match_id         |  9,338,334.00 | string   |    150,437.00    |     1,233.00    |           0.01 | category |    415,156.00 |  -8,923,178.00 |\n",
      "| adj_lemma        |  9,517,494.00 | string   |    150,437.00    |     6,134.00    |           0.04 | category |    835,914.00 |  -8,681,580.00 |\n",
      "| adj_form         |  9,520,638.00 | string   |    150,437.00    |     6,395.00    |           0.04 | category |    984,752.00 |  -8,535,886.00 |\n",
      "| colloc           | 10,659,162.00 | string   |    150,437.00    |    19,746.00    |           0.13 | category |  2,252,302.00 |  -8,406,860.00 |\n",
      "| hit_text         | 10,964,161.00 | string   |    150,437.00    |    29,493.00    |           0.20 | category |  3,581,143.00 |  -7,383,018.00 |\n",
      "| dep_str_full     | 15,620,880.00 | string   |    150,437.00    |    64,474.00    |           0.43 | category |  9,461,404.00 |  -6,159,476.00 |\n",
      "| relay_lemma      |  6,080,996.00 | string   |      2,891.00    |          650.00 |           0.22 | category |    358,424.00 |  -5,722,572.00 |\n",
      "| relay_form       |  6,082,598.00 | string   |      2,891.00    |          706.00 |           0.24 | category |    362,494.00 |  -5,720,104.00 |\n",
      "| dep_str_ix       | 13,796,015.00 | string   |    150,437.00    |    63,392.00    |           0.42 | category |  8,573,477.00 |  -5,222,538.00 |\n",
      "| sent_id          | 13,185,809.00 | string   |    150,437.00    |    85,959.00    |           0.57 | category | 10,249,989.00 |  -2,935,820.00 |\n",
      "| colloc_id        | 13,959,965.00 | string   |    150,437.00    |    98,943.00    |           0.66 | category | 11,901,361.00 |  -2,058,604.00 |\n",
      "| nr_index         |    451,311.00 | UInt16   |           457.00 |           47.00 |           0.10 | category |    151,650.00 |    -299,661.00 |\n",
      "| neg_index        |    451,311.00 | UInt16   |     51,496.00    |          124.00 |           0.00 | category |    154,977.00 |    -296,334.00 |\n",
      "| relay_index      |    300,874.00 | UInt8    |      2,891.00    |           55.00 |           0.02 | category |    152,651.00 |    -148,223.00 |\n",
      "| utt_len          |    451,311.00 | UInt16   |    150,437.00    |          230.00 |           0.00 | category |    309,860.00 |    -141,451.00 |\n",
      "| adv_index        |    451,311.00 | UInt16   |    150,437.00    |          307.00 |           0.00 | category |    310,091.00 |    -141,220.00 |\n",
      "| adj_index        |    451,311.00 | UInt16   |    150,437.00    |          307.00 |           0.00 | category |    310,091.00 |    -141,220.00 |\n",
      "| neg_form         |    153,300.00 | category |     51,496.00    |           29.00 |           0.00 | nan      |           nan |            nan |\n",
      "| neg_lemma        |    151,799.00 | category |     51,496.00    |           13.00 |           0.00 | nan      |           nan |            nan |\n",
      "| corpus           |    150,752.00 | category |    150,437.00    |            3.00 |           0.00 | nan      |           nan |            nan |\n",
      "| adv_form         |    415,485.00 | category |    150,437.00    |     1,241.00    |           0.01 | nan      |           nan |            nan |\n",
      "| adv_lemma        |    407,135.00 | category |    150,437.00    |     1,110.00    |           0.01 | nan      |           nan |            nan |\n",
      "| nr_lemma         |    152,124.00 | category |           457.00 |           18.00 |           0.04 | nan      |           nan |            nan |\n",
      "| nr_form          |    153,603.00 | category |           457.00 |           33.00 |           0.07 | nan      |           nan |            nan |\n",
      "✓\tcolloc\tcategory\n",
      "✕\tnr_form\tcategory\n",
      "✕\tneg_form\tcategory\n",
      "✕\tadv_form\tcategory\n",
      "✓\tadj_form\tcategory\n",
      "✓\thit_text\tcategory\n",
      "✓\tsent_id\tcategory\n",
      "✓\tmatch_id\tcategory\n",
      "✓\tcolloc_id\tcategory\n",
      "✓\tlemma_str\tcategory\n",
      "✕\tnr_lemma\tcategory\n",
      "✕\tneg_lemma\tcategory\n",
      "✕\tadv_lemma\tcategory\n",
      "✓\tadj_lemma\tcategory\n",
      "✓\tnr_index\tcategory\n",
      "✓\tneg_index\tcategory\n",
      "✓\tadv_index\tcategory\n",
      "✓\tadj_index\tcategory\n",
      "✓\tutt_len\tcategory\n",
      "✓\tcategory\tcategory\n",
      "✓\tdep_str\tcategory\n",
      "✓\tdep_str_ix\tcategory\n",
      "✓\tdep_str_full\tcategory\n",
      "✓\tdep_str_rel\tcategory\n",
      "✓\tdep_str_mask\tcategory\n",
      "✓\tdep_str_mask_rel\tcategory\n",
      "✕\tcorpus\tcategory\n",
      "✓\trelay_form\tcategory\n",
      "✓\trelay_lemma\tcategory\n",
      "✓\trelay_index\tcategory\n",
      "Category Converted dataframe:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150437 entries, apw_eng_20030918_0697_20:4-5-8-9 to pcc_eng_09_047.0803_x0745587_19:3-4\n",
      "Data columns (total 35 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   colloc            150437 non-null  category\n",
      " 1   nr_form           457 non-null     category\n",
      " 2   neg_form          51496 non-null   category\n",
      " 3   adv_form          150437 non-null  category\n",
      " 4   adj_form          150437 non-null  category\n",
      " 5   hit_text          150437 non-null  category\n",
      " 6   sent_id           150437 non-null  category\n",
      " 7   match_id          150437 non-null  category\n",
      " 8   colloc_id         150437 non-null  category\n",
      " 9   lemma_str         150437 non-null  category\n",
      " 10  nr_lemma          457 non-null     category\n",
      " 11  neg_lemma         51496 non-null   category\n",
      " 12  adv_lemma         150437 non-null  category\n",
      " 13  adj_lemma         150437 non-null  category\n",
      " 14  nr_index          457 non-null     category\n",
      " 15  neg_index         51496 non-null   category\n",
      " 16  adv_index         150437 non-null  category\n",
      " 17  adj_index         150437 non-null  category\n",
      " 18  dep_negraise      457 non-null     object  \n",
      " 19  dep_neg           51496 non-null   object  \n",
      " 20  dep_mod           150437 non-null  object  \n",
      " 21  utt_len           150437 non-null  category\n",
      " 22  category          150437 non-null  category\n",
      " 23  hits_df_path      150437 non-null  object  \n",
      " 24  dep_str           150437 non-null  category\n",
      " 25  dep_str_ix        150437 non-null  category\n",
      " 26  dep_str_full      150437 non-null  category\n",
      " 27  dep_str_rel       150437 non-null  category\n",
      " 28  dep_str_mask      150437 non-null  category\n",
      " 29  dep_str_mask_rel  150437 non-null  category\n",
      " 30  corpus            150437 non-null  category\n",
      " 31  relay_form        2891 non-null    category\n",
      " 32  relay_lemma       2891 non-null    category\n",
      " 33  relay_index       2891 non-null    category\n",
      " 34  dep_relay         2891 non-null    object  \n",
      "dtypes: category(30), object(5)\n",
      "memory usage: 155.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colloc</th>\n",
       "      <th>nr_form</th>\n",
       "      <th>neg_form</th>\n",
       "      <th>adv_form</th>\n",
       "      <th>adj_form</th>\n",
       "      <th>hit_text</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>match_id</th>\n",
       "      <th>colloc_id</th>\n",
       "      <th>lemma_str</th>\n",
       "      <th>...</th>\n",
       "      <th>dep_str_ix</th>\n",
       "      <th>dep_str_full</th>\n",
       "      <th>dep_str_rel</th>\n",
       "      <th>dep_str_mask</th>\n",
       "      <th>dep_str_mask_rel</th>\n",
       "      <th>corpus</th>\n",
       "      <th>relay_form</th>\n",
       "      <th>relay_lemma</th>\n",
       "      <th>relay_index</th>\n",
       "      <th>dep_relay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apw_eng_20030918_0697_20:4-5-8-9</th>\n",
       "      <td>exactly_safe</td>\n",
       "      <td>think</td>\n",
       "      <td>n't</td>\n",
       "      <td>exactly</td>\n",
       "      <td>safe</td>\n",
       "      <td>n't think it was exactly safe</td>\n",
       "      <td>apw_eng_20030918_0697_20</td>\n",
       "      <td>4-5-8-9</td>\n",
       "      <td>apw_eng_20030918_0697_20:8-9</td>\n",
       "      <td>`` I do not think it be exactly safe moviemake...</td>\n",
       "      <td>...</td>\n",
       "      <td>04:think&gt;03:not[=neg]; 08:safe&gt;07:exactly[=mod...</td>\n",
       "      <td>04:think&gt;[neg]&gt;03:not[=neg]; 08:safe&gt;[advmod]&gt;...</td>\n",
       "      <td>think&gt;[neg]&gt;not[=neg]; safe&gt;[advmod]&gt;exactly[=...</td>\n",
       "      <td>VB&gt;not; JJ&gt;RB; think&gt;JJ</td>\n",
       "      <td>VB&gt;[neg]&gt;not; JJ&gt;[advmod]&gt;RB; think&gt;[ccomp]&gt;JJ</td>\n",
       "      <td>exactly_apw</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_20020929_0547_33:3-4-7-8</th>\n",
       "      <td>exactly_clear</td>\n",
       "      <td>think</td>\n",
       "      <td>n't</td>\n",
       "      <td>exactly</td>\n",
       "      <td>clear</td>\n",
       "      <td>n't think they 're exactly clear</td>\n",
       "      <td>apw_eng_20020929_0547_33</td>\n",
       "      <td>3-4-7-8</td>\n",
       "      <td>apw_eng_20020929_0547_33:7-8</td>\n",
       "      <td>I do not think they be exactly clear how to do...</td>\n",
       "      <td>...</td>\n",
       "      <td>03:think&gt;02:not[=neg]; 07:clear&gt;06:exactly[=mo...</td>\n",
       "      <td>03:think&gt;[neg]&gt;02:not[=neg]; 07:clear&gt;[advmod]...</td>\n",
       "      <td>think&gt;[neg]&gt;not[=neg]; clear&gt;[advmod]&gt;exactly[...</td>\n",
       "      <td>VB&gt;not; JJ&gt;RB; think&gt;JJ</td>\n",
       "      <td>VB&gt;[neg]&gt;not; JJ&gt;[advmod]&gt;RB; think&gt;[ccomp]&gt;JJ</td>\n",
       "      <td>exactly_apw</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_20020206_1493_20:4-5-8-9</th>\n",
       "      <td>exactly_correct</td>\n",
       "      <td>think</td>\n",
       "      <td>not</td>\n",
       "      <td>exactly</td>\n",
       "      <td>correct</td>\n",
       "      <td>not think that is exactly correct</td>\n",
       "      <td>apw_eng_20020206_1493_20</td>\n",
       "      <td>4-5-8-9</td>\n",
       "      <td>apw_eng_20020206_1493_20:8-9</td>\n",
       "      <td>`` I do not think that be exactly correct , ''...</td>\n",
       "      <td>...</td>\n",
       "      <td>04:think&gt;03:not[=neg]; 08:correct&gt;07:exactly[=...</td>\n",
       "      <td>04:think&gt;[neg]&gt;03:not[=neg]; 08:correct&gt;[advmo...</td>\n",
       "      <td>think&gt;[neg]&gt;not[=neg]; correct&gt;[advmod]&gt;exactl...</td>\n",
       "      <td>VB&gt;not; JJ&gt;RB; think&gt;JJ</td>\n",
       "      <td>VB&gt;[neg]&gt;not; JJ&gt;[advmod]&gt;RB; think&gt;[ccomp]&gt;JJ</td>\n",
       "      <td>exactly_apw</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19980320_1318_52:03-04-18-19</th>\n",
       "      <td>exactly_high</td>\n",
       "      <td>want</td>\n",
       "      <td>n't</td>\n",
       "      <td>exactly</td>\n",
       "      <td>high</td>\n",
       "      <td>n't want to sound racist , but the intellectua...</td>\n",
       "      <td>apw_eng_19980320_1318_52</td>\n",
       "      <td>03-04-18-19</td>\n",
       "      <td>apw_eng_19980320_1318_52:18-19</td>\n",
       "      <td>I do not want to sound racist , but the intell...</td>\n",
       "      <td>...</td>\n",
       "      <td>03:want&gt;02:not[=neg]; 18:high&gt;17:exactly[=mod]...</td>\n",
       "      <td>03:want&gt;[neg]&gt;02:not[=neg]; 18:high&gt;[advmod]&gt;1...</td>\n",
       "      <td>want&gt;[neg]&gt;not[=neg]; high&gt;[advmod]&gt;exactly[=m...</td>\n",
       "      <td>VB&gt;not; JJ&gt;RB; want&gt;JJ</td>\n",
       "      <td>VB&gt;[neg]&gt;not; JJ&gt;[advmod]&gt;RB; want&gt;[conj]&gt;JJ</td>\n",
       "      <td>exactly_apw</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyt_eng_20091024_0144_21:04-05-10-11</th>\n",
       "      <td>exactly_sure</td>\n",
       "      <td>think</td>\n",
       "      <td>n't</td>\n",
       "      <td>exactly</td>\n",
       "      <td>sure</td>\n",
       "      <td>n't think any of us are exactly sure</td>\n",
       "      <td>nyt_eng_20091024_0144_21</td>\n",
       "      <td>04-05-10-11</td>\n",
       "      <td>nyt_eng_20091024_0144_21:10-11</td>\n",
       "      <td>`` I do not think any of we be exactly sure wh...</td>\n",
       "      <td>...</td>\n",
       "      <td>04:think&gt;03:not[=neg]; 10:sure&gt;09:exactly[=mod...</td>\n",
       "      <td>04:think&gt;[neg]&gt;03:not[=neg]; 10:sure&gt;[advmod]&gt;...</td>\n",
       "      <td>think&gt;[neg]&gt;not[=neg]; sure&gt;[advmod]&gt;exactly[=...</td>\n",
       "      <td>VB&gt;not; JJ&gt;RB; think&gt;JJ</td>\n",
       "      <td>VB&gt;[neg]&gt;not; JJ&gt;[advmod]&gt;RB; think&gt;[ccomp]&gt;JJ</td>\n",
       "      <td>exactly_nyt</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_09_047.1818_x0747142_10:4-5</th>\n",
       "      <td>exactly_new</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>exactly</td>\n",
       "      <td>new</td>\n",
       "      <td>exactly new</td>\n",
       "      <td>pcc_eng_09_047.1818_x0747142_10</td>\n",
       "      <td>4-5</td>\n",
       "      <td>pcc_eng_09_047.1818_x0747142_10:4-5</td>\n",
       "      <td>which be not exactly new for a company that be...</td>\n",
       "      <td>...</td>\n",
       "      <td>04:new&gt;03:exactly[=mod]</td>\n",
       "      <td>04:new&gt;[advmod]&gt;03:exactly[=mod]</td>\n",
       "      <td>new&gt;[advmod]&gt;exactly[=mod]</td>\n",
       "      <td>JJ&gt;RB</td>\n",
       "      <td>JJ&gt;[advmod]&gt;RB</td>\n",
       "      <td>exactly_puddin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_09_047.1686_x0746931_024:19-20</th>\n",
       "      <td>exactly_sure</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>exactly</td>\n",
       "      <td>sure</td>\n",
       "      <td>exactly sure</td>\n",
       "      <td>pcc_eng_09_047.1686_x0746931_024</td>\n",
       "      <td>19-20</td>\n",
       "      <td>pcc_eng_09_047.1686_x0746931_024:19-20</td>\n",
       "      <td>it may have have something to do with the lack...</td>\n",
       "      <td>...</td>\n",
       "      <td>19:sure&gt;18:exactly[=mod]</td>\n",
       "      <td>19:sure&gt;[advmod]&gt;18:exactly[=mod]</td>\n",
       "      <td>sure&gt;[advmod]&gt;exactly[=mod]</td>\n",
       "      <td>JJ&gt;RB</td>\n",
       "      <td>JJ&gt;[advmod]&gt;RB</td>\n",
       "      <td>exactly_puddin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_09_047.0948_x0745812_11:4-5</th>\n",
       "      <td>exactly_sure</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>exactly</td>\n",
       "      <td>sure</td>\n",
       "      <td>exactly sure</td>\n",
       "      <td>pcc_eng_09_047.0948_x0745812_11</td>\n",
       "      <td>4-5</td>\n",
       "      <td>pcc_eng_09_047.0948_x0745812_11:4-5</td>\n",
       "      <td>I be not exactly sure what the big difference ...</td>\n",
       "      <td>...</td>\n",
       "      <td>04:sure&gt;03:exactly[=mod]</td>\n",
       "      <td>04:sure&gt;[advmod]&gt;03:exactly[=mod]</td>\n",
       "      <td>sure&gt;[advmod]&gt;exactly[=mod]</td>\n",
       "      <td>JJ&gt;RB</td>\n",
       "      <td>JJ&gt;[advmod]&gt;RB</td>\n",
       "      <td>exactly_puddin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_09_047.0803_x0745587_20:4-5</th>\n",
       "      <td>unnervingly_apocalyptic</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>unnervingly</td>\n",
       "      <td>apocalyptic</td>\n",
       "      <td>unnervingly apocalyptic</td>\n",
       "      <td>pcc_eng_09_047.0803_x0745587_20</td>\n",
       "      <td>4-5</td>\n",
       "      <td>pcc_eng_09_047.0803_x0745587_20:4-5</td>\n",
       "      <td>to combat the unnervingly apocalyptic crise De...</td>\n",
       "      <td>...</td>\n",
       "      <td>04:apocalyptic&gt;03:unnervingly[=mod]</td>\n",
       "      <td>04:apocalyptic&gt;[advmod]&gt;03:unnervingly[=mod]</td>\n",
       "      <td>apocalyptic&gt;[advmod]&gt;unnervingly[=mod]</td>\n",
       "      <td>JJ&gt;RB</td>\n",
       "      <td>JJ&gt;[advmod]&gt;RB</td>\n",
       "      <td>exactly_puddin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_09_047.0803_x0745587_19:3-4</th>\n",
       "      <td>exactly_wrong</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>exactly</td>\n",
       "      <td>wrong</td>\n",
       "      <td>exactly wrong</td>\n",
       "      <td>pcc_eng_09_047.0803_x0745587_19</td>\n",
       "      <td>3-4</td>\n",
       "      <td>pcc_eng_09_047.0803_x0745587_19:3-4</td>\n",
       "      <td>well , exactly wrong .</td>\n",
       "      <td>...</td>\n",
       "      <td>03:wrong&gt;02:exactly[=mod]</td>\n",
       "      <td>03:wrong&gt;[advmod]&gt;02:exactly[=mod]</td>\n",
       "      <td>wrong&gt;[advmod]&gt;exactly[=mod]</td>\n",
       "      <td>JJ&gt;RB</td>\n",
       "      <td>JJ&gt;[advmod]&gt;RB</td>\n",
       "      <td>exactly_puddin</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150437 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         colloc nr_form  \\\n",
       "hit_id                                                                    \n",
       "apw_eng_20030918_0697_20:4-5-8-9                   exactly_safe   think   \n",
       "apw_eng_20020929_0547_33:3-4-7-8                  exactly_clear   think   \n",
       "apw_eng_20020206_1493_20:4-5-8-9                exactly_correct   think   \n",
       "apw_eng_19980320_1318_52:03-04-18-19               exactly_high    want   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11               exactly_sure   think   \n",
       "...                                                         ...     ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                 exactly_new    <NA>   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20             exactly_sure    <NA>   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                exactly_sure    <NA>   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5     unnervingly_apocalyptic    <NA>   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4               exactly_wrong    <NA>   \n",
       "\n",
       "                                       neg_form     adv_form     adj_form  \\\n",
       "hit_id                                                                      \n",
       "apw_eng_20030918_0697_20:4-5-8-9            n't      exactly         safe   \n",
       "apw_eng_20020929_0547_33:3-4-7-8            n't      exactly        clear   \n",
       "apw_eng_20020206_1493_20:4-5-8-9            not      exactly      correct   \n",
       "apw_eng_19980320_1318_52:03-04-18-19        n't      exactly         high   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11        n't      exactly         sure   \n",
       "...                                         ...          ...          ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5        <NA>      exactly          new   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20     <NA>      exactly         sure   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5        <NA>      exactly         sure   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5        <NA>  unnervingly  apocalyptic   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4        <NA>      exactly        wrong   \n",
       "\n",
       "                                                                                 hit_text  \\\n",
       "hit_id                                                                                      \n",
       "apw_eng_20030918_0697_20:4-5-8-9                            n't think it was exactly safe   \n",
       "apw_eng_20020929_0547_33:3-4-7-8                         n't think they 're exactly clear   \n",
       "apw_eng_20020206_1493_20:4-5-8-9                        not think that is exactly correct   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    n't want to sound racist , but the intellectua...   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11                 n't think any of us are exactly sure   \n",
       "...                                                                                   ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                                           exactly new   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20                                       exactly sure   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                                          exactly sure   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5                               unnervingly apocalyptic   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                                         exactly wrong   \n",
       "\n",
       "                                                                 sent_id  \\\n",
       "hit_id                                                                     \n",
       "apw_eng_20030918_0697_20:4-5-8-9                apw_eng_20030918_0697_20   \n",
       "apw_eng_20020929_0547_33:3-4-7-8                apw_eng_20020929_0547_33   \n",
       "apw_eng_20020206_1493_20:4-5-8-9                apw_eng_20020206_1493_20   \n",
       "apw_eng_19980320_1318_52:03-04-18-19            apw_eng_19980320_1318_52   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11            nyt_eng_20091024_0144_21   \n",
       "...                                                                  ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5      pcc_eng_09_047.1818_x0747142_10   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20  pcc_eng_09_047.1686_x0746931_024   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5      pcc_eng_09_047.0948_x0745812_11   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5      pcc_eng_09_047.0803_x0745587_20   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4      pcc_eng_09_047.0803_x0745587_19   \n",
       "\n",
       "                                           match_id  \\\n",
       "hit_id                                                \n",
       "apw_eng_20030918_0697_20:4-5-8-9            4-5-8-9   \n",
       "apw_eng_20020929_0547_33:3-4-7-8            3-4-7-8   \n",
       "apw_eng_20020206_1493_20:4-5-8-9            4-5-8-9   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    03-04-18-19   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    04-05-10-11   \n",
       "...                                             ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5             4-5   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20        19-20   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5             4-5   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5             4-5   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4             3-4   \n",
       "\n",
       "                                                                     colloc_id  \\\n",
       "hit_id                                                                           \n",
       "apw_eng_20030918_0697_20:4-5-8-9                  apw_eng_20030918_0697_20:8-9   \n",
       "apw_eng_20020929_0547_33:3-4-7-8                  apw_eng_20020929_0547_33:7-8   \n",
       "apw_eng_20020206_1493_20:4-5-8-9                  apw_eng_20020206_1493_20:8-9   \n",
       "apw_eng_19980320_1318_52:03-04-18-19            apw_eng_19980320_1318_52:18-19   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11            nyt_eng_20091024_0144_21:10-11   \n",
       "...                                                                        ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5        pcc_eng_09_047.1818_x0747142_10:4-5   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20  pcc_eng_09_047.1686_x0746931_024:19-20   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5        pcc_eng_09_047.0948_x0745812_11:4-5   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5        pcc_eng_09_047.0803_x0745587_20:4-5   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4        pcc_eng_09_047.0803_x0745587_19:3-4   \n",
       "\n",
       "                                                                                lemma_str  \\\n",
       "hit_id                                                                                      \n",
       "apw_eng_20030918_0697_20:4-5-8-9        `` I do not think it be exactly safe moviemake...   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        I do not think they be exactly clear how to do...   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        `` I do not think that be exactly correct , ''...   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    I do not want to sound racist , but the intell...   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    `` I do not think any of we be exactly sure wh...   \n",
       "...                                                                                   ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5     which be not exactly new for a company that be...   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20  it may have have something to do with the lack...   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5     I be not exactly sure what the big difference ...   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5     to combat the unnervingly apocalyptic crise De...   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                                well , exactly wrong .   \n",
       "\n",
       "                                        ...  \\\n",
       "hit_id                                  ...   \n",
       "apw_eng_20030918_0697_20:4-5-8-9        ...   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        ...   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        ...   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    ...   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    ...   \n",
       "...                                     ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5     ...   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20  ...   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5     ...   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5     ...   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4     ...   \n",
       "\n",
       "                                                                               dep_str_ix  \\\n",
       "hit_id                                                                                      \n",
       "apw_eng_20030918_0697_20:4-5-8-9        04:think>03:not[=neg]; 08:safe>07:exactly[=mod...   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        03:think>02:not[=neg]; 07:clear>06:exactly[=mo...   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        04:think>03:not[=neg]; 08:correct>07:exactly[=...   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    03:want>02:not[=neg]; 18:high>17:exactly[=mod]...   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    04:think>03:not[=neg]; 10:sure>09:exactly[=mod...   \n",
       "...                                                                                   ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                               04:new>03:exactly[=mod]   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20                           19:sure>18:exactly[=mod]   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                              04:sure>03:exactly[=mod]   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5                   04:apocalyptic>03:unnervingly[=mod]   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                             03:wrong>02:exactly[=mod]   \n",
       "\n",
       "                                                                             dep_str_full  \\\n",
       "hit_id                                                                                      \n",
       "apw_eng_20030918_0697_20:4-5-8-9        04:think>[neg]>03:not[=neg]; 08:safe>[advmod]>...   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        03:think>[neg]>02:not[=neg]; 07:clear>[advmod]...   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        04:think>[neg]>03:not[=neg]; 08:correct>[advmo...   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    03:want>[neg]>02:not[=neg]; 18:high>[advmod]>1...   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    04:think>[neg]>03:not[=neg]; 10:sure>[advmod]>...   \n",
       "...                                                                                   ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                      04:new>[advmod]>03:exactly[=mod]   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20                  19:sure>[advmod]>18:exactly[=mod]   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                     04:sure>[advmod]>03:exactly[=mod]   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5          04:apocalyptic>[advmod]>03:unnervingly[=mod]   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                    03:wrong>[advmod]>02:exactly[=mod]   \n",
       "\n",
       "                                                                              dep_str_rel  \\\n",
       "hit_id                                                                                      \n",
       "apw_eng_20030918_0697_20:4-5-8-9        think>[neg]>not[=neg]; safe>[advmod]>exactly[=...   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        think>[neg]>not[=neg]; clear>[advmod]>exactly[...   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        think>[neg]>not[=neg]; correct>[advmod]>exactl...   \n",
       "apw_eng_19980320_1318_52:03-04-18-19    want>[neg]>not[=neg]; high>[advmod]>exactly[=m...   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    think>[neg]>not[=neg]; sure>[advmod]>exactly[=...   \n",
       "...                                                                                   ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                            new>[advmod]>exactly[=mod]   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20                        sure>[advmod]>exactly[=mod]   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                           sure>[advmod]>exactly[=mod]   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5                apocalyptic>[advmod]>unnervingly[=mod]   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                          wrong>[advmod]>exactly[=mod]   \n",
       "\n",
       "                                                   dep_str_mask  \\\n",
       "hit_id                                                            \n",
       "apw_eng_20030918_0697_20:4-5-8-9        VB>not; JJ>RB; think>JJ   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        VB>not; JJ>RB; think>JJ   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        VB>not; JJ>RB; think>JJ   \n",
       "apw_eng_19980320_1318_52:03-04-18-19     VB>not; JJ>RB; want>JJ   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    VB>not; JJ>RB; think>JJ   \n",
       "...                                                         ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                       JJ>RB   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20                    JJ>RB   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                       JJ>RB   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5                       JJ>RB   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                       JJ>RB   \n",
       "\n",
       "                                                                      dep_str_mask_rel  \\\n",
       "hit_id                                                                                   \n",
       "apw_eng_20030918_0697_20:4-5-8-9        VB>[neg]>not; JJ>[advmod]>RB; think>[ccomp]>JJ   \n",
       "apw_eng_20020929_0547_33:3-4-7-8        VB>[neg]>not; JJ>[advmod]>RB; think>[ccomp]>JJ   \n",
       "apw_eng_20020206_1493_20:4-5-8-9        VB>[neg]>not; JJ>[advmod]>RB; think>[ccomp]>JJ   \n",
       "apw_eng_19980320_1318_52:03-04-18-19      VB>[neg]>not; JJ>[advmod]>RB; want>[conj]>JJ   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11    VB>[neg]>not; JJ>[advmod]>RB; think>[ccomp]>JJ   \n",
       "...                                                                                ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5                                     JJ>[advmod]>RB   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20                                  JJ>[advmod]>RB   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5                                     JJ>[advmod]>RB   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5                                     JJ>[advmod]>RB   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4                                     JJ>[advmod]>RB   \n",
       "\n",
       "                                                corpus relay_form relay_lemma  \\\n",
       "hit_id                                                                          \n",
       "apw_eng_20030918_0697_20:4-5-8-9           exactly_apw       <NA>        <NA>   \n",
       "apw_eng_20020929_0547_33:3-4-7-8           exactly_apw       <NA>        <NA>   \n",
       "apw_eng_20020206_1493_20:4-5-8-9           exactly_apw       <NA>        <NA>   \n",
       "apw_eng_19980320_1318_52:03-04-18-19       exactly_apw       <NA>        <NA>   \n",
       "nyt_eng_20091024_0144_21:04-05-10-11       exactly_nyt       <NA>        <NA>   \n",
       "...                                                ...        ...         ...   \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5     exactly_puddin       <NA>        <NA>   \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20  exactly_puddin       <NA>        <NA>   \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5     exactly_puddin       <NA>        <NA>   \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5     exactly_puddin       <NA>        <NA>   \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4     exactly_puddin       <NA>        <NA>   \n",
       "\n",
       "                                       relay_index dep_relay  \n",
       "hit_id                                                        \n",
       "apw_eng_20030918_0697_20:4-5-8-9               NaN       NaN  \n",
       "apw_eng_20020929_0547_33:3-4-7-8               NaN       NaN  \n",
       "apw_eng_20020206_1493_20:4-5-8-9               NaN       NaN  \n",
       "apw_eng_19980320_1318_52:03-04-18-19           NaN       NaN  \n",
       "nyt_eng_20091024_0144_21:04-05-10-11           NaN       NaN  \n",
       "...                                            ...       ...  \n",
       "pcc_eng_09_047.1818_x0747142_10:4-5            NaN       NaN  \n",
       "pcc_eng_09_047.1686_x0746931_024:19-20         NaN       NaN  \n",
       "pcc_eng_09_047.0948_x0745812_11:4-5            NaN       NaN  \n",
       "pcc_eng_09_047.0803_x0745587_20:4-5            NaN       NaN  \n",
       "pcc_eng_09_047.0803_x0745587_19:3-4            NaN       NaN  \n",
       "\n",
       "[150437 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf = optimize_hit_df(ddf, verbosity=2)\n",
    "odf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = odf.columns[~odf.columns.isin(cols_by_str(\n",
    "    odf, start_str=('dep_m', 'dep_n', 'dep_r', 'context')))].to_list()\n",
    "columns.sort()\n",
    "odf = odf.loc[:, columns]\n",
    "len(odf) == len(ddf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit to `exactly` adverbs *only*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = odf.loc[odf.adv_lemma == 'exactly', :]\n",
    "odf.loc[odf.corpus.str.endswith('puddin'), 'corpus_group'] = 'puddin'\n",
    "odf.loc[odf.corpus.str.endswith(('nyt', 'apw')), 'corpus_group'] = 'news'\n",
    "general_counts = show_counts(odf, ['category', 'corpus_group']).unstack(\n",
    ").sort_values(('count', 'puddin'), ascending=False)  # type: ignore\n",
    "general_counts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* 👇 *`odf` is shorter (fewer rows) than original loaded hits because adverbs other than `'exactly'` have been dropped.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(odf) < len(ddf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `conllu_id` and drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = odf.assign(conllu_id=odf.sent_id.str.rsplit('_', 2).str.get(0).str.split(\n",
    "    '.').str.get(0).astype('string').astype('category'))  # type: ignore\n",
    "tdf = odf[cols_by_str(odf, end_str=('lemma', 'id', 'text', 'window',\n",
    "                      'category', 'Pol')) + cols_by_str(odf, start_str=('corpus', 'lemma'))]\n",
    "print(f'Total \"exactly\" hits for all patterns: {len(tdf)}')\n",
    "summary_tdf = summarize_text_cols(tdf)\n",
    "summary_tdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify $PosPol$ and $NegPol$ contexts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A\n",
    "bare collocation tokens (`advadj.all-RB-JJs` pattern match) which do not appear as matches for any other pattern match (i.e. $NegPol$ contexts).\n",
    "\n",
    "*That is, the `colloc_id` (unique `ADV` & `ADJ` nodes in unique sentence tokens) is not duplicated.*\n",
    "\n",
    "```{python}\n",
    "tdfp_a = tdf.loc[(tdf.category=='advadj') & (~tdf.duplicated(subset='colloc_id', keep=False)), :]\n",
    "```\n",
    "\n",
    "### Option B\n",
    "categorize $NegPol$ set first (`tdfn`), then compute complement of that (i.e. $ALL - NegPol$)\n",
    "\n",
    "```{python}\n",
    "tdfn = tdf.loc[tdf.neg_lemma!='_', :]\n",
    "tdfp = tdf.loc[~tdf.colloc_id.isin(tdfn.colloc_id), :]\n",
    "```\n",
    "\n",
    "### Options A and B are identical\n",
    "\n",
    "`all(tdfp_a.index == tdfp_b.index)` evaluates as true\n",
    "\n",
    "So, since $NegPol$ is more directly defined, and has to be separated out anyway, it's simpler to just get the \"complement\", (`tdfp_b` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfn = tdf.loc[tdf.neg_lemma != '_', :]\n",
    "summarize_text_cols(tdfn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfp = tdf.loc[~tdf.colloc_id.isin(tdfn.colloc_id), :]\n",
    "summarize_text_cols(tdfp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign `polarity` and recombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfp = tdfp.assign(polarity='positive')\n",
    "tdfn = tdfn.assign(polarity='negative')\n",
    "tdf_with_overlap = tdf\n",
    "\n",
    "pol_union_df = pd.concat([tdfp, tdfn]).sort_values('colloc_id')\n",
    "\n",
    "print(\n",
    "    f'Total bare `exactly ADJ` collocations: {odf.category.value_counts()[\"advadj\"]}')\n",
    "print(\n",
    "    f'Total `exactly` pattern hits (+ NegPol pattern overlap): {len(pol_union_df)}')\n",
    "print(f'PosPol: {round(100*len(tdfp)/len(pol_union_df))}% : {len(tdfp)} hits')\n",
    "print(\n",
    "    f'NegPol: {str(round(100*len(tdfn)/len(pol_union_df))).zfill(2)}% : {len(tdfn)} hits')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Pos+Neg Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_union_df.to_pickle(\n",
    "    EXACTLY_OUT/'all-exactly-hits_text+polarity.pkl.gz')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency by Polarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Frequency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_200 = show_counts(pol_union_df, ['polarity', 'adj_lemma']).head(200)\n",
    "top_200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_to_camel(snake: str):\n",
    "    return ''.join([w.capitalize() for w in snake.split('_')])\n",
    "\n",
    "\n",
    "def save_for_ucs(df, col_1: str, col_2: str,\n",
    "                 output_dir: Path = None,\n",
    "                 ucs_path: Path = None,\n",
    "                 filter: dict = None,\n",
    "                 ):\n",
    "    if output_dir and not ucs_path:\n",
    "        ucs_path = (output_dir / 'ucs_format' /\n",
    "                    f'{snake_to_camel(col_1)}{snake_to_camel(col_2)}.tsv')\n",
    "    if filter:\n",
    "        for col, val in filter.items():\n",
    "            df = df.loc[df[col] == val, :]\n",
    "            if ucs_path:\n",
    "                ucs_path = ucs_path.with_stem(\n",
    "                    f\"{ucs_path.stem}_{snake_to_camel(col)}{val.upper()}\")\n",
    "    if ucs_path:\n",
    "        print(f'output path: {ucs_path}')\n",
    "        counts = show_counts(df, [col_1, col_2]).reset_index()[\n",
    "            ['count', col_1, col_2]]\n",
    "        counts.to_csv(ucs_path, encoding='utf8',\n",
    "                      sep='\\t', header=False, index=False)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_for_ucs(pol_union_df, 'polarity', 'adj_lemma', filter={'corpus_group':'news'}, output_dir = EXACTLY_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize import heatmap\n",
    "from utils.dataframes import transform_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_top_200 = top_200.unstack(level='polarity', fill_value=0)\n",
    "spread_top_200.sort_values(('count', 'negative'),\n",
    "                           ascending=False).head(30)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(transform_counts(spread_top_200.sort_values(('count', 'negative'),\n",
    "                           ascending=False).head(30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    spread_top_200.sort_values(('count', 'positive'),\n",
    "                               ascending=False)\n",
    "    .head(10).to_markdown(floatfmt=',.0f'))  # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstabulate adjective by context polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ratios(freq_dist:pd.DataFrame, dimensions:list, abbr_len:int=3):\n",
    "    for dimension in dimensions:\n",
    "        freq_dist[f'ratio_{dimension[:abbr_len]}'] = (\n",
    "            freq_dist[dimension]/freq_dist.TOTAL).round(3)\n",
    "\n",
    "\n",
    "def add_bins(freq_dist):\n",
    "    for ratio_col in freq_dist.copy().filter(like='ratio'):\n",
    "        dim_ratio = freq_dist[ratio_col]\n",
    "        bin_col = pd.to_numeric(dim_ratio.round(1), downcast='float')\n",
    "        bin_col = bin_col.astype('category')\n",
    "        freq_dist[ratio_col.replace('ratio', 'bin')] = bin_col\n",
    "\n",
    "\n",
    "def compute_meta_cols(df: pd.DataFrame,\n",
    "                      obs_col: str = 'adj_lemma',\n",
    "                      dim_col: str = 'polarity',\n",
    "                      dim_abbr_len: int = 3):\n",
    "    dimensions = list(df[dim_col].unique())\n",
    "    freq_dist = pd.crosstab(df[obs_col],\n",
    "                            df[dim_col],\n",
    "                            margins=True, margins_name='TOTAL')\n",
    "    freq_dist = freq_dist[['TOTAL'] + dimensions]\n",
    "    compute_ratios(freq_dist, dimensions, dim_abbr_len)\n",
    "    add_bins(freq_dist)\n",
    "\n",
    "    # cols = freq_dist.columns.to_list()\n",
    "    # freq_dist = freq_dist[[cols.pop(cols.index('TOTAL'))] + cols]\n",
    "    return freq_dist.sort_values('TOTAL', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = compute_meta_cols(pol_union_df)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_meta_cols(pol_union_df, obs_col='polarity', dim_col='corpus_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_meta_cols(pol_union_df, obs_col='polarity', dim_col='corpus_group', dim_abbr_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_meta_cols(pol_union_df, dim_col='polarity', obs_col='corpus_group', dim_abbr_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_meta_cols(pol_union_df, obs_col='corpus', dim_col='corpus_group', dim_abbr_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh5 = freq_dist.loc[freq_dist.TOTAL >= 5, :]\n",
    "freq_thresh5 = freq_thresh5.sort_values(\n",
    "    ['bin_neg', 'TOTAL', 'ratio_neg'], ascending=False)\n",
    "freq_thresh5.to_csv(\n",
    "    EXACTLY_OUT / 'freq_thresh5.csv')\n",
    "freq_thresh5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist.ratio_neg.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist.loc[freq_dist.index!='TOTAL', ['negative', 'positive']].plot(x='negative', y ='positive', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist.loc[freq_dist.index!='TOTAL', ['negative', 'positive']].plot(x='negative', y ='positive', kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist.ratio_pos.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh100 = freq_dist.loc[freq_dist.TOTAL >= 100, :]\n",
    "freq_thresh100 = freq_thresh100.sort_values(\n",
    "    ['bin_neg', 'TOTAL', 'ratio_neg'], ascending=False)\n",
    "freq_thresh100.to_csv(\n",
    "    '/share/compling/projects/sanpi/notebooks/exactly_out/freq_thresh100.csv')\n",
    "freq_thresh100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh200 = freq_dist.loc[freq_dist.TOTAL >= 200, :]\n",
    "freq_thresh200 = freq_thresh200.sort_values(\n",
    "    ['bin_neg', 'TOTAL', 'ratio_neg'], ascending=False)\n",
    "freq_thresh200.to_csv(EXACTLY_OUT / 'freq_thresh200.csv')\n",
    "freq_thresh200\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save $PosPol$ text data\n",
    "\n",
    "Since $PosPol$ is defined as the complement of $NegPol$, accuracy relies on $NegPol$ catching all relevant cases.\n",
    "\n",
    "To ensure pattern specifications are sufficiently inclusive, all sentences with supposedly positive polarity\n",
    "should be manually inspected for any errant (uncaught) negative lemmas, as identified in the $NegPol$ pattern specifications.\n",
    "\n",
    "```{ocaml}\n",
    "NEG [lemma=\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];  \n",
    "```\n",
    "\n",
    "- [x] create simplified output of $PosPol$/`tdfp` sentence text data, with necessary identifiers\n",
    "- [x] save as csv\n",
    "- [ ] grep `pos_sentences.csv` for each neg lemma:\\\n",
    "  *There should not be any negative lemmas an `exactly JJ` collocation in its scope.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = ['adj_lemma', 'text_window', 'sent_text',\n",
    "               'sent_id', 'conllu_id', 'corpus']\n",
    "pos_text_info = tdfp.loc[:, select_cols]\n",
    "pos_text_info = pos_text_info.assign(\n",
    "    adj_neg_ratio=pos_text_info.adj_lemma.apply(lambda a: freq_dist.loc[a, 'ratio_neg'] if a in freq_dist.index else None))  # type: ignore\n",
    "pos_text_info.sort_values(['adj_neg_ratio', 'conllu_id'], ascending=False)\n",
    "pos_text_info.to_csv(EXACTLY_OUT / 'pos_sentences.csv')\n",
    "\n",
    "# tdfp.adj_lemma[~tdfp.adj_lemma.isin(freq_dist.index)].value_counts()\n",
    "# TODO?? why does it say there is a mismatch between the crosstab and pospol adj set?\n",
    "# ^ 🤔 probably something to do with categorical dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh5.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pol_union_thresh5 = pol_union_df.loc[pol_union_df.adj_lemma]\n",
    "adj_x_cat = (pd.crosstab(index=pol_union_df.adj_lemma, columns=pol_union_df.category, \n",
    "                         margins=True, margins_name='TOTAL', normalize='index')\n",
    "             .rename(columns={'advadj':'PosPol', 'contig':'contig_NegPol', \n",
    "                              'raised': 'raised_NegPol', 'scoped':'scoped_NegPol'})\n",
    "             .round(3))\n",
    "adj_x_cat.loc[adj_x_cat.index.isin(freq_thresh200.index), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_x_neg = pd.crosstab(index=pol_union_df.adj_lemma,\n",
    "                        columns=pol_union_df.neg_lemma, margins=True)\n",
    "adj_x_neg.nlargest(20, columns=['All'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-sanpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
