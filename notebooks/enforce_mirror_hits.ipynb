{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arh234/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'POSmirror': PosixPath('/share/compling/data/sanpi/4_post-processed/POSmirror/trigger-bigrams_frq-thrMIN-7.35f.pkl.gz'),\n",
       " 'NEGmirror': PosixPath('/share/compling/data/sanpi/4_post-processed/NEGmirror/trigger-bigrams_frq-thrMIN-7.35f.pkl.gz')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from source.utils import POST_PROC_DIR, print_iter\n",
    "from source.utils.sample import sample_pickle\n",
    "\n",
    "HIT_EX_COLS = ['WITH::^.[il].*lower', 'WITH::text', 'token_str']\n",
    "# sanpi/4_post-processed/POSmirror/trigger-bigrams_frq-thrMIN-7.35f.pkl.gz\n",
    "pkl_name = 'trigger-bigrams_frq-thrMIN-7.35f.pkl.gz'\n",
    "path_dict = {p: POST_PROC_DIR / p / pkl_name for  p in ('POSmirror','NEGmirror')}\n",
    "path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmir = pd.read_pickle(path_dict['NEGmirror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmir = pd.read_pickle(path_dict['POSmirror'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bigram_lower\n",
       "v._good           2\n",
       "not_o.k           2\n",
       "even_1\\/4-inch    1\n",
       "v._expensive      1\n",
       "fun._amoral       1\n",
       "v._religious      1\n",
       "too*_ironic       1\n",
       "is*_remarkable    1\n",
       "v._profound       1\n",
       "def._wrong        1\n",
       "just_o.k.         1\n",
       "so*_expensive     1\n",
       "probably_no.      1\n",
       "c._ecstatic       1\n",
       "b)_impossible     1\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmir[pmir.bigram_lower.str.contains(r\"[^\\w'-]\", regex=True)].bigram_lower.value_counts().nlargest(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_form_combos(df): \n",
    "    \n",
    "    return df.assign(\n",
    "        all_forms_lower = df.filter(regex=r'^[nma][^l]\\w+lower$').apply(lambda x: '_'.join(x), axis=1),\n",
    "        bigram_lower = df.filter(['adv_form_lower', 'adj_form_lower']).apply(lambda x: '_'.join(x), axis=1)\n",
    "                     )\n",
    "\n",
    "\n",
    "def remove_odd_orth_forms(df):\n",
    "\n",
    "    df.loc[:, ['adv_form_lower', 'adj_form_lower', 'adj_lemma', 'adv_lemma']\n",
    "           ] = df.loc[:, ['adv_form_lower', 'adj_form_lower', 'adj_lemma', 'adv_lemma']\n",
    "                      ].astype('string')\n",
    "\n",
    "    def adv_is_very(df):\n",
    "        return df.adv_form_lower.str.contains(r'^v\\.?$|^ve+r+y+$', regex=True)\n",
    "\n",
    "    def adv_is_def(df):\n",
    "        return df.adv_form_lower.str.contains(r'^def\\.?$', regex=True)\n",
    "\n",
    "    def adj_is_ok(df):\n",
    "        return df.adj_form_lower.str.contains(r'^o*\\.?k+\\.?a*y*$', regex=True)\n",
    "\n",
    "    print('Dropping most bizarre...')\n",
    "    print(df.loc[df.bigram_lower.str.contains(r'[\\[\\\\\\/)]', regex=True),\n",
    "          ['adv_form_lower', 'adj_form_lower']].astype('string').value_counts()\n",
    "          .nlargest(10).to_frame().reset_index()\n",
    "          .to_markdown(floatfmt=',.0f', intfmt=','))\n",
    "    df = df.loc[~df.bigram_lower.str.contains(r'[\\[\\\\\\/)]', regex=True), :]\n",
    "\n",
    "    print('Dropping plain numerals as adjectives')\n",
    "    print(df.loc[df.adj_form_lower.astype('string').str.contains(r'^\\d+$'), ['adv_form_lower', 'adj_form_lower', 'text_window']]\n",
    "          .astype('string').value_counts().nlargest(10).to_frame().reset_index().to_markdown(floatfmt=',.0f')\n",
    "          )\n",
    "    df = df.loc[~df.adj_form_lower.astype('string').str.contains(r'^\\d+$'), :]\n",
    "\n",
    "    print('Translating some known orthographic quirks...')\n",
    "    # > variations on \"very\"\n",
    "    print('\\n==== very ====')\n",
    "    print(df.loc[adv_is_very(df), 'adv_form_lower']\n",
    "          .astype('string').value_counts().nlargest(10).to_frame()\n",
    "          .to_markdown(floatfmt=',.0f', intfmt=','))\n",
    "    df.loc[adv_is_very(df), :] = df.loc[adv_is_very(df), :].assign(\n",
    "        adv_lemma='very',\n",
    "        adv_form_lower='very')\n",
    "\n",
    "    # > variations on \"ok\"\n",
    "    print('\\n==== ok ====')\n",
    "    print(df.loc[adj_is_ok(df), 'adj_form_lower']\n",
    "          .astype('string').value_counts().nlargest(10).to_frame().reset_index()\n",
    "          .to_markdown(floatfmt=',.0f', intfmt=','))\n",
    "    df.loc[adj_is_ok(df), :] = df.loc[adj_is_ok(df), :].assign(\n",
    "        adj_form_lower='ok',\n",
    "        adj_lemma='ok')\n",
    "\n",
    "    # > variations on \"definitely\"\n",
    "    print('\\n==== definitely ====')\n",
    "    print(df.loc[adv_is_def(df), 'adv_form_lower']\n",
    "          .astype('string').value_counts().nlargest(10).to_frame().reset_index()\n",
    "          .to_markdown(floatfmt=',.0f', intfmt=','))\n",
    "    df.loc[adv_is_def(df), :] = df.loc[adv_is_def(df), :].assign(adv_form_lower='definitely',\n",
    "                                                                 adv_lemma='definitely')\n",
    "\n",
    "    # > drop any single character \"words\"\n",
    "    print(df.loc[df.adv_form_lower.str.contains(\n",
    "        r'^\\w\\W*$'), ['adv_form_lower', 'adj_form_lower']]\n",
    "        .astype('string').value_counts().nlargest(10).to_frame().reset_index()\n",
    "        .to_markdown(floatfmt=',.0f', intfmt=','))\n",
    "    print(df.loc[df.adj_form_lower.str.contains(\n",
    "        r'^\\w\\W*$'), ['adv_form_lower', 'adj_form_lower']]\n",
    "        .astype('string').value_counts().nlargest(10).to_frame().reset_index()\n",
    "        .to_markdown())\n",
    "    df = df.loc[~((df.adv_form_lower.str.contains(r'^\\w\\W*$'))\n",
    "                  | (df.adj_form_lower.str.contains(r'^\\w\\W*$'))), :]\n",
    "\n",
    "    # > delete remaining non-word characters (esp. `.` & `*`)\n",
    "    df = df.assign(\n",
    "        adv_form_lower=df.adv_form_lower.str.strip(\n",
    "            '-').str.replace(r'[^a-z0-9&-]', '', regex=True),\n",
    "        adv_lemma=df.adv_lemma.str.strip(\n",
    "            '-').str.replace(r'[^a-zA-Z0-9&-]', '', regex=True),\n",
    "        adj_form_lower=df.adj_form_lower.str.strip(\n",
    "            '-').str.replace(r'[^a-z0-9&-]', '', regex=True),\n",
    "        adj_lemma=df.adj_lemma.str.strip(\n",
    "            '-').str.replace(r'[^a-zA-Z0-9&-]', '', regex=True)\n",
    "    )\n",
    "    df = df.loc[~df.adv_form_lower.isin({'is', 'ie'}), :]\n",
    "    print('**** **** ****')\n",
    "\n",
    "    print(df.loc[(df.adv_form_lower.str.contains(r\"[^\\w'-]\", regex=True))\n",
    "                 | (df.adj_form_lower.str.contains(r\"[^\\w'-]\", regex=True)),\n",
    "                 ['adv_form_lower', 'adj_form_lower']]\n",
    "          .astype('string').value_counts()\n",
    "          .nlargest(10).to_frame().reset_index()\n",
    "          .to_markdown(floatfmt=',.0f', intfmt=',')\n",
    "          )\n",
    "    # print(df[df.adv_form_lower.str.contains(r\"[^\\w'-]\", regex=True)].value_counts(['adv_lemma', 'adv_form_lower','adj_form_lower']))\n",
    "    # print()\n",
    "    # print(df[df.adj_form_lower.str.contains(r\"[^\\w'-]\", regex=True)].value_counts(['adj_lemma', 'adj_form_lower','adv_form_lower']))\n",
    "    df.loc[:, ['adv_form_lower', 'adj_form_lower', 'adj_lemma', 'adv_lemma']\n",
    "           ] = df.loc[:, ['adv_form_lower', 'adj_form_lower', 'adj_lemma', 'adv_lemma']\n",
    "                      ].astype('category')\n",
    "    return df.convert_dtypes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping most bizarre...\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | 1/3rd            | smaller          |       1 |\n",
      "|  1 | a)               | sick             |       1 |\n",
      "|  2 | b)               | better           |       1 |\n",
      "|  3 | b)               | impossible       |       1 |\n",
      "|  4 | even             | 1\\/4-inch        |       1 |\n",
      "Dropping plain numerals as adjectives\n",
      "|    | adv_form_lower   |   adj_form_lower | text_window                                              |   count |\n",
      "|---:|:-----------------|-----------------:|:---------------------------------------------------------|--------:|\n",
      "|  0 | more             |              401 | be popping up in many more 401 -LRB- k -RRB- plans       |       1 |\n",
      "|  1 | mostly           |              401 | workers retire on all or mostly 401 -LRB- k -RRB- assets |       1 |\n",
      "Translating some known orthographic quirks...\n",
      "\n",
      "==== very ====\n",
      "| adv_form_lower   |   count |\n",
      "|:-----------------|--------:|\n",
      "| very             | 191,968 |\n",
      "| v                |      13 |\n",
      "| v.               |       8 |\n",
      "| verrrry          |       5 |\n",
      "| verrry           |       4 |\n",
      "| veeeery          |       3 |\n",
      "| verry            |       2 |\n",
      "| veeeeeery        |       2 |\n",
      "| veeery           |       1 |\n",
      "| veryyy           |       1 |\n",
      "\n",
      "==== ok ====\n",
      "|    | adj_form_lower   |   count |\n",
      "|---:|:-----------------|--------:|\n",
      "|  0 | okay             |     807 |\n",
      "|  1 | ok               |     801 |\n",
      "|  2 | ooky             |       8 |\n",
      "|  3 | o.k              |       4 |\n",
      "|  4 | o.k.             |       3 |\n",
      "\n",
      "==== definitely ====\n",
      "|    | adv_form_lower   |   count |\n",
      "|---:|:-----------------|--------:|\n",
      "|  0 | def.             |       1 |\n",
      "|  1 | def              |       1 |\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | a-               | ok               |       2 |\n",
      "|  1 | t                | interested       |       2 |\n",
      "|  2 | t                | lazy             |       1 |\n",
      "|  3 | n                | unreal           |       1 |\n",
      "|  4 | n                | worse            |       1 |\n",
      "|  5 | o                | dear             |       1 |\n",
      "|  6 | o                | negative         |       1 |\n",
      "|  7 | o                | nice             |       1 |\n",
      "|  8 | o                | unpleasant       |       1 |\n",
      "|  9 | t                | concerned        |       1 |\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | not              | b                |       2 |\n",
      "|  1 | so               | e                |       2 |\n",
      "|  2 | just             | b                |       1 |\n",
      "|  3 | relatively       | g                |       1 |\n",
      "|  4 | too              | h                |       1 |\n",
      "|  5 | so               | r                |       1 |\n",
      "|  6 | so               | q                |       1 |\n",
      "|  7 | so               | m                |       1 |\n",
      "|  8 | so               | h                |       1 |\n",
      "|  9 | so               | f                |       1 |\n",
      "**** **** ****\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | faintly          | s&m              |       1 |\n",
      "|  1 | more             | r&b              |       1 |\n",
      "|  2 | more             | r&b-oriented     |       1 |\n"
     ]
    }
   ],
   "source": [
    "pmir = remove_odd_orth_forms(pmir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping most bizarre...\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | more             | [...]            |       1 |\n",
      "Dropping plain numerals as adjectives\n",
      "|    | adv_form_lower   |   adj_form_lower | text_window                       |   count |\n",
      "|---:|:-----------------|-----------------:|:----------------------------------|--------:|\n",
      "|  0 | out              |                0 | Shanthakumaran Sreesanth no out 0 |       1 |\n",
      "Translating some known orthographic quirks...\n",
      "\n",
      "==== very ====\n",
      "| adv_form_lower   |   count |\n",
      "|:-----------------|--------:|\n",
      "| very             |   8,956 |\n",
      "\n",
      "==== ok ====\n",
      "|    | adj_form_lower   |   count |\n",
      "|---:|:-----------------|--------:|\n",
      "|  0 | ok               |      58 |\n",
      "|  1 | okay             |      51 |\n",
      "\n",
      "==== definitely ====\n",
      "| adv_form_lower   | count   |\n",
      "|------------------|---------|\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | c.               | bad              |       1 |\n",
      "|  1 | e                | detailed         |       1 |\n",
      "|  2 | i.               | many             |       1 |\n",
      "|  3 | l                | great            |       1 |\n",
      "|  4 | l                | impressed        |       1 |\n",
      "|  5 | n                | submissive       |       1 |\n",
      "|  6 | t                | conducive        |       1 |\n",
      "|  7 | t                | possible         |       1 |\n",
      "|  8 | t                | worthy           |       1 |\n",
      "|  9 | u                | equal            |       1 |\n",
      "|    | adv_form_lower   | adj_form_lower   |   count |\n",
      "|---:|:-----------------|:-----------------|--------:|\n",
      "|  0 | far              | f                |       1 |\n",
      "|  1 | not              | m                |       1 |\n",
      "|  2 | really           | a-               |       1 |\n",
      "|  3 | there            | m                |       1 |\n",
      "|  4 | too              | b                |       1 |\n",
      "**** **** ****\n",
      "| adv_form_lower   | adj_form_lower   | count   |\n",
      "|------------------|------------------|---------|\n"
     ]
    }
   ],
   "source": [
    "nmir = remove_odd_orth_forms(nmir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_id</th>\n",
       "      <th>token_str</th>\n",
       "      <th>pattern</th>\n",
       "      <th>category</th>\n",
       "      <th>adv_form</th>\n",
       "      <th>adj_form</th>\n",
       "      <th>text_window</th>\n",
       "      <th>mir_deprel</th>\n",
       "      <th>mir_lemma</th>\n",
       "      <th>adv_lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>mir_form</th>\n",
       "      <th>mir_index</th>\n",
       "      <th>adv_index</th>\n",
       "      <th>adj_index</th>\n",
       "      <th>mir_form_lower</th>\n",
       "      <th>adv_form_lower</th>\n",
       "      <th>adj_form_lower</th>\n",
       "      <th>bigram_lower</th>\n",
       "      <th>all_forms_lower</th>\n",
       "      <th>prev_form_lower</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apw_eng_19941111_0004_1:14-15-16</th>\n",
       "      <td>apw_eng_19941111_0004_1:15-16</td>\n",
       "      <td>after being locked out for 41 days , National ...</td>\n",
       "      <td>pos-mirror-R</td>\n",
       "      <td>POSmirror</td>\n",
       "      <td>too</td>\n",
       "      <td>happy</td>\n",
       "      <td>Hockey League players were all too happy Thurs...</td>\n",
       "      <td>advmod</td>\n",
       "      <td>all</td>\n",
       "      <td>too</td>\n",
       "      <td>...</td>\n",
       "      <td>all</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>all</td>\n",
       "      <td>too</td>\n",
       "      <td>happy</td>\n",
       "      <td>too_happy</td>\n",
       "      <td>all_too_happy</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19941111_0011_5:27-29-30</th>\n",
       "      <td>apw_eng_19941111_0011_5:29-30</td>\n",
       "      <td>Volpe and Poland were the only two of the 13 w...</td>\n",
       "      <td>pos-mirror-R</td>\n",
       "      <td>POSmirror</td>\n",
       "      <td>too</td>\n",
       "      <td>dangerous</td>\n",
       "      <td>government 's claim that both were too dangero...</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>both</td>\n",
       "      <td>too</td>\n",
       "      <td>...</td>\n",
       "      <td>both</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>both</td>\n",
       "      <td>too</td>\n",
       "      <td>dangerous</td>\n",
       "      <td>too_dangerous</td>\n",
       "      <td>both_too_dangerous</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19941111_0090_14:09-12-13</th>\n",
       "      <td>apw_eng_19941111_0090_14:12-13</td>\n",
       "      <td>`` It seems like the next meeting could always...</td>\n",
       "      <td>pos-mirror-R</td>\n",
       "      <td>POSmirror</td>\n",
       "      <td>most</td>\n",
       "      <td>important</td>\n",
       "      <td>the next meeting could always be the most impo...</td>\n",
       "      <td>advmod</td>\n",
       "      <td>always</td>\n",
       "      <td>most</td>\n",
       "      <td>...</td>\n",
       "      <td>always</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>always</td>\n",
       "      <td>most</td>\n",
       "      <td>important</td>\n",
       "      <td>most_important</td>\n",
       "      <td>always_most_important</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19941112_0323_28:08-16-17</th>\n",
       "      <td>apw_eng_19941112_0323_28:16-17</td>\n",
       "      <td>that was a big concern , for all of us -- that...</td>\n",
       "      <td>pos-mirror-L</td>\n",
       "      <td>POSmirror</td>\n",
       "      <td>so</td>\n",
       "      <td>appalling</td>\n",
       "      <td>big concern , for all of us -- that he would b...</td>\n",
       "      <td>dep</td>\n",
       "      <td>all</td>\n",
       "      <td>so</td>\n",
       "      <td>...</td>\n",
       "      <td>all</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>all</td>\n",
       "      <td>so</td>\n",
       "      <td>appalling</td>\n",
       "      <td>so_appalling</td>\n",
       "      <td>all_so_appalling</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19941113_0019_21:2-8-9</th>\n",
       "      <td>apw_eng_19941113_0019_21:8-9</td>\n",
       "      <td>Handling all of the egos involved was surprisi...</td>\n",
       "      <td>pos-mirror-R</td>\n",
       "      <td>POSmirror</td>\n",
       "      <td>surprisingly</td>\n",
       "      <td>easy</td>\n",
       "      <td>Handling all of the egos involved was surprisi...</td>\n",
       "      <td>advmod</td>\n",
       "      <td>all</td>\n",
       "      <td>surprisingly</td>\n",
       "      <td>...</td>\n",
       "      <td>all</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>all</td>\n",
       "      <td>surprisingly</td>\n",
       "      <td>easy</td>\n",
       "      <td>surprisingly_easy</td>\n",
       "      <td>all_surprisingly_easy</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        bigram_id  \\\n",
       "hit_id                                                              \n",
       "apw_eng_19941111_0004_1:14-15-16    apw_eng_19941111_0004_1:15-16   \n",
       "apw_eng_19941111_0011_5:27-29-30    apw_eng_19941111_0011_5:29-30   \n",
       "apw_eng_19941111_0090_14:09-12-13  apw_eng_19941111_0090_14:12-13   \n",
       "apw_eng_19941112_0323_28:08-16-17  apw_eng_19941112_0323_28:16-17   \n",
       "apw_eng_19941113_0019_21:2-8-9       apw_eng_19941113_0019_21:8-9   \n",
       "\n",
       "                                                                           token_str  \\\n",
       "hit_id                                                                                 \n",
       "apw_eng_19941111_0004_1:14-15-16   after being locked out for 41 days , National ...   \n",
       "apw_eng_19941111_0011_5:27-29-30   Volpe and Poland were the only two of the 13 w...   \n",
       "apw_eng_19941111_0090_14:09-12-13  `` It seems like the next meeting could always...   \n",
       "apw_eng_19941112_0323_28:08-16-17  that was a big concern , for all of us -- that...   \n",
       "apw_eng_19941113_0019_21:2-8-9     Handling all of the egos involved was surprisi...   \n",
       "\n",
       "                                        pattern   category      adv_form  \\\n",
       "hit_id                                                                     \n",
       "apw_eng_19941111_0004_1:14-15-16   pos-mirror-R  POSmirror           too   \n",
       "apw_eng_19941111_0011_5:27-29-30   pos-mirror-R  POSmirror           too   \n",
       "apw_eng_19941111_0090_14:09-12-13  pos-mirror-R  POSmirror          most   \n",
       "apw_eng_19941112_0323_28:08-16-17  pos-mirror-L  POSmirror            so   \n",
       "apw_eng_19941113_0019_21:2-8-9     pos-mirror-R  POSmirror  surprisingly   \n",
       "\n",
       "                                    adj_form  \\\n",
       "hit_id                                         \n",
       "apw_eng_19941111_0004_1:14-15-16       happy   \n",
       "apw_eng_19941111_0011_5:27-29-30   dangerous   \n",
       "apw_eng_19941111_0090_14:09-12-13  important   \n",
       "apw_eng_19941112_0323_28:08-16-17  appalling   \n",
       "apw_eng_19941113_0019_21:2-8-9          easy   \n",
       "\n",
       "                                                                         text_window  \\\n",
       "hit_id                                                                                 \n",
       "apw_eng_19941111_0004_1:14-15-16   Hockey League players were all too happy Thurs...   \n",
       "apw_eng_19941111_0011_5:27-29-30   government 's claim that both were too dangero...   \n",
       "apw_eng_19941111_0090_14:09-12-13  the next meeting could always be the most impo...   \n",
       "apw_eng_19941112_0323_28:08-16-17  big concern , for all of us -- that he would b...   \n",
       "apw_eng_19941113_0019_21:2-8-9     Handling all of the egos involved was surprisi...   \n",
       "\n",
       "                                  mir_deprel mir_lemma     adv_lemma  ...  \\\n",
       "hit_id                                                                ...   \n",
       "apw_eng_19941111_0004_1:14-15-16      advmod       all           too  ...   \n",
       "apw_eng_19941111_0011_5:27-29-30       nsubj      both           too  ...   \n",
       "apw_eng_19941111_0090_14:09-12-13     advmod    always          most  ...   \n",
       "apw_eng_19941112_0323_28:08-16-17        dep       all            so  ...   \n",
       "apw_eng_19941113_0019_21:2-8-9        advmod       all  surprisingly  ...   \n",
       "\n",
       "                                  mir_form mir_index  adv_index  adj_index  \\\n",
       "hit_id                                                                       \n",
       "apw_eng_19941111_0004_1:14-15-16       all        13         14         15   \n",
       "apw_eng_19941111_0011_5:27-29-30      both        26         28         29   \n",
       "apw_eng_19941111_0090_14:09-12-13   always         8         11         12   \n",
       "apw_eng_19941112_0323_28:08-16-17      all         7         15         16   \n",
       "apw_eng_19941113_0019_21:2-8-9         all         1          7          8   \n",
       "\n",
       "                                   mir_form_lower adv_form_lower  \\\n",
       "hit_id                                                             \n",
       "apw_eng_19941111_0004_1:14-15-16              all            too   \n",
       "apw_eng_19941111_0011_5:27-29-30             both            too   \n",
       "apw_eng_19941111_0090_14:09-12-13          always           most   \n",
       "apw_eng_19941112_0323_28:08-16-17             all             so   \n",
       "apw_eng_19941113_0019_21:2-8-9                all   surprisingly   \n",
       "\n",
       "                                  adj_form_lower       bigram_lower  \\\n",
       "hit_id                                                                \n",
       "apw_eng_19941111_0004_1:14-15-16           happy          too_happy   \n",
       "apw_eng_19941111_0011_5:27-29-30       dangerous      too_dangerous   \n",
       "apw_eng_19941111_0090_14:09-12-13      important     most_important   \n",
       "apw_eng_19941112_0323_28:08-16-17      appalling       so_appalling   \n",
       "apw_eng_19941113_0019_21:2-8-9              easy  surprisingly_easy   \n",
       "\n",
       "                                         all_forms_lower prev_form_lower  \n",
       "hit_id                                                                    \n",
       "apw_eng_19941111_0004_1:14-15-16           all_too_happy             all  \n",
       "apw_eng_19941111_0011_5:27-29-30      both_too_dangerous            were  \n",
       "apw_eng_19941111_0090_14:09-12-13  always_most_important             the  \n",
       "apw_eng_19941112_0323_28:08-16-17       all_so_appalling              be  \n",
       "apw_eng_19941113_0019_21:2-8-9     all_surprisingly_easy             was  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmir.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 293947 entries, pcc_eng_11_001.0326_x0000513_088:14-15-16 to pcc_eng_10_108.10108_x1747378_18:7-8-9\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count   Dtype   \n",
      "---  ------           --------------   -----   \n",
      " 0   bigram_id        293947 non-null  string  \n",
      " 1   token_str        293947 non-null  string  \n",
      " 2   pattern          293947 non-null  category\n",
      " 3   category         293947 non-null  category\n",
      " 4   neg_form         293947 non-null  string  \n",
      " 5   adv_form         293947 non-null  string  \n",
      " 6   adj_form         293947 non-null  string  \n",
      " 7   text_window      293947 non-null  string  \n",
      " 8   neg_deprel       293947 non-null  string  \n",
      " 9   neg_lemma        293947 non-null  string  \n",
      " 10  adv_lemma        293947 non-null  category\n",
      " 11  adj_lemma        293947 non-null  category\n",
      " 12  neg_index        293947 non-null  UInt16  \n",
      " 13  adv_index        293947 non-null  UInt16  \n",
      " 14  adj_index        293947 non-null  UInt16  \n",
      " 15  neg_form_lower   293947 non-null  string  \n",
      " 16  adv_form_lower   293947 non-null  category\n",
      " 17  adj_form_lower   293947 non-null  category\n",
      " 18  bigram_lower     293947 non-null  string  \n",
      " 19  all_forms_lower  293947 non-null  string  \n",
      " 20  prev_form_lower  293947 non-null  string  \n",
      "dtypes: UInt16(3), category(6), string(12)\n",
      "memory usage: 35.2 MB\n"
     ]
    }
   ],
   "source": [
    "nmir.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmir.loc[:, ['bigram_lower','all_forms_lower']] = update_form_combos(pmir.filter(like='lower')).loc[:, ['bigram_lower','all_forms_lower']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmir.loc[:, ['bigram_lower','all_forms_lower']] = update_form_combos(nmir.filter(like='lower')).loc[:, ['bigram_lower','all_forms_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_cat(df):\n",
    "    cat_cols = df.filter(regex=r'form|bigram|lemma|deprel|head').columns\n",
    "    df[cat_cols] = df[cat_cols].astype('category')\n",
    "    # df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmir = str_to_cat(pmir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmir = str_to_cat(nmir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_col_widths(df):\n",
    "    cols = df.copy().reset_index().columns\n",
    "    width_dict = (\n",
    "        {c: None for c in cols}\n",
    "        | {c: 22 for c in cols[cols.str.contains('_id')]}\n",
    "        | {c: 45 for c in cols[cols.str.contains('text')]}\n",
    "        | {c: 30 for c in cols[cols.str.contains('forms')]}\n",
    "        | {c: 60 for c in cols[cols.str.contains('_str')]})\n",
    "    return list(width_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POSmirror columns:\n",
      "▸ bigram_id\n",
      "▸ token_str\n",
      "▸ pattern\n",
      "▸ category\n",
      "▸ adv_form\n",
      "▸ adj_form\n",
      "▸ text_window\n",
      "▸ mir_deprel\n",
      "▸ mir_lemma\n",
      "▸ adv_lemma\n",
      "▸ adj_lemma\n",
      "▸ mir_form\n",
      "▸ mir_index\n",
      "▸ adv_index\n",
      "▸ adj_index\n",
      "▸ mir_form_lower\n",
      "▸ adv_form_lower\n",
      "▸ adj_form_lower\n",
      "▸ bigram_lower\n",
      "▸ all_forms_lower\n",
      "▸ prev_form_lower\n",
      "\n",
      "NEGmirror columns:\n",
      "▸ bigram_id\n",
      "▸ token_str\n",
      "▸ pattern\n",
      "▸ category\n",
      "▸ neg_form\n",
      "▸ adv_form\n",
      "▸ adj_form\n",
      "▸ text_window\n",
      "▸ neg_deprel\n",
      "▸ neg_lemma\n",
      "▸ adv_lemma\n",
      "▸ adj_lemma\n",
      "▸ neg_index\n",
      "▸ adv_index\n",
      "▸ adj_index\n",
      "▸ neg_form_lower\n",
      "▸ adv_form_lower\n",
      "▸ adj_form_lower\n",
      "▸ bigram_lower\n",
      "▸ all_forms_lower\n",
      "▸ prev_form_lower\n"
     ]
    }
   ],
   "source": [
    "print_iter(header = 'POSmirror columns:', iter_obj= pmir.columns.to_list())\n",
    "print_iter(header = 'NEGmirror columns:', iter_obj= nmir.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(df: pd.DataFrame,\n",
    "                format: str = 'grid',\n",
    "                limit_cols: bool = True):\n",
    "    if limit_cols and format != 'pipe':\n",
    "        col_widths_list = set_col_widths(df)\n",
    "    else:\n",
    "        col_widths_list = [None] * len(df.columns)\n",
    "    print(df.to_markdown(\n",
    "        floatfmt=',.0f', intfmt=',',\n",
    "        maxcolwidths=col_widths_list, \n",
    "        tablefmt=format\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| pattern      |     count |\n",
      "|:-------------|----------:|\n",
      "| pos-mirror-R | 1,364,547 |\n",
      "| pos-mirror-L |   373,490 |\n"
     ]
    }
   ],
   "source": [
    "show_sample(pmir.pattern.value_counts().to_frame(), limit_cols=False, format='pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| pattern      |   count |\n",
      "|:-------------|--------:|\n",
      "| neg-mirror-R | 216,886 |\n",
      "| neg-mirror-L |  77,061 |\n"
     ]
    }
   ],
   "source": [
    "show_sample(nmir.pattern.value_counts().to_frame(), limit_cols=False, format='pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGNOT=r\" (n[o']t) \"\n",
    "def embolden(series,\n",
    "            bold_regex=None):\n",
    "    bold_regex = re.compile(bold_regex) if bold_regex else REGNOT\n",
    "    return series.apply(\n",
    "        lambda x: bold_regex.sub(r' __`\\1`__ ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Sentences\n",
    "\n",
    "The following examples are all from the `POSmirror` data set which should not include any negative triggers. \n",
    "I believe the issue may be due to unexpected parses or cases where the negative trigger dependency is indirect or scopes over the identified positive trigger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = True\n",
      "  - Filter expression `token_str==  (n[o']t)  .* exactly ` matched zero rows. Filter not applied.\n",
      "  - ✓ Applied filter: `adv_form_lower==^exactly$`\n",
      "  - ✓ Applied filter: `pattern==.*L$`\n",
      "\n",
      "### 6 random rows matching filter(s) from `input frame`\n",
      "\n",
      "| hit_id                                    | mir_form_lower   | bigram_lower      | text_window                                                             | token_str                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|:------------------------------------------|:-----------------|:------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| nyt_eng_19990108_0419_27:5-8-9            | someone          | exactly_simpatico | and he is not someone who was __`exactly`__ simpatico with the Gramm of | and he is not someone who was __`exactly`__ simpatico with the Gramm of old .                                                                                                                                                                                                                                                                                                                  |\n",
      "| apw_eng_20070219_0931_10:22-24-25         | something        | exactly_alien     | of the dictum -- something not __`exactly`__ alien to him .             | Bellamy , who has a history of losing his temper , looks to be the first casualty of the dictum -- something not __`exactly`__ alien to him .                                                                                                                                                                                                                                                  |\n",
      "| pcc_eng_21_004.3936_x0054774_071:62-63-64 | something        | exactly_new       | so it 's not something __`exactly`__ new with him ] )                   | It 's also missing some of his earlier stuff , but the \" KMD \" ( Kurious and another random dude Doom was supposedly gonna make a new KMD album with [ another of the many rumored - but- never- seen Doom projects a la Madvillainy 2 & the Ghost collab - this was from like 2000 so it 's not something __`exactly`__ new with him ] ) track Sorcerors is on there , which is really nice . |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28876/2108415995.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  problems.loc[problems.token_str.str.contains(f'{REGNOT}.*{adv}')].assign(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = True\n",
      "  - Filter expression `token_str==  (n[o']t)  .* exactly ` matched zero rows. Filter not applied.\n",
      "  - ✓ Applied filter: `adv_form_lower==^exactly$`\n",
      "  - ✓ Applied filter: `pattern==.*R$`\n",
      "\n",
      "### 6 random rows matching filter(s) from `input frame`\n",
      "\n",
      "| hit_id                                   | mir_form_lower   | bigram_lower   | text_window                                             | token_str                                               |\n",
      "|:-----------------------------------------|:-----------------|:---------------|:--------------------------------------------------------|:--------------------------------------------------------|\n",
      "| pcc_eng_22_098.7323_x1579436_44:04-09-10 | all              | exactly_true   | Well , not all of these things are __`exactly`__ true . | Well , not all of these things are __`exactly`__ true . |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28876/2108415995.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  problems.loc[problems.token_str.str.contains(f'{REGNOT}.*{adv}')].assign(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = True\n",
      "  - Filter expression `token_str==  (n[o']t)  .* ever ` matched zero rows. Filter not applied.\n",
      "  - ✓ Applied filter: `adv_form_lower==^ever$`\n",
      "  - ✓ Applied filter: `pattern==.*L$`\n",
      "\n",
      "### 6 random rows matching filter(s) from `input frame`\n",
      "\n",
      "| hit_id                                  | mir_form_lower   | bigram_lower   | text_window                                                             | token_str                                                                                                                                                                                                                                                  |\n",
      "|:----------------------------------------|:-----------------|:---------------|:------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| pcc_eng_11_083.0903_x1328650_1:25-26-27 | everyone         | ever_involved  | admittedly not familiar with everyone __`ever`__ involved with the Wo W | There are other items in the patch files that look to be named after certain individuals , but I 'm admittedly not familiar with everyone __`ever`__ involved with the Wo W community , so I 'll leave it to you guys to let me know if I missed someone . |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28876/2108415995.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  problems.loc[problems.token_str.str.contains(f'{REGNOT}.*{adv}')].assign(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = True\n",
      "  - Filter expression `token_str==  (n[o']t)  .* ever ` matched zero rows. Filter not applied.\n",
      "  - ✓ Applied filter: `adv_form_lower==^ever$`\n",
      "  - ✓ Applied filter: `pattern==.*R$`\n",
      "\n",
      "### 6 random rows matching filter(s) from `input frame`\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28876/2108415995.py:12: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  problems.loc[problems.token_str.str.contains(f'{REGNOT}.*{adv}')].assign(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "boolean value of NA is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pat_suff \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     problems \u001b[38;5;241m=\u001b[39m sample_pickle(\n\u001b[1;32m      4\u001b[0m         data\u001b[38;5;241m=\u001b[39mpmir, sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, print_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m         filters\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_str== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREGNOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m .* \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmir_form_lower\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigram_lower\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_window\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_str\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m         sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_forms_lower\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mshow_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mproblems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREGNOT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.*\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43madv\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membolden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREGNOT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43madv\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtext_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membolden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREGNOT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43madv\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m) \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpipe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m, in \u001b[0;36mshow_sample\u001b[0;34m(df, format, limit_cols)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     col_widths_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_markdown\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloatfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,.0f\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxcolwidths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_widths_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtablefmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/pandas/core/frame.py:2757\u001b[0m, in \u001b[0;36mDataFrame.to_markdown\u001b[0;34m(self, buf, mode, index, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshowindex\u001b[39m\u001b[38;5;124m\"\u001b[39m, index)\n\u001b[1;32m   2756\u001b[0m tabulate \u001b[38;5;241m=\u001b[39m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtabulate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2757\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtabulate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2759\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/tabulate/__init__.py:2048\u001b[0m, in \u001b[0;36mtabulate\u001b[0;34m(tabular_data, headers, tablefmt, floatfmt, intfmt, numalign, stralign, missingval, showindex, disable_numparse, colalign, maxcolwidths, rowalign, maxheadercolwidths)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tabular_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2046\u001b[0m     tabular_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2048\u001b[0m list_of_lists, headers \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize_tabular_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtabular_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshowindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshowindex\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m list_of_lists, separating_lines \u001b[38;5;241m=\u001b[39m _remove_separating_lines(list_of_lists)\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxcolwidths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/tabulate/__init__.py:1471\u001b[0m, in \u001b[0;36m_normalize_tabular_data\u001b[0;34m(tabular_data, headers, showindex)\u001b[0m\n\u001b[1;32m   1469\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, headers))\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;66;03m#    rows = list(map(list, rows))\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_is_separating_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;66;03m# add or remove an index column\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m showindex_is_a_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(showindex) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/tabulate/__init__.py:1471\u001b[0m, in \u001b[0;36m_normalize_tabular_data.<locals>.<lambda>\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m   1469\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, headers))\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;66;03m#    rows = list(map(list, rows))\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m r: r \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_separating_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(r), rows))\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;66;03m# add or remove an index column\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m showindex_is_a_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(showindex) \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/tabulate/__init__.py:107\u001b[0m, in \u001b[0;36m_is_separating_line\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_separating_line\u001b[39m(row):\n\u001b[1;32m    105\u001b[0m     row_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(row)\n\u001b[1;32m    106\u001b[0m     is_sl \u001b[38;5;241m=\u001b[39m (row_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m row_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m--> 107\u001b[0m         (\u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SEPARATING_LINE)\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m SEPARATING_LINE)\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_sl\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/pandas/_libs/missing.pyx:388\u001b[0m, in \u001b[0;36mpandas._libs.missing.NAType.__bool__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: boolean value of NA is ambiguous"
     ]
    }
   ],
   "source": [
    "for adv in ['exactly', 'ever', 'necessarily', 'yet']:\n",
    "    for pat_suff in ['L', 'R']:\n",
    "        problems = sample_pickle(\n",
    "            data=pmir, sample_size=6, regex=True, print_sample=False,\n",
    "            filters=[f'token_str== {REGNOT} .* {adv} ',\n",
    "                    f'adv_form_lower==^{adv}$', \n",
    "                    f'pattern==.*{pat_suff}$'],\n",
    "            columns=['mir_form_lower', 'bigram_lower', 'text_window', 'token_str'],\n",
    "            sort_by='all_forms_lower')\n",
    "\n",
    "        show_sample(\n",
    "            problems.loc[problems.token_str.str.contains(f'{REGNOT}.*{adv}')].assign(\n",
    "                token_str=embolden(problems.token_str, f' ({REGNOT}|{adv}) '),\n",
    "                text_window=embolden(problems.text_window, f' ({REGNOT}|{adv}) ')\n",
    "            ),\n",
    "            format='pipe', limit_cols=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This could be dealt with by modifying the patterns (i.e. the `WITHOUT` clauses specifically) and rerunning everything, but\n",
    "  1. There's no telling how long that would take \n",
    "  2. verifying its accuracy is difficult\n",
    "  3. even with 100% accurate patterns for *correct* parses, there is no way to prevent or really even predict all possible *mis*parses\n",
    "- So there is a better way: \n",
    "  \n",
    "  The preponderance of positive data provides a large margin for additional data exclusions without unbalancing the samples---in fact, \n",
    "  it actually brings `[POSMIR,f1]` _closer_ to the negative sample size, `[NEGMIR, f1]`.\n",
    "\n",
    "  Therefore, it is possible to simply drop anything with a likely negation preceding the bigram, \n",
    "  regardless of the polarity environment the particular syntactic configuration creates, and call it a day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmir['adv_index'] = pd.to_numeric(pmir.index.to_series().str.split(':').str.get(-1).apply(lambda i: re.search(r'-(\\d+)-', i).group().strip('-')), downcast='unsigned')\n",
    "pmir['preceding_text'] = pmir.apply(lambda x: x.token_str.split()[:x.adv_index - 1], axis='columns').astype('string').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------------------+----------------+--------------------------------------------------------------+\n",
      "| hit_id                 | preceding_text                                | bigram_lower   | token_str                                                    |\n",
      "+========================+===============================================+================+==============================================================+\n",
      "| pcc_eng_23_038.1062_x0 | As can be seen , Lian and Brian 's episode    | too_familiar   | As can be seen , Lian and Brian 's episode will focus on the |\n",
      "| 599468_38:24-25-26     | will focus on the topic of autism , a subject |                | topic of autism , a subject Ler himself is all too familiar  |\n",
      "|                        | Ler himself is                                |                | with as he has family members afflicted with autism and he   |\n",
      "|                        |                                               |                | explains that it is partly because of this that he feels a   |\n",
      "|                        |                                               |                | strong need to raise awareness of the issue in Singapore .   |\n",
      "+------------------------+-----------------------------------------------+----------------+--------------------------------------------------------------+\n",
      "| pcc_eng_24_108.05817_x | Tosh was always                               | most_militant  | Tosh was always the most militant of the original Wailers    |\n",
      "| 1741307_08:3-5-6       |                                               |                | and this album reflects that outlook .                       |\n",
      "+------------------------+-----------------------------------------------+----------------+--------------------------------------------------------------+\n",
      "| pcc_eng_19_013.6113_x0 | So even                                       | as_simple      | So even something as simple as reminding a tenant what to do |\n",
      "| 203717_51:3-4-5        |                                               |                | and reporting any unusual - coloured water or temperatures   |\n",
      "|                        |                                               |                | to a landlord can help .                                     |\n",
      "+------------------------+-----------------------------------------------+----------------+--------------------------------------------------------------+\n",
      "| pcc_eng_28_018.9428_x0 | If surgery is necessary , Bushnell said , the | very_good      | If surgery is necessary , Bushnell said , the technology is  |\n",
      "| 290042_162:26-27-28    | technology is so advanced , and the surgery   |                | so advanced , and the surgery is so common , that the        |\n",
      "|                        | is so common , that the outcome is            |                | outcome is often very good , giving people like the Johnsons |\n",
      "|                        |                                               |                | and the Woznickis a chance to further enjoy the hearts they  |\n",
      "|                        |                                               |                | gave to one another more than half a century ago .           |\n",
      "+------------------------+-----------------------------------------------+----------------+--------------------------------------------------------------+\n",
      "| pcc_eng_22_048.9377_x0 | If someone would                              | so_kind        | If someone would be so kind as to link me to a Google Doc    |\n",
      "| 774719_5:2-5-6         |                                               |                | containing , IDK , maybe a .zip file with a Pokemon Platinum |\n",
      "|                        |                                               |                | rom inside , that would be just plain amazing and I 'd love  |\n",
      "|                        |                                               |                | it .                                                         |\n",
      "+------------------------+-----------------------------------------------+----------------+--------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "show_sample(pmir[['preceding_text', 'bigram_lower', 'token_str']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| hit_id                                   | preceding_text                                                                                                                                                                                                              | bigram_lower             | token_str                                                                                                                                                                                                                                                 |\n",
      "|:-----------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| pcc_eng_11_072.9334_x1164379_04:30-32-33 | This pierces ( but does not completely shatter ) what I believe has been a six-decade embargo by prison administrators to hold back information on how many prisoners are or                                                | nt_religious             | This pierces ( but does not completely shatter ) what I believe has been a six-decade embargo by prison administrators to hold back information on how many prisoners are or are n't religious .                                                          |\n",
      "| pcc_eng_20_036.2341_x0569583_6:37-38-39  | I recently completed a collection of poems based on Old Testament stories called ' Ichabod and Other Voices ' and am interested in the way in which the mundane and inconsequential co-exists alongside the deeply profound | historically_significant | I recently completed a collection of poems based on Old Testament stories called ' Ichabod and Other Voices ' and am interested in the way in which the mundane and inconsequential co-exists alongside the deeply profound or historically significant . |\n",
      "| pcc_eng_00_012.3002_x0182514_19:6-7-8    | I 'm glad you 're                                                                                                                                                                                                           | equally_prosperous       | I 'm glad you 're both equally prosperous in your careers .                                                                                                                                                                                               |\n",
      "| pcc_eng_16_078.3949_x1252557_31:13-14-15 | There are many ways of refining coconut oil made from copra ,                                                                                                                                                               | more_healthy             | There are many ways of refining coconut oil made from copra , some more healthy than others .                                                                                                                                                             |\n",
      "| pcc_eng_16_042.5589_x0672568_30:26-28-29 | The loss of use coverages of homeowners policies issued by insurers that do not use the standard ISO HO 2 or HO 3 policy forms often                                                                                        | substantially_similar    | The loss of use coverages of homeowners policies issued by insurers that do not use the standard ISO HO 2 or HO 3 policy forms often are substantially similar to the standard form policies .                                                            |\n"
     ]
    }
   ],
   "source": [
    "show_sample(pmir[['preceding_text', 'bigram_lower', 'token_str']].sample(5), format='pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28876/3396796712.py:1: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  pmir['after_neg'] = pmir.preceding_text.str.lower().str.contains(r\"\\b(no|n[o']t|no(body| one|thing|where)|(rare|scarce|bare|hard)ly|seldom|without|never)\\b\", regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| hit_id                 | preceding_text                                | bigram_lower         | token_str                                                    |\n",
      "+========================+===============================================+======================+==============================================================+\n",
      "| pcc_eng_01_026.0198_x0 | Sitting around in 95 degree weather just      | necessarily_fun      | Sitting around in 95 degree weather just waiting on some     |\n",
      "| 404751_13:22-23-24     | waiting on some sweaty dudes to show up all   |                      | sweaty dudes to show up all at once is not easy or           |\n",
      "|                        | at once is not easy                           |                      | necessarily fun .                                            |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_25_047.4015_x0 | I may not have much of a social life , but I  | even_better          | I may not have much of a social life , but I have something  |\n",
      "| 751293_059:14-15-16    | have                                          |                      | even better .                                                |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_26_051.2414_x0 | It 's not comical to assume someone would     | more_productive      | It 's not comical to assume someone would be more productive |\n",
      "| 812297_37:07-10-11     |                                               |                      | at something when doing a thing they 're historically good   |\n",
      "|                        |                                               |                      | at than something they have no experience in .               |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_04_031.9275_x0 | This is not to be confused with wheel         | completely_different | This is not to be confused with wheel balancing , which is   |\n",
      "| 499730_07:13-14-15     | balancing , which is                          |                      | something completely different , though they both affect the |\n",
      "|                        |                                               |                      | ride and handling of a motor vehicle .                       |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_02_053.0154_x0 | But once this practical possibility no longer | purely_philosophical | But once this practical possibility no longer seems feasible |\n",
      "| 841379_26:15-17-18     | seems feasible , then this approach would     |                      | , then this approach would either be purely philosophical or |\n",
      "|                        | either                                        |                      | it would turn against the potentialities of the present An   |\n",
      "|                        |                                               |                      | Aging India : Perspectives , Prospects , and Policies .      |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| apw_eng_20000228_0098_ | `` Some were not for something that           | medically_necessary  | `` Some were not for something that was medically necessary  |\n",
      "| 11:06-09-10            |                                               |                      | , '' Klimmerman .                                            |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_14_090.1559_x1 | Some who remain without coverage simply       | nt_eligible          | Some who remain without coverage simply are n't eligible     |\n",
      "| 441113_69:1-8-9        |                                               |                      | under the new law : undocumented immigrants and people who   |\n",
      "|                        |                                               |                      | live in states that opt out of the Medicaid expansion .      |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_27_084.2710_x1 | Rarely a person is so traumatized , or so     | so_stuck             | Rarely a person is so traumatized , or so full of entities , |\n",
      "| 346849_177:14-15-16    | full of entities ,                            |                      | or so stuck for some other reason , that it is difficult to  |\n",
      "|                        |                                               |                      | work with him or her .                                       |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_04_020.3016_x0 | When a basic U-lock wo n't do , riders have   | most_convenient      | When a basic U-lock wo n't do , riders have been turning to  |\n",
      "| 311689_02:29-30-31     | been turning to chain - based locks that      |                      | chain - based locks that protect your bike , but are n't     |\n",
      "|                        | protect your bike , but are n't exactly the   |                      | exactly the easiest or most convenient thing to carry around |\n",
      "|                        | easiest                                       |                      | .                                                            |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_23_040.4716_x0 | \" When I said people who look like me -- I    | necessarily_smart    | \" When I said people who look like me -- I meant the people  |\n",
      "| 637758_21:20-21-22     | meant the people who do n't look strong       |                      | who do n't look strong or necessarily smart , or like they   |\n",
      "|                        |                                               |                      | 're in control etc.                                          |\n",
      "+------------------------+-----------------------------------------------+----------------------+--------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "pmir['after_neg'] = pmir.preceding_text.str.lower().str.contains(r\"\\b(no|n[o']t|no(body| one|thing|where)|(rare|scarce|bare|hard)ly|seldom|without|never)\\b\", regex=True)\n",
    "show_sample(pmir.loc[pmir.after_neg, ['preceding_text', 'bigram_lower', 'token_str']].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| hit_id                                    | preceding_text                                                                                                                                   | bigram_lower       | token_str                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|:------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| pcc_eng_13_095.7120_x1530725_085:17-19-20 | I promise you , the followers of the dark were __`never`__ stronger , more numerous , or                                                         | openly_influential | I promise you , the followers of the dark were never stronger , more numerous , or more openly influential than in the early decades of the 20th century .                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "| pcc_eng_28_001.2565_x0004261_3:10-11-12   | \" I do n't think there 's ever been                                                                                                              | so_qualified       | \" I do n't think there 's ever been someone so qualified to hold this office . \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "| pcc_eng_14_016.3271_x0247591_48:28-30-31  | I owe my salvation to __`nothing`__ more than a sudden change of heart on the part of the pack which , having spotted an even more detestable or | more_edible        | I owe my salvation to nothing more than a sudden change of heart on the part of the pack which , having spotted an even more detestable or apparently more edible figure on the sidewalk on Teatinos , left off attacking me and threw themselves upon him with even greater fury , so that the unfortunate man was forced to fight them off like the hapless prey he was , signalling his distress with much waving of arms and loud cries , attracting no more assistance on my part than he himself had demonstrated just a few moments before , with myself in the role of the helpless victim . |\n",
      "| pcc_eng_15_056.4401_x0896152_040:08-12-13 | If you ca n't take it anymore or you 're                                                                                                         | not_able           | If you ca n't take it anymore or you 're simply not able to physically , hire a landscaper or a contractor .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
      "| pcc_eng_24_028.3067_x0441558_23:11-12-13  | You open it if it does n't seem too big                                                                                                          | too_overwhelming   | You open it if it does n't seem too big or too overwhelming .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "| nyt_eng_20010925_0328_126:05-11-12        | you may not need all of the flour-butter mixture                                                                                                 | only_enough        | you may not need all of the flour-butter mixture ; only enough to thickened the broth slightly .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n"
     ]
    }
   ],
   "source": [
    "some_neg_ex = pmir.loc[pmir.after_neg, ['preceding_text', 'bigram_lower', 'token_str']].sample(6)\n",
    "show_sample(some_neg_ex.assign(\n",
    "    preceding_text=embolden(some_neg_ex.preceding_text, \n",
    "                            f' ({REGNOT}|nobody|nothing|never|none|no) ')\n",
    "    ), format='pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* $1,490,579$ tokens in `POSmirror` hits not preceded by negation\n",
      "  > - I.e. what would remain if _all_ potential contaminants were excluded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > - _247,458_ potential exclusions\n",
      "* $293,947$ tokens in `NEGmirror` hits\n",
      "* Remaining Sample Size Discrepancy: $1,196,632$\n"
     ]
    }
   ],
   "source": [
    "print(f'* ${pmir.after_neg.value_counts()[False]:,}$ tokens in `POSmirror` hits not preceded by negation')\n",
    "print('  > - I.e. what would remain if _all_ potential contaminants were excluded')\n",
    "print(f'  > - _{pmir.after_neg.value_counts()[True]:,}_ potential exclusions')\n",
    "print(f'* ${len(nmir):,}$ tokens in `NEGmirror` hits')\n",
    "print(f'* Remaining Sample Size Discrepancy: ${pmir.after_neg.value_counts()[False] - len(nmir):,}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Negation Removals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### For 868+ frequency filtered ad* forms \n",
    "  \n",
    "  _Without considering any upper case_\n",
    "  * ~~__1,457,913__ tokens in `POSmirror` hits not preceded by negation~~\n",
    "      * ~~I.e. what would remain if _all_ potential contaminants were excluded~~\n",
    "      * ~~_217,588_ potential exclusions~~\n",
    "  ---\n",
    "  _Without considering fully upper case triggers_\n",
    "  * ~~__1,460,126__ tokens in `POSmirror` hits not preceded by negation~~\n",
    "  * ~~I.e. what would remain if _all_ potential contaminants were excluded~~\n",
    "  * ~~_215,375_ potential exclusions~~\n",
    "  ---\n",
    "  _Normalized for case first, but not catching negation at very end of preceding text (no whitespace following)_\n",
    "  * ~~**1,459,568** tokens in `POSmirror` hits not preceded by negation~~\n",
    "  > - ~~I.e. what would remain if _all_ potential contaminants were excluded~~\n",
    "  > - ~~_215,933_ potential exclusions~~\n",
    "  * ~~Updated difference in hit subtotals: **1,174,133**~~\n",
    "  * $285,435$ tokens in `NEGmirror` hits\n",
    "  ---\n",
    "  **_Fixed to catch even `preceding_text` final negative triggers_**\n",
    "  * ~~**1,455,547** tokens in `POSmirror` hits not preceded by negation~~\n",
    "  > - ~~I.e. what would remain if _all_ potential contaminants were excluded~~\n",
    "  > - ~~_219,954_ potential exclusions~~\n",
    "  * $285,435$ tokens in `NEGmirror` hits\n",
    "  * ~~Remaining Sample Size Discrepancy: **1,170,112**~~\n",
    "  ---\n",
    "  **Strengthened even furthre to catch negative adverbs and \"without\" and triggers at the _beginning_ of the `preceding_text`**\n",
    "  * $1,434,420$ tokens in `POSmirror` hits not preceded by negation\n",
    "  > - I.e. what would remain if _all_ potential contaminants were excluded\n",
    "  > - _241,081_ potential exclusions\n",
    "  * $285,435$ tokens in `NEGmirror` hits\n",
    "  * Remaining Sample Size Discrepancy: $1,148,985$\n",
    "\n",
    "\n",
    "### For 7+ frequency filtered ad\\* forms\n",
    "\n",
    "_Without orthography adjustments_\n",
    "* ~~**1,487,458** tokens in `POSmirror` hits not preceded by negation~~\n",
    "    > - ~~I.e. what would remain if _all_ potential contaminants were excluded~~\n",
    "    > - ~~_250,661_ potential exclusions~~\n",
    "* ~~**293,964** tokens in `NEGmirror` hits~~\n",
    "* ~~Remaining Sample Size Discrepancy: **1,193,494**~~\n",
    "\n",
    "_**With** orthography adjustments/filtering_\n",
    "* $1,490,579$ tokens in `POSmirror` hits not preceded by negation\n",
    "  > - I.e. what would remain if _all_ potential contaminants were excluded\n",
    "  > - _247,458_ potential exclusions\n",
    "* $293,947$ tokens in `NEGmirror` hits\n",
    "* Remaining Sample Size Discrepancy: $1,196,632$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1490579 entries, apw_eng_19941111_0004_1:14-15-16 to pcc_eng_26_108.10246_x1747457_06:6-8-9\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Non-Null Count    Dtype   \n",
      "---  ------           --------------    -----   \n",
      " 0   bigram_id        1490579 non-null  category\n",
      " 1   token_str        1490579 non-null  string  \n",
      " 2   pattern          1490579 non-null  category\n",
      " 3   category         1490579 non-null  category\n",
      " 4   adv_form         1490579 non-null  category\n",
      " 5   adj_form         1490579 non-null  category\n",
      " 6   text_window      1490579 non-null  string  \n",
      " 7   mir_deprel       1490579 non-null  category\n",
      " 8   mir_lemma        1490579 non-null  category\n",
      " 9   adv_lemma        1490579 non-null  category\n",
      " 10  adj_lemma        1490579 non-null  category\n",
      " 11  mir_form         1490579 non-null  category\n",
      " 12  mir_index        1490579 non-null  UInt16  \n",
      " 13  adv_index        1490579 non-null  UInt16  \n",
      " 14  adj_index        1490579 non-null  UInt16  \n",
      " 15  mir_form_lower   1490579 non-null  category\n",
      " 16  adv_form_lower   1490579 non-null  category\n",
      " 17  adj_form_lower   1490579 non-null  category\n",
      " 18  bigram_lower     1490579 non-null  category\n",
      " 19  all_forms_lower  1490579 non-null  category\n",
      " 20  prev_form_lower  1490579 non-null  category\n",
      " 21  preceding_text   1490579 non-null  string  \n",
      "dtypes: UInt16(3), category(16), string(3)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "enforced_pos= pmir.loc[~pmir.after_neg, :'preceding_text']\n",
    "enforced_pos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = False\n",
      "  - ✓ Applied filter: `adv_form_lower==exactly`\n",
      "\n",
      "### 10 random rows matching filter(s) from `input frame`\n",
      "\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| hit_id                 | all_forms_lower            | token_str                                                    |\n",
      "+========================+============================+==============================================================+\n",
      "| pcc_eng_04_104.1453_x1 | something_exactly_opposite | misrepresentation is the kind of thing you and Goerzen do    |\n",
      "| 666254_08:20-21-22     |                            | where you outright lie and say that somebody said something  |\n",
      "|                        |                            | __`exactly`__ opposite to what they actually said . >        |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_07_101.4447_x1 | all_exactly_alike          | The Man Whose Teeth Were All Exactly Alike                   |\n",
      "| 623411_110:6-7-8       |                            |                                                              |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_12_044.6593_x0 | or_exactly_enough          | The cabbage that accompanied a Muscovy duck breast was       |\n",
      "| 705786_21:19-20-21     |                            | braised with too much cinnamon , some thought , or           |\n",
      "|                        |                            | __`exactly`__ enough , thought others .                      |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_12_098.6128_x1 | or_exactly_alike           | 3 . sameness : the fact or condition of being the same or    |\n",
      "| 577474_138:13-14-15    |                            | __`exactly`__ alike                                          |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_14_021.3253_x0 | many_exactly_alike         | We will advance all reasonable errors , spelling mistakes ,  |\n",
      "| 328325_67:43-47-48     |                            | run-on facilities , receive your personal , we will send you |\n",
      "|                        |                            | a good link where you can say clip awkward sentences and     |\n",
      "|                        |                            | omit any unfamiliar words , phrases , per day , many of      |\n",
      "|                        |                            | which nigeria __`exactly`__ alike .                          |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_19_078.5861_x1 | all_exactly_opposite       | All are __`exactly`__ opposite of the best strategies for    |\n",
      "| 253458_06:1-3-4        |                            | learning .                                                   |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_20_063.3872_x1 | all_exactly_alike          | \" Communism , the New Deal , Fascism , Nazism , \" he wrote   |\n",
      "| 007929_21:37-38-39     |                            | in his Memoirs , \" are merely so-many trade-names for        |\n",
      "|                        |                            | collectivist Statism , like the trade-names for tooth -      |\n",
      "|                        |                            | pastes which are all __`exactly`__ alike except for the      |\n",
      "|                        |                            | flavouring . \" ...                                           |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_22_049.7992_x0 | all_exactly_right          | If you 're an online supplier and you want to win a customer |\n",
      "| 788588_32:29-30-31     |                            | 's repeat business , you need to make sure prices , products |\n",
      "|                        |                            | and service are all __`exactly`__ right .                    |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_25_078.3454_x1 | or_exactly_music-related   | This may be tangentially music-related or __`exactly`__      |\n",
      "| 252325_2:6-7-8         |                            | music-related , depending on where you 're sitting :         |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n",
      "| pcc_eng_25_097.4853_x1 | everything_exactly_perfect | And , it 's time - aware enough ( by stating \" every day \" ) |\n",
      "| 561507_25:34-37-38     |                            | that your mind can accept and acknowledge linear progress    |\n",
      "|                        |                            | while your internal vibration reorients to trust that        |\n",
      "|                        |                            | everything IS already __`exactly`__ perfect in this moment , |\n",
      "|                        |                            | even if it feels like it 's not - because , again , change   |\n",
      "|                        |                            | takes time , and lasting change does n't always appear in    |\n",
      "|                        |                            | the external world right away .                              |\n",
      "+------------------------+----------------------------+--------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "adv = 'exactly'\n",
    "new_exactly_ex = sample_pickle(\n",
    "    data=enforced_pos,\n",
    "    print_sample=False, sample_size=10,\n",
    "    columns=['all_forms_lower', 'token_str'],\n",
    "    filters=[f'adv_form_lower=={adv}'],\n",
    ")\n",
    "\n",
    "show_sample(new_exactly_ex.assign(token_str=embolden(new_exactly_ex.token_str, r' (exactly) ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = False\n",
      "  - ✓ Applied filter: `adv_form_lower==exactly`\n",
      "  - ✓ Applied filter: `pattern==pos-mirror-R`\n",
      "\n",
      "### 8 random rows matching filter(s) from `input frame`\n",
      "\n",
      "| hit_id                                    | all_forms_lower          | text_window                                                                 | token_str                                                                                                                                                                         |\n",
      "|:------------------------------------------|:-------------------------|:----------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| pcc_eng_03_074.5040_x1190619_26:1-6-7     | or_exactly_correct       | Or maybe the result wasnt __`exactly`__ correct because he was n't          | Or maybe the result wasnt __`exactly`__ correct because he was n't where he thought he was .                                                                                      |\n",
      "| pcc_eng_14_008.0840_x0114338_03:08-09-10  | or_exactly_false         | is either __`exactly`__ true or __`exactly`__ false is called a statement   | A sentence which is either __`exactly`__ true or __`exactly`__ false is called a statement .                                                                                      |\n",
      "| pcc_eng_13_079.2956_x1265427_12:10-12-13  | or_exactly_funny         | suposed to be fun or more __`exactly`__ funny .                             | This is MLS it is suposed to be fun or more __`exactly`__ funny .                                                                                                                 |\n",
      "| pcc_eng_14_047.8465_x0756834_55:4-5-6     | both_exactly_right       | Representative Murphy is both __`exactly`__ right and spectacularly wrong . | Representative Murphy is both __`exactly`__ right and spectacularly wrong .                                                                                                       |\n",
      "| pcc_eng_22_073.0874_x1165093_82:13-15-16  | everything_exactly_right | to make sure that everything is __`exactly`__ right , especially with our   | He works closely with our nurses and doctors to make sure that everything is __`exactly`__ right , especially with our children .                                                 |\n",
      "| pcc_eng_23_094.7577_x1515449_43:32-33-34  | all_exactly_right        | vigilante killer \" are all __`exactly`__ right .                            | The emphasis on \" unlawful use of violence , \" the evocation of \" vigilantism , \" and the description of Tiller 's killer as a \" vigilante killer \" are all __`exactly`__ right . |\n",
      "| pcc_eng_03_019.3127_x0296060_12:12-13-14  | always_exactly_right     | - THE CHURCH IS ALWAYS EXACTLY RIGHT .                                      | Let people get it through their heads - THE CHURCH IS ALWAYS EXACTLY RIGHT .                                                                                                      |\n",
      "| pcc_eng_20_092.6565_x1480742_101:24-25-26 | all_exactly_same         | are messaging it are all __`exactly`__ same as their last time              | The product mix they chose and the level of price increase , the markets they chose and how they are messaging it are all __`exactly`__ same as their last time .                 |\n",
      "\n",
      "- *filtering rows...*\n",
      "  - regex parsing = False\n",
      "  - ✓ Applied filter: `adv_form_lower==exactly`\n",
      "  - ✓ Applied filter: `pattern==pos-mirror-L`\n",
      "\n",
      "### 8 random rows matching filter(s) from `input frame`\n",
      "\n",
      "| hit_id                                    | all_forms_lower            | text_window                                                                   | token_str                                                                                                                                                                   |\n",
      "|:------------------------------------------|:---------------------------|:------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| apw_eng_20070219_0931_10:22-24-25         | something_exactly_alien    | of the dictum -- something not __`exactly`__ alien to him .                   | Bellamy , who has a history of losing his temper , looks to be the first casualty of the dictum -- something not __`exactly`__ alien to him .                               |\n",
      "| pcc_eng_02_048.1298_x0762484_04:1-2-3     | everybody_exactly_alike    | Everybody __`exactly`__ alike . \"                                             | Everybody __`exactly`__ alike . \"                                                                                                                                           |\n",
      "| pcc_eng_05_069.4747_x1107978_086:1-2-3    | everybody_exactly_alike    | Everybody __`exactly`__ alike . \"                                             | Everybody __`exactly`__ alike . \"                                                                                                                                           |\n",
      "| pcc_eng_06_038.9890_x0614425_107:3-6-7    | all_exactly_alike          | They are all of them __`exactly`__ alike , and there is                       | They are all of them __`exactly`__ alike , and there is not one of them can be eaten .                                                                                      |\n",
      "| pcc_eng_04_104.1453_x1666254_08:20-21-22  | something_exactly_opposite | say that somebody said something __`exactly`__ opposite to what they actually | misrepresentation is the kind of thing you and Goerzen do where you outright lie and say that somebody said something __`exactly`__ opposite to what they actually said . > |\n",
      "| pcc_eng_15_106.5980_x1707092_015:16-17-18 | something_exactly_related  | , you offered them something __`exactly`__ related to the post they           | What if , instead of pointing visitors to a semi-related resource , you offered them something __`exactly`__ related to the post they just read ?                           |\n",
      "| pcc_eng_08_106.0266_x1700594_12:1-2-3     | everything_exactly_same    | Everything __`exactly`__ same , minimum 290 people                            | Everything __`exactly`__ same , minimum 290 people dead .                                                                                                                   |\n",
      "| pcc_eng_11_064.7133_x1031125_24:1-2-3     | everything_exactly_same    | Everything __`exactly`__ same , minimum two hundred                           | Everything __`exactly`__ same , minimum two hundred ninety people dead .                                                                                                    |\n"
     ]
    }
   ],
   "source": [
    "for pat_suff in ['R', 'L']:\n",
    "    new_exactly_ex = sample_pickle(\n",
    "        data=enforced_pos, sample_size=8,\n",
    "        print_sample=False, sort_by='adj_form_lower',\n",
    "        columns=['all_forms_lower', 'text_window', 'token_str'],\n",
    "        filters=[f'adv_form_lower=={adv}', \n",
    "                f'pattern==pos-mirror-{pat_suff}'],\n",
    "    )\n",
    "\n",
    "    show_sample(new_exactly_ex.assign(\n",
    "        text_window=embolden(new_exactly_ex.text_window, f' ({adv}) '),\n",
    "        token_str=embolden(new_exactly_ex.token_str, f' ({adv}) ')\n",
    "    ), format='pipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicated `text_window`+`all_forms_lower`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforced_pos['utt_len']= pd.to_numeric(enforced_pos.token_str.apply(lambda x: int(len(x.split()))), downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Potental Removals (no `utt_len` filter applied)\n",
      "▸ token_str:   15,394\n",
      "▸ text_window: 36,885\n",
      "▸ both:        14,861\n"
     ]
    }
   ],
   "source": [
    "dups_token_str = enforced_pos.loc[enforced_pos.duplicated(subset=['token_str', 'all_forms_lower']), ['all_forms_lower', 'token_str','text_window', 'utt_len']]\n",
    "dups_text_window = enforced_pos.loc[enforced_pos.duplicated(subset=['text_window', 'all_forms_lower']), ['all_forms_lower', 'token_str', 'text_window','utt_len']]\n",
    "dups_both = enforced_pos.loc[enforced_pos.duplicated(subset=['text_window', 'token_str', 'all_forms_lower']), ['all_forms_lower', 'token_str','text_window', 'utt_len']]\n",
    "# show_sample(dups.loc[dups.utt_len>=80, :].sort_values(['utt_len', 'token_str']).head(6))\n",
    "print_iter([f'token_str:   {len(dups_token_str):,}', \n",
    "            f'text_window: {len(dups_text_window):,}',\n",
    "            f'both:        {len(dups_both):,}'], header='Potental Removals (no `utt_len` filter applied)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`text_window` duplicates when restricted to 20+ tokens in `token_str`: 16,570\n"
     ]
    }
   ],
   "source": [
    "print(f'`text_window` duplicates when restricted to 20+ tokens in `token_str`: {len(dups_text_window.loc[dups_text_window.utt_len>=20, :]):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These duplicates are retained since it's too messy to separate the clearly carbon copy utterances from plausible genuine production. \n",
    "\n",
    "Highly Suspicious\n",
    "\n",
    "|     | text_window                                               | \\#tokens in sentence | \\#duplications |\n",
    "|----:|:----------------------------------------------------------|---------------------:|---------------:|\n",
    "|   7 | _Everything you see here is absolutely FREE to watch ._   |                   10 |             60 |\n",
    "|  17 | _All of Swedenborg 's works are well worth reading ._     |                   10 |             43 |\n",
    "|  60 | _2 fig Something quintessentially Canadian ._             |                    6 |             20 |\n",
    "|  65 | _Everybody is Super Heady in our sandbox ._               |                    8 |             19 |\n",
    "| 119 | _Because sometimes , 140 characters just is n't enough ._ |                   10 |             13 |\n",
    "| 143 | _\" Or maybe stupid , \" Ebenezar countered_                |                    9 |             11 |\n",
    "| 188 | _Because Sometimes 140 Characters Just Is n't Enough_     |                    8 |              9 |\n",
    "| 283 | _There 's something very wrong with our pterosaurs ._     |                    9 |              7 |\n",
    "| 290 | _The plaid decoration is all very good ._                 |                    8 |              7 |\n",
    "| 482 | _wrap up something as special as she is [...]_            |                    9 |              5 |\n",
    "\n",
    "Plausible Production\n",
    "\n",
    "|    | text_window                                    | \\#tokens in sentence | \\#duplications |\n",
    "|---:|:-----------------------------------------------|---------------------:|---------------:|\n",
    "|  2 | _Something was n't right ._                    |                    5 |            118 |\n",
    "|  3 | _But I 'm sure everything will be just fine ._ |                   10 |            102 |\n",
    "|  8 | _It 's all very confusing ._                   |                    6 |             56 |\n",
    "|  6 | _We should all be so lucky ._                  |                    7 |             69 |\n",
    "| 10 | _It 's all very exciting ._                    |                    6 |             54 |\n",
    "| 19 | _Something is definitely wrong ._              |                    5 |             41 |\n",
    "| 20 | _Looking for something more specific ?_        |                    6 |             40 |\n",
    "| 25 | _Both are equally important ._                 |                    5 |             33 |\n",
    "| 31 | _My cup is always half full ._                 |                    7 |             30 |\n",
    "| 39 | _If only it were all so simple !_              |                    9 |             24 |\n",
    "| 48 | _Everything is not awesome ._                  |                    5 |             23 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_window</th>\n",
       "      <th>utt_len</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some of your changes are now live .</td>\n",
       "      <td>8</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And now for something completely different .</td>\n",
       "      <td>7</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Something was n't right .</td>\n",
       "      <td>5</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But I 'm sure everything will be just fine .</td>\n",
       "      <td>10</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And now for something completely different ...</td>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>\" And now for something completely different . \"</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>\" All of us at Whataburger are so happy to get...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7964</th>\n",
       "      <td>\" All along I have been completely consistent ...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8972</th>\n",
       "      <td>\" Abusers are often very adept at identifying ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>\" ... something truly special . \"</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11125 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text_window  utt_len  count\n",
       "0                   Some of your changes are now live .        8    384\n",
       "1          And now for something completely different .        7    122\n",
       "2                             Something was n't right .        5    118\n",
       "3          But I 'm sure everything will be just fine .       10    102\n",
       "4        And now for something completely different ...        7     75\n",
       "...                                                 ...      ...    ...\n",
       "7968   \" And now for something completely different . \"       17      1\n",
       "7965  \" All of us at Whataburger are so happy to get...       16      1\n",
       "7964  \" All along I have been completely consistent ...       18      1\n",
       "8972  \" Abusers are often very adept at identifying ...       10      1\n",
       "3034                  \" ... something truly special . \"        7      1\n",
       "\n",
       "[11125 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dups_text_window.loc[(dups_text_window.utt_len < 20), :].value_counts(['text_window', 'utt_len']).to_frame().reset_index().sort_values(['count','text_window', ], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weed_windows(df):\n",
    "    return pd.concat(\n",
    "        [df.loc[df.utt_len < 20, :],\n",
    "         df.loc[df.utt_len >= 20, :].drop_duplicates(\n",
    "            subset=['text_window', 'all_forms_lower'])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1490579 entries, apw_eng_19941111_0004_1:14-15-16 to pcc_eng_26_108.10246_x1747457_06:6-8-9\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Non-Null Count    Dtype   \n",
      "---  ------           --------------    -----   \n",
      " 0   bigram_id        1490579 non-null  category\n",
      " 1   token_str        1490579 non-null  string  \n",
      " 2   pattern          1490579 non-null  category\n",
      " 3   category         1490579 non-null  category\n",
      " 4   adv_form         1490579 non-null  category\n",
      " 5   adj_form         1490579 non-null  category\n",
      " 6   text_window      1490579 non-null  string  \n",
      " 7   mir_deprel       1490579 non-null  category\n",
      " 8   mir_lemma        1490579 non-null  category\n",
      " 9   adv_lemma        1490579 non-null  category\n",
      " 10  adj_lemma        1490579 non-null  category\n",
      " 11  mir_form         1490579 non-null  category\n",
      " 12  mir_index        1490579 non-null  UInt16  \n",
      " 13  adv_index        1490579 non-null  UInt16  \n",
      " 14  adj_index        1490579 non-null  UInt16  \n",
      " 15  mir_form_lower   1490579 non-null  category\n",
      " 16  adv_form_lower   1490579 non-null  category\n",
      " 17  adj_form_lower   1490579 non-null  category\n",
      " 18  bigram_lower     1490579 non-null  category\n",
      " 19  all_forms_lower  1490579 non-null  category\n",
      " 20  prev_form_lower  1490579 non-null  category\n",
      " 21  preceding_text   1490579 non-null  string  \n",
      " 22  utt_len          1490579 non-null  int16   \n",
      "dtypes: UInt16(3), category(16), int16(1), string(3)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "enforced_pos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,475,109 hits remaining in `POSmirror` set after additional filtering\n",
      "(15,470 hits removed as duplicates.)\n"
     ]
    }
   ],
   "source": [
    "new_pmir = weed_windows(enforced_pos)\n",
    "print(f'{len(new_pmir):,} hits remaining in `POSmirror` set after additional filtering', \n",
    "      f'({len(enforced_pos) - len(new_pmir):,} hits removed as duplicates.)', \n",
    "      sep='\\n')\n",
    "# show_sample(new_pmir.sample(6)[['all_forms_lower', 'text_window']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1475109 entries, apw_eng_19941111_0090_14:09-12-13 to pcc_eng_26_108.10246_x1747457_06:6-8-9\n",
      "Data columns (total 23 columns):\n",
      " #   Column           Non-Null Count    Dtype   \n",
      "---  ------           --------------    -----   \n",
      " 0   bigram_id        1475109 non-null  category\n",
      " 1   token_str        1475109 non-null  string  \n",
      " 2   pattern          1475109 non-null  category\n",
      " 3   category         1475109 non-null  category\n",
      " 4   adv_form         1475109 non-null  category\n",
      " 5   adj_form         1475109 non-null  category\n",
      " 6   text_window      1475109 non-null  string  \n",
      " 7   mir_deprel       1475109 non-null  category\n",
      " 8   mir_lemma        1475109 non-null  category\n",
      " 9   adv_lemma        1475109 non-null  category\n",
      " 10  adj_lemma        1475109 non-null  category\n",
      " 11  mir_form         1475109 non-null  category\n",
      " 12  mir_index        1475109 non-null  UInt16  \n",
      " 13  adv_index        1475109 non-null  UInt16  \n",
      " 14  adj_index        1475109 non-null  UInt16  \n",
      " 15  mir_form_lower   1475109 non-null  category\n",
      " 16  adv_form_lower   1475109 non-null  category\n",
      " 17  adj_form_lower   1475109 non-null  category\n",
      " 18  bigram_lower     1475109 non-null  category\n",
      " 19  all_forms_lower  1475109 non-null  category\n",
      " 20  prev_form_lower  1475109 non-null  category\n",
      " 21  preceding_text   1475109 non-null  string  \n",
      " 22  utt_len          1475109 non-null  int16   \n",
      "dtypes: UInt16(3), category(16), int16(1), string(3)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "new_pmir.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `trigger_lower` column to `POSmirror` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bigram_id', 'token_str', 'pattern', 'category', 'adv_form', 'adj_form',\n",
       "       'text_window', 'mir_deprel', 'mir_lemma', 'adv_lemma', 'adj_lemma',\n",
       "       'mir_form', 'mir_index', 'adv_index', 'adj_index', 'mir_form_lower',\n",
       "       'adv_form_lower', 'adj_form_lower', 'bigram_lower', 'all_forms_lower',\n",
       "       'trigger_lower'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pmir = new_pmir.loc[:, :'all_forms_lower']\n",
    "new_pmir['trigger_lower'] = new_pmir['mir_form_lower'].astype('category')\n",
    "new_pmir.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated `POSmirror` hits dataframe saved as:\\ \n",
      "  `/share/compling/data/sanpi/4_post-processed/POSmirror/LimitedPOS-trigger-bigrams_frq-thrMIN-7.35f.pkl.gz`\n"
     ]
    }
   ],
   "source": [
    "new_path = path_dict['POSmirror'].with_name('LimitedPOS-'+path_dict['POSmirror'].name)\n",
    "\n",
    "if not new_path.is_file():\n",
    "    \n",
    "    new_pmir.loc[:, ].to_pickle(new_path)\n",
    "    print(f'Updated `POSmirror` hits dataframe saved as:\\ \\n  `{new_path}`')\n",
    "else: \n",
    "    print(f'Updated `POSmirror` hits dataframe already exists:\\ \\n  `{new_path}`')\n",
    "    print('\\n```shell')\n",
    "    !ls -ho {new_path}\n",
    "    print('```')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplication from `NEGmirror` as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 293947 entries, pcc_eng_11_001.0326_x0000513_088:14-15-16 to pcc_eng_10_108.10108_x1747378_18:7-8-9\n",
      "Data columns (total 21 columns):\n",
      " #   Column           Non-Null Count   Dtype   \n",
      "---  ------           --------------   -----   \n",
      " 0   bigram_id        293947 non-null  category\n",
      " 1   token_str        293947 non-null  string  \n",
      " 2   pattern          293947 non-null  category\n",
      " 3   category         293947 non-null  category\n",
      " 4   neg_form         293947 non-null  category\n",
      " 5   adv_form         293947 non-null  category\n",
      " 6   adj_form         293947 non-null  category\n",
      " 7   text_window      293947 non-null  string  \n",
      " 8   neg_deprel       293947 non-null  category\n",
      " 9   neg_lemma        293947 non-null  category\n",
      " 10  adv_lemma        293947 non-null  category\n",
      " 11  adj_lemma        293947 non-null  category\n",
      " 12  neg_index        293947 non-null  UInt16  \n",
      " 13  adv_index        293947 non-null  UInt16  \n",
      " 14  adj_index        293947 non-null  UInt16  \n",
      " 15  neg_form_lower   293947 non-null  category\n",
      " 16  adv_form_lower   293947 non-null  category\n",
      " 17  adj_form_lower   293947 non-null  category\n",
      " 18  bigram_lower     293947 non-null  category\n",
      " 19  all_forms_lower  293947 non-null  category\n",
      " 20  prev_form_lower  293947 non-null  category\n",
      "dtypes: UInt16(3), category(16), string(2)\n",
      "memory usage: 169.5 MB\n"
     ]
    }
   ],
   "source": [
    "nmir.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------+-------------------------------+-----------------------------------------------+\n",
      "| hit_id                 | trigger_lower   | all_forms_lower               | text_window                                   |\n",
      "+========================+=================+===============================+===============================================+\n",
      "| pcc_eng_15_002.5425_x0 | never           | never_entirely_separate       | the two worlds can never be entirely separate |\n",
      "| 024650_37:6-8-9        |                 |                               | , the overlap in                              |\n",
      "+------------------------+-----------------+-------------------------------+-----------------------------------------------+\n",
      "| pcc_eng_10_038.8129_x0 | nor             | nor_epistemically_transparent | constrained in its options nor epistemically  |\n",
      "| 611668_40:13-14-15     |                 |                               | transparent , as the examples                 |\n",
      "+------------------------+-----------------+-------------------------------+-----------------------------------------------+\n",
      "| pcc_eng_07_023.7148_x0 | never           | never_so_cold                 | bitterly that they had never been so cold . [ |\n",
      "| 367258_118:33-35-36    |                 |                               | 1 ]                                           |\n",
      "+------------------------+-----------------+-------------------------------+-----------------------------------------------+\n",
      "| nyt_eng_19990830_0387_ | no              | no_more_accurate              | garbage '' and `` no more accurate than       |\n",
      "| 4:42-43-44             |                 |                               | information from Web                          |\n",
      "+------------------------+-----------------+-------------------------------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "nmir['utt_len']= nmir.token_str.apply(lambda x: len(x.split()))\n",
    "new_nmir = weed_windows(nmir)\n",
    "new_nmir = new_nmir.loc[:, :'all_forms_lower']\n",
    "new_nmir['trigger_lower'] = new_nmir.neg_form_lower.astype('category')\n",
    "show_sample(new_nmir.sample(4)[['trigger_lower', 'all_forms_lower', 'text_window']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* 293,947 original hits in `NEGmirror` (`NEGmirror/trigger-bigrams_frq-thrMIN-7.35f.pkl.gz`)\n",
      "* 289,759 hits remaining in `NEGmirror` set after additional filtering of duplicate hits\n",
      "  (4,188 hits removed as duplicates.)\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n* {len(nmir):,} original hits in `NEGmirror` (`{path_dict[\"NEGmirror\"].relative_to(POST_PROC_DIR)}`)')\n",
    "print(f'* {len(new_nmir):,} hits remaining in `NEGmirror` set after additional filtering of duplicate hits')\n",
    "print(f'  ({len(nmir) - len(new_nmir):,} hits removed as duplicates.)', \n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated `NEGmirror` hits dataframe saved as:\\ \n",
      "  `/share/compling/data/sanpi/4_post-processed/NEGmirror/LimitedNEG-trigger-bigrams_frq-thrMIN-7.35f.pkl.gz`\n"
     ]
    }
   ],
   "source": [
    "new_path = path_dict['NEGmirror'].with_name('LimitedNEG-'+path_dict['NEGmirror'].name)\n",
    "if not new_path.is_file():\n",
    "    \n",
    "    new_nmir.to_pickle(new_path)\n",
    "    print(f'Updated `NEGmirror` hits dataframe saved as:\\ \\n  `{new_path}`')\n",
    "else: \n",
    "    print(f'Updated `NEGmirror` hits dataframe already exists:\\ \\n  `{new_path}`')\n",
    "    print('\\n```shell')\n",
    "    !ls -ho {new_path}\n",
    "    print('```')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Removing Additional Duplication\n",
    "\n",
    "* $1,472,077$ hits remaining in `POSmirror` set after additional filtering\\\n",
    "  (15,381 hits removed as duplicates.)\n",
    "\n",
    "* $289,776$ hits remaining in `NEGmirror` set after additional filtering of duplicate hits\\\n",
    "  (4,188 hits removed as duplicates.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-sanpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
