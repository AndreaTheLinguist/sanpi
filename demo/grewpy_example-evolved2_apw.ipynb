{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\"original copied from https://github.com/grew-nlp/grewpy/blob/master/examples/test_corpus.py\"\"\"\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "# from grewpy import Graph, CorpusDraft, Request, Corpus, request_counter\n",
    "from grewpy import (Corpus, \n",
    "                    # CorpusDraft, Graph, \n",
    "                    Request, request_counter)\n",
    "from grewpy.grew import GrewError as GrewError\n",
    "\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join( os.path.dirname(__file__), \"../\"))) # Use local grew lib\n",
    "\n",
    "_META_TUP = namedtuple(\n",
    "    'meta_info', \n",
    "    ['sent_id', 'doc_id', 'sent_int', 'prev_id', 'prev_text', 'next_id', 'next_text'])\n",
    "\n",
    "def corpus_from_path(path):\n",
    "    return Corpus(str(path))\n",
    "\n",
    "def docs(expr):\n",
    "\n",
    "    try:\n",
    "        print(expr.__doc__)\n",
    "    except AttributeError:\n",
    "        print('None')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "connected to port: 8888\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "conllu_path = Path(\n",
    "    \"data/corpora/gitrepo_puddin/2smallest.conll/apw_eng_199911.conllu\"\n",
    "    # \"data/corpora/gitrepo_puddin/2smallest.conll/nyt_eng_200405.conllu\"\n",
    "    )\n",
    "co = corpus_from_path(conllu_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Should be able to set this up to have `conllu_path` (and `pat_path`?)\n",
    " as input, and run it in parallel on a list of files,\n",
    " even files from different directories\n",
    "\n",
    " ...did I just rewrite the entire subset code today? 🤦‍♀️"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(\"\\n=============== len ===============\")\n",
    "print(f\"sentence count in {conllu_path.name} = {len(co)}\")\n",
    "print(request_counter())\n",
    "\n",
    "print(f\"len(co[0]) = {len(co[0])}\")\n",
    "print(f\"len(co[-1]) = {len(co[-1])}\")\n",
    "print(f\"[len(g) for g in co[-3:]] = {[len(g) for g in co[-3:]]}\")\n",
    "# other forms co[-3:-1], co[1:7:2], ..."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=============== len ===============\n",
      "sentence count in apw_eng_199911.conllu = 1147\n",
      "1\n",
      "len(co[0]) = 33\n",
      "len(co[-1]) = 10\n",
      "[len(g) for g in co[-3:]] = [9, 31, 10]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(\"\\n=============== Count request in a corpus ===============\")\n",
    "for xpos in (\"RB.*\", \"JJ.*\"):\n",
    "    # xpos=\"RB.*\"\n",
    "    if xpos.startswith('RB'):\n",
    "        print('# ADVERBS')\n",
    "    elif xpos.startswith('JJ'):\n",
    "        print('# ADJECTIVES')\n",
    "\n",
    "    req = Request(f'X[xpos=re\"{xpos}\"]')\n",
    "\n",
    "    print(\" ----- basic count -----\")\n",
    "    print(f\"total {xpos} in {conllu_path.name} = \", co.count(req))\n",
    "\n",
    "    print(\" ----- count with clustering -----\")\n",
    "    print(f\"total {xpos} in {conllu_path.name}, clustered by exact POS:\")\n",
    "    # print(request_counter())\n",
    "    print(pd.Series(co.count(req, [\"X.xpos\"])).to_frame().rename(columns={0:'total'}).to_markdown(), '\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=============== Count request in a corpus ===============\n",
      "# ADVERBS\n",
      " ----- basic count -----\n",
      "total RB.* in apw_eng_199911.conllu =  840\n",
      " ----- count with clustering -----\n",
      "total RB.* in apw_eng_199911.conllu, clustered by exact POS:\n",
      "|     |   total |\n",
      "|:----|--------:|\n",
      "| RBS |      10 |\n",
      "| RBR |      32 |\n",
      "| RB  |     798 | \n",
      "\n",
      "# ADJECTIVES\n",
      " ----- basic count -----\n",
      "total JJ.* in apw_eng_199911.conllu =  1809\n",
      " ----- count with clustering -----\n",
      "total JJ.* in apw_eng_199911.conllu, clustered by exact POS:\n",
      "|     |   total |\n",
      "|:----|--------:|\n",
      "| JJS |      55 |\n",
      "| JJR |      66 |\n",
      "| JJ  |    1688 | \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#* ALL bigrams\n",
    "req = Request('ADJ [xpos=re\"JJ.?\"];'\n",
    "              'mod: ADJ -[advmod]-> ADV;'\n",
    "              'ADV < ADJ'\n",
    "              )\n",
    "print(str(req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];mod: ADJ -[advmod]-> ADV;ADV < ADJ}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pat_path = Path('Pat/advadj/all-RB-JJs.pat')\n",
    "pat_str = pat_path.read_text(encoding='utf8')\n",
    "print(pat_str)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern { \n",
      "    ADJ [xpos=re\"JJ.?\"]; \n",
      "    mod: ADJ -[advmod]-> ADV;  \n",
      "    ADV < ADJ\n",
      "}\n",
      "\n",
      "% will match e.g. `not uninteresting`\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ⚠️ Just running the raw pattern file text will result in an error:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(str(Request(pat_str)))\n",
    "try: \n",
    "    co.count(Request(pat_str))\n",
    "except: \n",
    "    print('ERROR! Bad request. (handled to prevent cancelation of following cells)')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {pattern { \n",
      "    ADJ [xpos=re\"JJ.?\"];mod: ADJ -[advmod]-> ADV;ADV < ADJ\n",
      "}\n",
      "\n",
      "% will match e.g. `not uninteresting`}\n",
      "ERROR! Bad request. (handled to prevent cancelation of following cells)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def grewpize_pat(raw_text): \n",
    "    return ''.join(line.strip() for line in raw_text.split('{', 1)[1].split('}',1)[0].strip().splitlines())\n",
    "clean_str = grewpize_pat(pat_str)\n",
    "print(clean_str.replace(';', ';\\n'))\n",
    "print('# actual form:')\n",
    "print(clean_str)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ADJ [xpos=re\"JJ.?\"];\n",
      "mod: ADJ -[advmod]-> ADV;\n",
      "ADV < ADJ\n",
      "# actual form:\n",
      "ADJ [xpos=re\"JJ.?\"];mod: ADJ -[advmod]-> ADV;ADV < ADJ\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "read_req = Request(clean_str)\n",
    "print(str(read_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];mod: ADJ -[advmod]-> ADV;ADV < ADJ}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# or, all in one go: \n",
    "full_read_req = Request(grewpize_pat(pat_path.read_text(encoding='utf8')))\n",
    "str(full_read_req) == str(read_req)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "co.count(read_req)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(\"\\n=============== Count `ADV ADJ` bigrams ===============\")\n",
    "print(f\"total `ADV ADJ` bigrams in {conllu_path.name}: {co.count(req)}\")\n",
    "print(\"\\n----- count with clustering -----\")\n",
    "print(f\"`ADV ADJ` bigrams in {conllu_path.name}, clustered by ADV lemma:\")\n",
    "# print(json.dumps(co.count(req, [\"ADV.lemma\"]), indent=2))\n",
    "pd.Series(co.count(req, [\"ADV.lemma\"])).to_frame().reset_index().rename(\n",
    "    columns={'index':'adverb', 0:'total_bigrams'}\n",
    "    ).sort_values('total_bigrams', ascending=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=============== Count `ADV ADJ` bigrams ===============\n",
      "total `ADV ADJ` bigrams in apw_eng_199911.conllu: 80\n",
      "\n",
      "----- count with clustering -----\n",
      "`ADV ADJ` bigrams in apw_eng_199911.conllu, clustered by ADV lemma:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adverb</th>\n",
       "      <th>total_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>more</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>most</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>too</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>very</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>so</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>as</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>how</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reportedly</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>long</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nearly</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>heavily</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fully</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>widely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ethnically</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>entirely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>immediately</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>best</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>barely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>constitutionally</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>little</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>increasingly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>justifiably</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>keenly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>less</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>much</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>once</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>only</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pretty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>racially</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>slightly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>still</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>allegedly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              adverb  total_bigrams\n",
       "15              more             12\n",
       "14              most              9\n",
       "2                too              7\n",
       "1               very              6\n",
       "4                 so              5\n",
       "31                as              5\n",
       "23               how              3\n",
       "6         reportedly              3\n",
       "7             really              3\n",
       "16              long              2\n",
       "12            nearly              2\n",
       "24           heavily              1\n",
       "25             fully              1\n",
       "0             widely              1\n",
       "26        ethnically              1\n",
       "27          entirely              1\n",
       "22       immediately              1\n",
       "29              best              1\n",
       "30            barely              1\n",
       "32              also              1\n",
       "28  constitutionally              1\n",
       "17            little              1\n",
       "21      increasingly              1\n",
       "20       justifiably              1\n",
       "19            keenly              1\n",
       "18              less              1\n",
       "13              much              1\n",
       "11              once              1\n",
       "10              only              1\n",
       "9             pretty              1\n",
       "8           racially              1\n",
       "5           slightly              1\n",
       "3              still              1\n",
       "33         allegedly              1"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "print(f\"Top 10 `ADV ADJ` bigrams in {conllu_path.name}\")\n",
    "pd.json_normalize(co.count(req, [\"ADV.lemma\", \"ADJ.lemma\"]), sep='_').transpose(\n",
    "    ).rename(columns={0:'total'}).nlargest(10, 'total')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 10 `ADV ADJ` bigrams in apw_eng_199911.conllu\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reportedly_close</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_much</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very_long</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>too_high</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so_bad</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>most_valuable</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>most_important</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more_expensive</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more_difficult</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_admired</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  total\n",
       "reportedly_close      3\n",
       "how_much              3\n",
       "very_long             2\n",
       "too_high              2\n",
       "so_bad                2\n",
       "most_valuable         2\n",
       "most_important        2\n",
       "more_expensive        2\n",
       "more_difficult        2\n",
       "long_admired          2"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "match_list = co.search(req)\n",
    "\n",
    "print(\"\\n=============== `ADV ADJ` bigram match info ===============\")\n",
    "pd.json_normalize(match_list, sep='_')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=============== `ADV ADJ` bigram match info ===============\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>matching_nodes_ADV</th>\n",
       "      <th>matching_nodes_ADJ</th>\n",
       "      <th>matching_edges_mod_source</th>\n",
       "      <th>matching_edges_mod_label</th>\n",
       "      <th>matching_edges_mod_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0059_18</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>advmod</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apw_eng_19991101_0059_16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>advmod</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apw_eng_19991101_0059_15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>advmod</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apw_eng_19991101_0059_5</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>advmod</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apw_eng_19991101_0059_5</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>advmod</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>apw_eng_19991101_0006_20</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>advmod</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>apw_eng_19991101_0006_8</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>advmod</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>apw_eng_19991101_0005_14</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>advmod</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>apw_eng_19991101_0002_19</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>advmod</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>apw_eng_19991101_0001_8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>advmod</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sent_id matching_nodes_ADV matching_nodes_ADJ  \\\n",
       "0   apw_eng_19991101_0059_18                 23                 24   \n",
       "1   apw_eng_19991101_0059_16                  4                  5   \n",
       "2   apw_eng_19991101_0059_15                  5                  6   \n",
       "3    apw_eng_19991101_0059_5                 28                 29   \n",
       "4    apw_eng_19991101_0059_5                 31                 32   \n",
       "..                       ...                ...                ...   \n",
       "75  apw_eng_19991101_0006_20                 28                 29   \n",
       "76   apw_eng_19991101_0006_8                 10                 11   \n",
       "77  apw_eng_19991101_0005_14                  9                 10   \n",
       "78  apw_eng_19991101_0002_19                 13                 14   \n",
       "79   apw_eng_19991101_0001_8                  4                  5   \n",
       "\n",
       "   matching_edges_mod_source matching_edges_mod_label  \\\n",
       "0                         24                   advmod   \n",
       "1                          5                   advmod   \n",
       "2                          6                   advmod   \n",
       "3                         29                   advmod   \n",
       "4                         32                   advmod   \n",
       "..                       ...                      ...   \n",
       "75                        29                   advmod   \n",
       "76                        11                   advmod   \n",
       "77                        10                   advmod   \n",
       "78                        14                   advmod   \n",
       "79                         5                   advmod   \n",
       "\n",
       "   matching_edges_mod_target  \n",
       "0                         23  \n",
       "1                          4  \n",
       "2                          5  \n",
       "3                         28  \n",
       "4                         31  \n",
       "..                       ...  \n",
       "75                        28  \n",
       "76                        10  \n",
       "77                         9  \n",
       "78                        13  \n",
       "79                         4  \n",
       "\n",
       "[80 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def gen_conllus(match_list, corpus):\n",
    "    \n",
    "    # for sent in match_list:\n",
    "    for i, sent in enumerate(match_list):\n",
    "        parse = corpus.get(sent['sent_id'])\n",
    "        if i < 3:\n",
    "            print(parse.to_conll())\n",
    "        yield parse.to_conll()+'\\n'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "conllu_gen = gen_conllus(match_list, co)\n",
    "\n",
    "subset_dir = conllu_path.parent.joinpath(f'subset_{pat_path.parent.stem}')\n",
    "if not subset_dir.is_dir(): \n",
    "    subset_dir.mkdir()\n",
    "    \n",
    "subset_path = subset_dir.joinpath(f'{pat_path.stem}+{conllu_path.name}')\n",
    "subset_path.write_text('\\n'.join(conllu_gen), encoding='utf8')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# sent_id = apw_eng_19991101_0059_18\n",
      "# text = Katz , an adviser to local governments and an expert on stadium financing , has tried to position himself as a moderate best able to build on the economic recovery generated under popular two-term Mayor Edward G. Rendell .\n",
      "1\tKatz\tKatz\t_\tNNP\t_\t16\tnsubj\t16:nsubj\t_\n",
      "2\t,\t,\t_\t,\t_\t0\t-\t0:-\t_\n",
      "3\tan\ta\t_\tDT\t_\t4\tdet\t4:det\t_\n",
      "4\tadviser\tadviser\t_\tNN\t_\t1\tappos\t1:appos\t_\n",
      "5\tto\tto\t_\tTO\t_\t4\tprep\t4:prep\t_\n",
      "6\tlocal\tlocal\t_\tJJ\t_\t7\tamod\t7:amod\t_\n",
      "7\tgovernments\tgovernment\t_\tNNS\t_\t5\tpobj\t5:pobj\t_\n",
      "8\tand\tand\t_\tCC\t_\t4\tcc\t4:cc\t_\n",
      "9\tan\ta\t_\tDT\t_\t10\tdet\t10:det\t_\n",
      "10\texpert\texpert\t_\tNN\t_\t4\tconj\t4:conj\t_\n",
      "11\ton\ton\t_\tIN\t_\t10\tprep\t10:prep\t_\n",
      "12\tstadium\tstadium\t_\tNN\t_\t13\tnn\t13:nn\t_\n",
      "13\tfinancing\tfinancing\t_\tNN\t_\t11\tpobj\t11:pobj\t_\n",
      "14\t,\t,\t_\t,\t_\t0\t-\t0:-\t_\n",
      "15\thas\thas\t_\tAUXZ\t_\t16\tdep\t16:dep\t_\n",
      "16\ttried\ttry\t_\tVBN\t_\t0\troot\t0:root\t_\n",
      "17\tto\tto\t_\tTO\t_\t18\taux\t18:aux\t_\n",
      "18\tposition\tposition\t_\tVB\t_\t16\txcomp\t16:xcomp\t_\n",
      "19\thimself\thimself\t_\tPRP\t_\t18\tdobj\t18:dobj\t_\n",
      "20\tas\tas\t_\tIN\t_\t18\tprep\t18:prep\t_\n",
      "21\ta\ta\t_\tDT\t_\t22\tdet\t22:det\t_\n",
      "22\tmoderate\tmoderate\t_\tJJ\t_\t20\tpobj\t20:pobj\t_\n",
      "23\tbest\tbest\t_\tRBS\t_\t24\tadvmod\t24:advmod\t_\n",
      "24\table\table\t_\tJJ\t_\t22\tamod\t22:amod\t_\n",
      "25\tto\tto\t_\tTO\t_\t26\taux\t26:aux\t_\n",
      "26\tbuild\tbuild\t_\tVB\t_\t24\txcomp\t24:xcomp\t_\n",
      "27\ton\ton\t_\tIN\t_\t26\tprep\t26:prep\t_\n",
      "28\tthe\tthe\t_\tDT\t_\t30\tdet\t30:det\t_\n",
      "29\teconomic\teconomic\t_\tJJ\t_\t30\tamod\t30:amod\t_\n",
      "30\trecovery\trecovery\t_\tNN\t_\t27\tpobj\t27:pobj\t_\n",
      "31\tgenerated\tgenerate\t_\tVBN\t_\t30\tpartmod\t30:partmod\t_\n",
      "32\tunder\tunder\t_\tIN\t_\t31\tprep\t31:prep\t_\n",
      "33\tpopular\tpopular\t_\tJJ\t_\t38\tamod\t38:amod\t_\n",
      "34\ttwo-term\ttwo-term\t_\tJJ\t_\t38\tamod\t38:amod\t_\n",
      "35\tMayor\tMayor\t_\tNNP\t_\t38\tnn\t38:nn\t_\n",
      "36\tEdward\tEdward\t_\tNNP\t_\t38\tnn\t38:nn\t_\n",
      "37\tG.\tG.\t_\tNNP\t_\t38\tnn\t38:nn\t_\n",
      "38\tRendell\tRendell\t_\tNNP\t_\t32\tpobj\t32:pobj\t_\n",
      "39\t.\t.\t_\t.\t_\t0\t-\t0:-\t_\n",
      "\n",
      "# sent_id = apw_eng_19991101_0059_16\n",
      "# text = Katz is  -LBQ-  very seasoned ,  -RDQ-  said Democratic political consultant Larry Ceisler .\n",
      "1\tKatz\tKatz\t_\tNNP\t_\t5\tnsubj\t5:nsubj\t_\n",
      "2\tis\tbe\t_\tVBZ\t_\t5\tcop\t5:cop\t_\n",
      "3\t``\t``\t_\t``\t_\t0\t-\t0:-\t_\n",
      "4\tvery\tvery\t_\tRB\t_\t5\tadvmod\t5:advmod\t_\n",
      "5\tseasoned\tseasoned\t_\tJJ\t_\t8\tccomp\t8:ccomp\t_\n",
      "6\t,\t,\t_\t,\t_\t0\t-\t0:-\t_\n",
      "7\t''\t''\t_\t''\t_\t0\t-\t0:-\t_\n",
      "8\tsaid\tsay\t_\tVBD\t_\t0\troot\t0:root\t_\n",
      "9\tDemocratic\tdemocratic\t_\tJJ\t_\t13\tamod\t13:amod\t_\n",
      "10\tpolitical\tpolitical\t_\tJJ\t_\t13\tamod\t13:amod\t_\n",
      "11\tconsultant\tconsultant\t_\tNN\t_\t13\tnn\t13:nn\t_\n",
      "12\tLarry\tLarry\t_\tNNP\t_\t13\tnn\t13:nn\t_\n",
      "13\tCeisler\tCeisler\t_\tNNP\t_\t8\tnsubj\t8:nsubj\t_\n",
      "14\t.\t.\t_\t.\t_\t0\t-\t0:-\t_\n",
      "\n",
      "# sent_id = apw_eng_19991101_0059_15\n",
      "# text = He may be the more experienced campaigner , even though Street has spent 19 years on the City Council and seven years as its president .\n",
      "1\the\the\t_\tPRP\t_\t7\tnsubj\t7:nsubj\t_\n",
      "2\tmay\tmay\t_\tMD\t_\t7\taux\t7:aux\t_\n",
      "3\tbe\tbe\t_\tVB\t_\t7\tcop\t7:cop\t_\n",
      "4\tthe\tthe\t_\tDT\t_\t7\tdet\t7:det\t_\n",
      "5\tmore\tmore\t_\tRBR\t_\t6\tadvmod\t6:advmod\t_\n",
      "6\texperienced\texperienced\t_\tJJ\t_\t7\tamod\t7:amod\t_\n",
      "7\tcampaigner\tcampaigner\t_\tNN\t_\t0\troot\t0:root\t_\n",
      "8\t,\t,\t_\t,\t_\t0\t-\t0:-\t_\n",
      "9\teven\teven\t_\tRB\t_\t13\tadvmod\t13:advmod\t_\n",
      "10\tthough\tthough\t_\tIN\t_\t13\tdep\t13:dep\t_\n",
      "11\tStreet\tStreet\t_\tNNP\t_\t13\tnsubj\t13:nsubj\t_\n",
      "12\thas\thas\t_\tAUXZ\t_\t13\tdep\t13:dep\t_\n",
      "13\tspent\tspend\t_\tVBN\t_\t7\tdep\t7:dep\t_\n",
      "14\t19\t19\t_\tCD\t_\t15\tnum\t15:num\t_\n",
      "15\tyears\tyear\t_\tNNS\t_\t13\ttmod\t13:tmod\t_\n",
      "16\ton\ton\t_\tIN\t_\t13\tprep\t13:prep\t_\n",
      "17\tthe\tthe\t_\tDT\t_\t19\tdet\t19:det\t_\n",
      "18\tCity\tCity\t_\tNNP\t_\t19\tnn\t19:nn\t_\n",
      "19\tCouncil\tCouncil\t_\tNNP\t_\t16\tpobj\t16:pobj\t_\n",
      "20\tand\tand\t_\tCC\t_\t19\tcc\t19:cc\t_\n",
      "21\tseven\tseven\t_\tCD\t_\t22\tnum\t22:num\t_\n",
      "22\tyears\tyear\t_\tNNS\t_\t19\tconj\t19:conj\t_\n",
      "23\tas\tas\t_\tIN\t_\t13\tprep\t13:prep\t_\n",
      "24\tits\tits\t_\tPRP$\t_\t25\tposs\t25:poss\t_\n",
      "25\tpresident\tpresident\t_\tNN\t_\t23\tpobj\t23:pobj\t_\n",
      "26\t.\t.\t_\t.\t_\t0\t-\t0:-\t_\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "101831"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "advadj_subset = corpus_from_path(subset_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## modifier bigram only"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    # 'NEG [lemma=\"not\"];'\n",
    "    # 'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    # 'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    # 'neg: ADJ -> NEG;'\n",
    "    # 'neg: NEG -> ADJ;'\n",
    "    # 'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ}\n",
      "-------------------------\n",
      "hits: 80\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## `not` somewhere in sentence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    'NEG [lemma=\"not\"];'\n",
    "    # 'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    # 'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    # 'neg: ADJ -> NEG;'\n",
    "    # 'neg: NEG -> ADJ;'\n",
    "    # 'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ;\n",
      "\t NEG [lemma=\"not\"]}\n",
      "-------------------------\n",
      "hits: 14\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## known `NEG` lemma somewhere in sentence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    # 'NEG [lemma=\"not\"];'\n",
    "    'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    # 'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    # 'neg: ADJ -> NEG;'\n",
    "    # 'neg: NEG -> ADJ;'\n",
    "    # 'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ;\n",
      "\t NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"]}\n",
      "-------------------------\n",
      "hits: 24\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## known `NEG` lemma preceding `ADV` token node"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    # 'NEG [lemma=\"not\"];'\n",
    "    'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    # 'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    # 'neg: ADJ -> NEG;'\n",
    "    # 'neg: NEG -> ADJ;'\n",
    "    'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ;\n",
      "\t NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];\n",
      "\t NEG << ADV}\n",
      "-------------------------\n",
      "hits: 20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## known `NEG` lemma with `ADJ` node as its **target** (in dependency relationship)\n",
    " This is not expected. No $NegPol$ patterns cover relationships of this direction."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    # 'NEG [lemma=\"not\"];'\n",
    "    'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    # 'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    # 'neg: ADJ -> NEG;'\n",
    "    'neg: NEG -> ADJ;'\n",
    "    'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ;\n",
      "\t NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];\n",
      "\t neg: NEG -> ADJ;\n",
      "\t NEG << ADV}\n",
      "-------------------------\n",
      "hits: 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## known `NEG` lemma with `ADJ` node as its **head/source**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    # 'NEG [lemma=\"not\"];'\n",
    "    'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    # 'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    'neg: ADJ -> NEG;'\n",
    "    # 'neg: NEG -> ADJ;'\n",
    "    'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ;\n",
      "\t NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];\n",
      "\t neg: ADJ -> NEG;\n",
      "\t NEG << ADV}\n",
      "-------------------------\n",
      "hits: 6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " 👆 This pattern, which does not impose any restrictions on the *type*\n",
    " of dependency relationship between `NEG` and `ADJ` yields duplicate results\n",
    " in the case of any \"enhanced\" parsing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "hits = co.search(neg_req)\n",
    "pprint(hits[:2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'matching': {'edges': {'mod': {'label': 'advmod',\n",
      "                                 'source': '12',\n",
      "                                 'target': '11'},\n",
      "                         'neg': {'label': {'1': 'neg', 'enhanced': 'yes'},\n",
      "                                 'source': '12',\n",
      "                                 'target': '10'}},\n",
      "               'nodes': {'ADJ': '12', 'ADV': '11', 'NEG': '10'}},\n",
      "  'sent_id': 'apw_eng_19991101_0031_5'},\n",
      " {'matching': {'edges': {'mod': {'label': 'advmod',\n",
      "                                 'source': '12',\n",
      "                                 'target': '11'},\n",
      "                         'neg': {'label': 'neg',\n",
      "                                 'source': '12',\n",
      "                                 'target': '10'}},\n",
      "               'nodes': {'ADJ': '12', 'ADV': '11', 'NEG': '10'}},\n",
      "  'sent_id': 'apw_eng_19991101_0031_5'}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "pprint({f\"{h['sent_id']} ~ hit {i} neg label\":h['matching']['edges']['neg']['label'] for i,h in enumerate(hits[:16])})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'apw_eng_19991101_0021_10 ~ hit 4 neg label': {'1': 'neg', 'enhanced': 'yes'},\n",
      " 'apw_eng_19991101_0021_10 ~ hit 5 neg label': 'neg',\n",
      " 'apw_eng_19991101_0025_10 ~ hit 2 neg label': {'1': 'neg', 'enhanced': 'yes'},\n",
      " 'apw_eng_19991101_0025_10 ~ hit 3 neg label': 'neg',\n",
      " 'apw_eng_19991101_0031_5 ~ hit 0 neg label': {'1': 'neg', 'enhanced': 'yes'},\n",
      " 'apw_eng_19991101_0031_5 ~ hit 1 neg label': 'neg'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## known `NEG` lemma with `ADJ` node as its *head/source* and the relationship type does not start with \"E\"\n",
    " This is to prevent issue with duplicate records for \"Enhanced\" versions of the dependency."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "neg_req = Request(\n",
    "    'ADJ [xpos=re\"JJ.?\"];'\n",
    "    'mod: ADJ -[advmod]-> ADV;'\n",
    "    'ADV < ADJ;'\n",
    "    # 'NEG [lemma=\"not\"];'\n",
    "    'NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];'\n",
    "    'neg: ADJ -[re\"[^E].*\"]-> NEG;'\n",
    "    # 'neg: ADJ -> NEG;'\n",
    "    # 'neg: NEG -> ADJ;'\n",
    "    'NEG << ADV;'\n",
    "    )\n",
    "print(str(neg_req).replace(';', ';\\n\\t '))\n",
    "print('-------------------------\\nhits:', co.count(neg_req))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ;\n",
      "\t NEG [lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"];\n",
      "\t neg: ADJ -[re\"[^E].*\"]-> NEG;\n",
      "\t NEG << ADV}\n",
      "-------------------------\n",
      "hits: 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ...and now the duplicate hits (as well as the output structure unpredictability) have been removed:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "pprint({f\"{h['sent_id']} ~ hit {i} neg label\":h['matching']['edges']['neg']['label'] for i,h in enumerate(co.search(neg_req)[:16])})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'apw_eng_19991101_0021_10 ~ hit 2 neg label': 'neg',\n",
      " 'apw_eng_19991101_0025_10 ~ hit 1 neg label': 'neg',\n",
      " 'apw_eng_19991101_0031_5 ~ hit 0 neg label': 'neg'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "pd.json_normalize(co.count(neg_req, [\"ADV.lemma\", \"NEG.lemma\"]), sep='_').transpose().rename(columns={0:'total'}).nlargest(10,'total')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>so_not</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fully_not</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           total\n",
       "so_not         2\n",
       "fully_not      1"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "neg_df = pd.json_normalize(co.search(neg_req), sep='_').convert_dtypes()\n",
    "neg_df.columns = (\n",
    "    neg_df.columns\n",
    "    .str.replace('matching_', '')\n",
    "    .str.replace('nodes_', 'index_')\n",
    "    .str.replace('edges_', 'dep_'))\n",
    "neg_df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>index_NEG</th>\n",
       "      <th>index_ADV</th>\n",
       "      <th>index_ADJ</th>\n",
       "      <th>dep_neg_source</th>\n",
       "      <th>dep_neg_label</th>\n",
       "      <th>dep_neg_target</th>\n",
       "      <th>dep_mod_source</th>\n",
       "      <th>dep_mod_label</th>\n",
       "      <th>dep_mod_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0031_5</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>neg</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>advmod</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apw_eng_19991101_0025_10</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>advmod</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apw_eng_19991101_0021_10</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>advmod</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sent_id index_NEG index_ADV index_ADJ dep_neg_source  \\\n",
       "0   apw_eng_19991101_0031_5        10        11        12             12   \n",
       "1  apw_eng_19991101_0025_10        12        14        15             15   \n",
       "2  apw_eng_19991101_0021_10        12        14        15             15   \n",
       "\n",
       "  dep_neg_label dep_neg_target dep_mod_source dep_mod_label dep_mod_target  \n",
       "0           neg             10             12        advmod             11  \n",
       "1           neg             12             15        advmod             14  \n",
       "2           neg             12             15        advmod             14  "
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "for m in neg_df.sample(min(len(neg_df),8)).sent_id.apply(lambda i: co.get(i).meta): \n",
    "    print(f\"{m['sent_id']}:\\t{m['text']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "apw_eng_19991101_0031_5:\tIt was not clear why the Ingush border was not fully open .\n",
      "apw_eng_19991101_0021_10:\tAT & claims that limiting the number of access providers may not be so bad , because a company that can plan its investment in all the equipment it requires to run the Internet through TV cables can offer service faster , cheaper and more efficiently .\n",
      "apw_eng_19991101_0025_10:\tAT & claims that limiting the number of access providers may not be so bad , because a company that can plan its investment in all the equipment it requires to run the Internet through TV cables can offer service faster , cheaper and more efficiently .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Thoughts...\n",
    " Instead of creating new subset `.conllu` files with all matching\n",
    " sentences + their context sentences (preceding and following),\n",
    " it should be possible to take only what is needed for the context sentences\n",
    " (i.e. `sent_id` and `sent_text`, the only things put into the hit table)\n",
    " and then pull those directly from the meta info.\n",
    "\n",
    " The next big question, though, is whether to create the subset file at all in that case.\n",
    " **This new module essentially makes the first 2 steps of the pipeline code obsolete.**\n",
    " If this functionality had existed 2 years ago... smh 😐\n",
    "\n",
    " But I'm not going to rework *everything* at this point.\n",
    "\n",
    " ## Plan\n",
    " I will create the `advadj` subset and then run the pipeline on those.\n",
    "\n",
    " If I remove the context sentences, `fill_match_info` will need to be modified to not collect them\n",
    " (since anything it would pull would be incorrect).\n",
    " However, it would not necessarily need to add them at all at that point.\n",
    " I wanted to make sure I had access to what they are, but so far, I haven't used them.\n",
    " If there exists a table where they can be looked up if need be, that should suffice.\n",
    " The sentence IDs are stable and unique identifiers, so any table indexed by `sent_id`\n",
    " will be easy to connect with the existing data.\n",
    "\n",
    " Mock Table Schematic\n",
    "\n",
    " > | sent_id | doc_id | conllu_id | sent_text | prev_id | next_id | prev_text | next_text |\n",
    " > |:--------|:-------|:----------|:----------|:--------|:--------|:----------|:----------|\n",
    " > | ... | ... | ... | ... | ... | ... | ... | ... |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# hit_meta = pd.json_normalize(neg_df.sent_id.apply(lambda i: co.get(i).meta['text'])\n",
    "#                              ).convert_dtypes()#.rename(columns={'':'newdoc_id'})\n",
    "hit_meta = neg_df.sent_id.to_frame()\n",
    "hit_meta = hit_meta.assign(sent_text = hit_meta.sent_id.apply(lambda i: co.get(i).meta['text']),\n",
    "                           conllu_id=conllu_path.stem, \n",
    "                        #    newdoc_id=hit_meta.newdoc_id.str.replace('# newdoc id = ', '').fillna('')\n",
    "                        )\n",
    "hit_meta"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>sent_text</th>\n",
       "      <th>conllu_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0031_5</td>\n",
       "      <td>It was not clear why the Ingush border was not...</td>\n",
       "      <td>apw_eng_199911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apw_eng_19991101_0025_10</td>\n",
       "      <td>AT &amp; claims that limiting the number of access...</td>\n",
       "      <td>apw_eng_199911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apw_eng_19991101_0021_10</td>\n",
       "      <td>AT &amp; claims that limiting the number of access...</td>\n",
       "      <td>apw_eng_199911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sent_id  \\\n",
       "0   apw_eng_19991101_0031_5   \n",
       "1  apw_eng_19991101_0025_10   \n",
       "2  apw_eng_19991101_0021_10   \n",
       "\n",
       "                                           sent_text       conllu_id  \n",
       "0  It was not clear why the Ingush border was not...  apw_eng_199911  \n",
       "1  AT & claims that limiting the number of access...  apw_eng_199911  \n",
       "2  AT & claims that limiting the number of access...  apw_eng_199911  "
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def parse_sent_id(sent_id):\n",
    "    doc_id, ordinal_str = sent_id.rsplit('_', 1)\n",
    "    ordinal_int = int(ordinal_str)\n",
    "\n",
    "    row = (sent_id, doc_id, ordinal_int)\n",
    "    for context_ix in (ordinal_int + i for i in (-1, 1)): \n",
    "        c_text = ''\n",
    "        c_id = ''\n",
    "        #> conllu doc sentence numbering starts at 1\n",
    "        if context_ix > 0:\n",
    "            c_id = f'{doc_id}_{context_ix}'\n",
    "            try: \n",
    "                c_obj = co.get(c_id)\n",
    "            except GrewError: \n",
    "                c_id = ''\n",
    "            else: \n",
    "                c_text = c_obj.meta['text']\n",
    "        row += (c_id, c_text)\n",
    "        \n",
    "    yield _META_TUP._make(row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "sid = hit_meta.sent_id[0]\n",
    "print(pd.DataFrame(parse_sent_id(sid)).set_index('sent_id').transpose().to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|           | apw_eng_19991101_0031_5                                                                                                                        |\n",
      "|:----------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| doc_id    | apw_eng_19991101_0031                                                                                                                          |\n",
      "| sent_int  | 5                                                                                                                                              |\n",
      "| prev_id   | apw_eng_19991101_0031_4                                                                                                                        |\n",
      "| prev_text | But no other vehicles were let through , and a line of cars and trucks crammed with refugees stretched for several miles on the Chechen side . |\n",
      "| next_id   | apw_eng_19991101_0031_6                                                                                                                        |\n",
      "| next_text | -LBQ-  We are ready to let the refugees pass , but it is necessary to keep order ,  -RDQ-  Col. Arkady Krimsky said .                          |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "context_info = pd.concat(pd.DataFrame(parse_sent_id(s)) for s in hit_meta.sent_id)\n",
    "context_info.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_int</th>\n",
       "      <th>prev_id</th>\n",
       "      <th>prev_text</th>\n",
       "      <th>next_id</th>\n",
       "      <th>next_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0031_5</td>\n",
       "      <td>apw_eng_19991101_0031</td>\n",
       "      <td>5</td>\n",
       "      <td>apw_eng_19991101_0031_4</td>\n",
       "      <td>But no other vehicles were let through , and a...</td>\n",
       "      <td>apw_eng_19991101_0031_6</td>\n",
       "      <td>-LBQ-  We are ready to let the refugees pass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0025_10</td>\n",
       "      <td>apw_eng_19991101_0025</td>\n",
       "      <td>10</td>\n",
       "      <td>apw_eng_19991101_0025_9</td>\n",
       "      <td>-LBQ-  That 's called logical , consistent th...</td>\n",
       "      <td>apw_eng_19991101_0025_11</td>\n",
       "      <td>-LBQ-  We 're moving into an information-base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0021_10</td>\n",
       "      <td>apw_eng_19991101_0021</td>\n",
       "      <td>10</td>\n",
       "      <td>apw_eng_19991101_0021_9</td>\n",
       "      <td>-LBQ-  That 's called logical , consistent th...</td>\n",
       "      <td>apw_eng_19991101_0021_11</td>\n",
       "      <td>-LBQ-  We 're moving into an information-base...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sent_id                 doc_id  sent_int  \\\n",
       "0   apw_eng_19991101_0031_5  apw_eng_19991101_0031         5   \n",
       "0  apw_eng_19991101_0025_10  apw_eng_19991101_0025        10   \n",
       "0  apw_eng_19991101_0021_10  apw_eng_19991101_0021        10   \n",
       "\n",
       "                   prev_id                                          prev_text  \\\n",
       "0  apw_eng_19991101_0031_4  But no other vehicles were let through , and a...   \n",
       "0  apw_eng_19991101_0025_9   -LBQ-  That 's called logical , consistent th...   \n",
       "0  apw_eng_19991101_0021_9   -LBQ-  That 's called logical , consistent th...   \n",
       "\n",
       "                    next_id                                          next_text  \n",
       "0   apw_eng_19991101_0031_6   -LBQ-  We are ready to let the refugees pass ...  \n",
       "0  apw_eng_19991101_0025_11   -LBQ-  We 're moving into an information-base...  \n",
       "0  apw_eng_19991101_0021_11   -LBQ-  We 're moving into an information-base...  "
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "meta_info = hit_meta.set_index('sent_id').join(context_info.set_index('sent_id')).rename(columns={'text':'sent_text'})\n",
    "meta_info = meta_info[['conllu_id', 'doc_id', 'sent_int', 'prev_id', 'next_id', 'prev_text', 'sent_text', 'next_text']]\n",
    "print(meta_info.sample(min(len(meta_info),2)).to_markdown())\n",
    "print(\"\\n---\\n\")\n",
    "print(meta_info.iloc[-1, :].squeeze().to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| sent_id                  | conllu_id      | doc_id                |   sent_int | prev_id                 | next_id                  | prev_text                                                                                                                                                                                                                                            | sent_text                                                                                                                                                                                                                                                    | next_text                                                |\n",
      "|:-------------------------|:---------------|:----------------------|-----------:|:------------------------|:-------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------|\n",
      "| apw_eng_19991101_0025_10 | apw_eng_199911 | apw_eng_19991101_0025 |         10 | apw_eng_19991101_0025_9 | apw_eng_19991101_0025_11 | -LBQ-  That 's called logical , consistent thinking , which unfortunately is not the way of Washington , D.C. , right now ,  -RDQ-  said A. Michael Noll , a University of Southern California communications professor and a former AT & employee . | AT & claims that limiting the number of access providers may not be so bad , because a company that can plan its investment in all the equipment it requires to run the Internet through TV cables can offer service faster , cheaper and more efficiently . | -LBQ-  We 're moving into an information-based economy . |\n",
      "| apw_eng_19991101_0021_10 | apw_eng_199911 | apw_eng_19991101_0021 |         10 | apw_eng_19991101_0021_9 | apw_eng_19991101_0021_11 | -LBQ-  That 's called logical , consistent thinking , which unfortunately is not the way of Washington , D.C. , right now ,  -RDQ-  said A. Michael Noll , a University of Southern California communications professor and a former AT & employee . | AT & claims that limiting the number of access providers may not be so bad , because a company that can plan its investment in all the equipment it requires to run the Internet through TV cables can offer service faster , cheaper and more efficiently . | -LBQ-  We 're moving into an information-based economy . |\n",
      "\n",
      "---\n",
      "\n",
      "|           | apw_eng_19991101_0021_10                                                                                                                                                                                                                                     |\n",
      "|:----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| conllu_id | apw_eng_199911                                                                                                                                                                                                                                               |\n",
      "| doc_id    | apw_eng_19991101_0021                                                                                                                                                                                                                                        |\n",
      "| sent_int  | 10                                                                                                                                                                                                                                                           |\n",
      "| prev_id   | apw_eng_19991101_0021_9                                                                                                                                                                                                                                      |\n",
      "| next_id   | apw_eng_19991101_0021_11                                                                                                                                                                                                                                     |\n",
      "| prev_text | -LBQ-  That 's called logical , consistent thinking , which unfortunately is not the way of Washington , D.C. , right now ,  -RDQ-  said A. Michael Noll , a University of Southern California communications professor and a former AT & employee .         |\n",
      "| sent_text | AT & claims that limiting the number of access providers may not be so bad , because a company that can plan its investment in all the equipment it requires to run the Internet through TV cables can offer service faster , cheaper and more efficiently . |\n",
      "| next_text | -LBQ-  We 're moving into an information-based economy .                                                                                                                                                                                                     |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "meta_info.to_csv(subset_dir.joinpath(conllu_path.stem+'+meta.psv'), sep='|')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "neg_df.loc[:, ['sent_text', 'conllu_id', 'doc_id']] =  neg_df.sent_id.apply(\n",
    "    lambda x: meta_info.loc[x, ['sent_text', 'conllu_id', 'doc_id']])\n",
    "neg_df.sample(min(len(neg_df),10))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>index_NEG</th>\n",
       "      <th>index_ADV</th>\n",
       "      <th>index_ADJ</th>\n",
       "      <th>dep_neg_source</th>\n",
       "      <th>dep_neg_label</th>\n",
       "      <th>dep_neg_target</th>\n",
       "      <th>dep_mod_source</th>\n",
       "      <th>dep_mod_label</th>\n",
       "      <th>dep_mod_target</th>\n",
       "      <th>sent_text</th>\n",
       "      <th>conllu_id</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apw_eng_19991101_0031_5</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>neg</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>advmod</td>\n",
       "      <td>11</td>\n",
       "      <td>It was not clear why the Ingush border was not...</td>\n",
       "      <td>apw_eng_199911</td>\n",
       "      <td>apw_eng_19991101_0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apw_eng_19991101_0021_10</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>advmod</td>\n",
       "      <td>14</td>\n",
       "      <td>AT &amp; claims that limiting the number of access...</td>\n",
       "      <td>apw_eng_199911</td>\n",
       "      <td>apw_eng_19991101_0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apw_eng_19991101_0025_10</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>neg</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>advmod</td>\n",
       "      <td>14</td>\n",
       "      <td>AT &amp; claims that limiting the number of access...</td>\n",
       "      <td>apw_eng_199911</td>\n",
       "      <td>apw_eng_19991101_0025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sent_id index_NEG index_ADV index_ADJ dep_neg_source  \\\n",
       "0   apw_eng_19991101_0031_5        10        11        12             12   \n",
       "2  apw_eng_19991101_0021_10        12        14        15             15   \n",
       "1  apw_eng_19991101_0025_10        12        14        15             15   \n",
       "\n",
       "  dep_neg_label dep_neg_target dep_mod_source dep_mod_label dep_mod_target  \\\n",
       "0           neg             10             12        advmod             11   \n",
       "2           neg             12             15        advmod             14   \n",
       "1           neg             12             15        advmod             14   \n",
       "\n",
       "                                           sent_text       conllu_id  \\\n",
       "0  It was not clear why the Ingush border was not...  apw_eng_199911   \n",
       "2  AT & claims that limiting the number of access...  apw_eng_199911   \n",
       "1  AT & claims that limiting the number of access...  apw_eng_199911   \n",
       "\n",
       "                  doc_id  \n",
       "0  apw_eng_19991101_0031  \n",
       "2  apw_eng_19991101_0021  \n",
       "1  apw_eng_19991101_0025  "
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}