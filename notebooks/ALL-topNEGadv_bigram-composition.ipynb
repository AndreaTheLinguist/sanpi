{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_SET</th>\n",
       "      <th>f_SET</th>\n",
       "      <th>dP1_SET</th>\n",
       "      <th>P1_SET</th>\n",
       "      <th>LRC_SET</th>\n",
       "      <th>G2_SET</th>\n",
       "      <th>MI_SET</th>\n",
       "      <th>odds_r_disc_SET</th>\n",
       "      <th>t_SET</th>\n",
       "      <th>N_SET</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_t</th>\n",
       "      <th>mean_N</th>\n",
       "      <th>mean_f1</th>\n",
       "      <th>mean_f2</th>\n",
       "      <th>mean_expF</th>\n",
       "      <th>mean_unexpF</th>\n",
       "      <th>r_f_MIR</th>\n",
       "      <th>r_N_MIR</th>\n",
       "      <th>r_f1_MIR</th>\n",
       "      <th>r_f2_MIR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>necessarily</th>\n",
       "      <td>NEGany~necessarily</td>\n",
       "      <td>42595</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.87</td>\n",
       "      <td>7.10</td>\n",
       "      <td>230,257.34</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.17</td>\n",
       "      <td>196.05</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>110.48</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>25,027.00</td>\n",
       "      <td>1,161.20</td>\n",
       "      <td>20,617.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>NEGany~that</td>\n",
       "      <td>164768</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6.34</td>\n",
       "      <td>831,137.25</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.94</td>\n",
       "      <td>383.56</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>217.42</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>106,878.00</td>\n",
       "      <td>5,007.91</td>\n",
       "      <td>79,530.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exactly</th>\n",
       "      <td>NEGany~exactly</td>\n",
       "      <td>43813</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.94</td>\n",
       "      <td>210,126.60</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.82</td>\n",
       "      <td>197.11</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>109.68</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>29,842.00</td>\n",
       "      <td>1,366.77</td>\n",
       "      <td>20,946.23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>NEGany~any</td>\n",
       "      <td>15384</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4.07</td>\n",
       "      <td>50,880.96</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>111.95</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>69.16</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>17,789.50</td>\n",
       "      <td>851.61</td>\n",
       "      <td>7,373.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remotely</th>\n",
       "      <td>NEGany~remotely</td>\n",
       "      <td>5661</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.40</td>\n",
       "      <td>15,284.49</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.06</td>\n",
       "      <td>65.73</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>49.63</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>9,383.50</td>\n",
       "      <td>558.48</td>\n",
       "      <td>3,192.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever</th>\n",
       "      <td>NEGany~ever</td>\n",
       "      <td>5932</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>183.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>12.49</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>34.23</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>59,567.50</td>\n",
       "      <td>2,918.83</td>\n",
       "      <td>2,401.67</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yet</th>\n",
       "      <td>NEGany~yet</td>\n",
       "      <td>51867</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4.65</td>\n",
       "      <td>197,610.98</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.42</td>\n",
       "      <td>209.42</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>109.75</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>48,289.00</td>\n",
       "      <td>2,156.07</td>\n",
       "      <td>23,937.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immediately</th>\n",
       "      <td>NEGany~immediately</td>\n",
       "      <td>56099</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.86</td>\n",
       "      <td>224,059.55</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>219.01</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>114.44</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>49,084.00</td>\n",
       "      <td>2,215.00</td>\n",
       "      <td>26,036.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>particularly</th>\n",
       "      <td>NEGany~particularly</td>\n",
       "      <td>55527</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.38</td>\n",
       "      <td>37,272.74</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>140.66</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>106.81</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>263,335.50</td>\n",
       "      <td>12,304.83</td>\n",
       "      <td>20,080.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inherently</th>\n",
       "      <td>NEGany~inherently</td>\n",
       "      <td>6743</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.75</td>\n",
       "      <td>7,022.02</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>56.75</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>46.91</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>26,468.00</td>\n",
       "      <td>1,481.33</td>\n",
       "      <td>3,322.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terribly</th>\n",
       "      <td>NEGany~terribly</td>\n",
       "      <td>17949</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>3.19</td>\n",
       "      <td>43,741.44</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.98</td>\n",
       "      <td>114.80</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>67.21</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>31,787.00</td>\n",
       "      <td>1,679.65</td>\n",
       "      <td>8,078.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          key_SET   f_SET  dP1_SET  P1_SET  LRC_SET  \\\n",
       "adv                                                                   \n",
       "necessarily    NEGany~necessarily   42595     0.83    0.87     7.10   \n",
       "that                  NEGany~that  164768     0.75    0.79     6.34   \n",
       "exactly            NEGany~exactly   43813     0.70    0.75     5.94   \n",
       "any                    NEGany~any   15384     0.40    0.45     4.07   \n",
       "remotely          NEGany~remotely    5661     0.30    0.34     3.40   \n",
       "ever                  NEGany~ever    5932     0.01    0.05     0.16   \n",
       "yet                    NEGany~yet   51867     0.50    0.54     4.65   \n",
       "immediately    NEGany~immediately   56099     0.54    0.58     4.86   \n",
       "particularly  NEGany~particularly   55527     0.07    0.11     1.38   \n",
       "inherently      NEGany~inherently    6743     0.10    0.14     1.75   \n",
       "terribly          NEGany~terribly   17949     0.26    0.30     3.19   \n",
       "\n",
       "                 G2_SET  MI_SET  odds_r_disc_SET  t_SET     N_SET  ...  \\\n",
       "adv                                                                ...   \n",
       "necessarily  230,257.34    1.30             2.17 196.05  72839589  ...   \n",
       "that         831,137.25    1.26             1.94 383.56  72839589  ...   \n",
       "exactly      210,126.60    1.23             1.82 197.11  72839589  ...   \n",
       "any           50,880.96    1.01             1.25 111.95  72839589  ...   \n",
       "remotely      15,284.49    0.90             1.06  65.73  72839589  ...   \n",
       "ever             183.92    0.08             0.08  12.49  72839589  ...   \n",
       "yet          197,610.98    1.09             1.42 209.42  72839589  ...   \n",
       "immediately  224,059.55    1.12             1.49 219.01  72839589  ...   \n",
       "particularly  37,272.74    0.39             0.43 140.66  72839589  ...   \n",
       "inherently     7,022.02    0.51             0.56  56.75  72839589  ...   \n",
       "terribly      43,741.44    0.84             0.98 114.80  72839589  ...   \n",
       "\n",
       "              mean_t    mean_N  mean_f1    mean_f2 mean_expF mean_unexpF  \\\n",
       "adv                                                                        \n",
       "necessarily   110.48  37270759  1732696  25,027.00  1,161.20   20,617.80   \n",
       "that          217.42  37270759  1732696 106,878.00  5,007.91   79,530.09   \n",
       "exactly       109.68  37270759  1732696  29,842.00  1,366.77   20,946.23   \n",
       "any            69.16  37270759  1732696  17,789.50    851.61    7,373.39   \n",
       "remotely       49.63  37270759  1732696   9,383.50    558.48    3,192.02   \n",
       "ever           34.23  37270759  1732696  59,567.50  2,918.83    2,401.67   \n",
       "yet           109.75  37270759  1732696  48,289.00  2,156.07   23,937.43   \n",
       "immediately   114.44  37270759  1732696  49,084.00  2,215.00   26,036.00   \n",
       "particularly  106.81  37270759  1732696 263,335.50 12,304.83   20,080.17   \n",
       "inherently     46.91  37270759  1732696  26,468.00  1,481.33    3,322.17   \n",
       "terribly       67.21  37270759  1732696  31,787.00  1,679.65    8,078.35   \n",
       "\n",
       "              r_f_MIR  r_N_MIR  r_f1_MIR  r_f2_MIR  \n",
       "adv                                                 \n",
       "necessarily      0.02     0.02      0.09      0.02  \n",
       "that             0.03     0.02      0.09      0.03  \n",
       "exactly          0.02     0.02      0.09      0.02  \n",
       "any              0.07     0.02      0.09      0.03  \n",
       "remotely         0.33     0.02      0.09      0.14  \n",
       "ever             0.79     0.02      0.09      0.04  \n",
       "yet              0.01     0.02      0.09      0.01  \n",
       "immediately      0.01     0.02      0.09      0.01  \n",
       "particularly     0.17     0.02      0.09      0.03  \n",
       "inherently       0.42     0.02      0.09      0.11  \n",
       "terribly         0.09     0.02      0.09      0.08  \n",
       "\n",
       "[11 rows x 47 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from am_notebooks import *\n",
    "\n",
    "from source.utils import HIT_TABLES_DIR, print_iter, timestamp_today\n",
    "from source.utils.associate import AM_DF_DIR, TOP_AM_DIR, adjust_am_names as adjust_assoc_columns \n",
    "from source.utils.dataframes import catify_hit_table as catify\n",
    "\n",
    "# from source.utils.sample import sample_pickle as sp\n",
    "\n",
    "TAG='ALL'\n",
    "K=8\n",
    "BK = max(K+2, 10)\n",
    "BIGRAM_F_FLOOR=50 if TAG == 'ALL' else 25\n",
    "\n",
    "ADV_F_FLOOR=5000\n",
    "\n",
    "# METRIC_PRIORITY = ['LRC', 'P1', 'G2', 'P2'] if TAG=='NEQ' else ['dP1', 'LRC', 'G2', 'P1']\n",
    "TOP_AM_TAG_DIR = TOP_AM_DIR / TAG\n",
    "TAG_TOP_STR = f'{TAG}-Top{K}'\n",
    "TAG_TOP_DIR = TOP_AM_TAG_DIR / TAG_TOP_STR\n",
    "DATE=timestamp_today()\n",
    "FOCUS_ORIG = ['f', 'E11', 'unexpected_f',\n",
    "              'am_p1_given2', 'am_p1_given2_simple', \n",
    "              'am_p2_given1', 'am_p2_given1_simple', \n",
    "              'conservative_log_ratio',\n",
    "              'am_log_likelihood', \n",
    "            #   't_score',\n",
    "            #   'mutual_information', 'am_odds_ratio_disc',\n",
    "              'N', 'f1', 'f2', 'l1', 'l2']\n",
    "FOCUS = adjust_assoc_columns(FOCUS_ORIG)\n",
    "pd.set_option(\"display.float_format\", '{:,.2f}'.format)\n",
    "pd.set_option(\"display.max_colwidth\", 80)\n",
    "adv_am = seek_top_adv_am(DATE, ADV_F_FLOOR, TAG_TOP_STR, TAG_TOP_DIR)\n",
    "adv_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_HITS_PATH = HIT_TABLES_DIR /'RBdirect'/'ALL-RBdirect_final.parq'\n",
    "POS_HITS_PATH = HIT_TABLES_DIR /'not-RBdirect'/f'{TAG}_not-RBdirect_final.parq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Top 8 Most Negative Adverbs\n",
      "1. necessarily\n",
      "1. that\n",
      "1. exactly\n",
      "1. any\n",
      "1. remotely\n",
      "1. ever\n",
      "1. yet\n",
      "1. immediately\n",
      "1. particularly\n",
      "1. inherently\n",
      "1. terribly\n"
     ]
    }
   ],
   "source": [
    "adv_list = adv_am.index.to_list()\n",
    "print_iter(adv_list, header=f'## Top {K} Most Negative Adverbs',bullet = '1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 8 Most Negative Adverbs\n",
    "1. necessarily\n",
    "1. that\n",
    "1. exactly\n",
    "1. any\n",
    "1. remotely\n",
    "1. ever\n",
    "1. yet\n",
    "1. immediately\n",
    "1. particularly\n",
    "1. inherently\n",
    "1. terribly\n",
    "\n",
    "\n",
    "\n",
    "## Load AM table for `adv~adj` comparison (bigram composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────────┬──────────────────────────────────────────────────────────────────────────╮\n",
      "│        │ path to \"context-blind\" AM scores                                        │\n",
      "├────────┼──────────────────────────────────────────────────────────────────────────┤\n",
      "│ mirror │ /share/compling/projects/sanpi/results/assoc_df/adv_adj/ANYmirror/extra/ │\n",
      "│        │ AdvAdj_ALL_any-mirror_final-freq_min50x_extra.parq                       │\n",
      "├────────┼──────────────────────────────────────────────────────────────────────────┤\n",
      "│ direct │ /share/compling/projects/sanpi/results/assoc_df/adv_adj/ANYdirect/extra/ │\n",
      "│        │ AdvAdj_ALL_any-direct_final-freq_min50x_extra.parq                       │\n",
      "╰────────┴──────────────────────────────────────────────────────────────────────────╯\n"
     ]
    }
   ],
   "source": [
    "blind_am_iter = AM_DF_DIR.joinpath('adv_adj').rglob(f'AdvAdj_{TAG}*min{BIGRAM_F_FLOOR}x_extra.parq')\n",
    "blam_dict = {blamp.parent.parent.name.strip('ANY'): blamp for blamp in blind_am_iter}\n",
    "print(pd.Series(blam_dict)\n",
    "       .to_frame('path to \"context-blind\" AM scores')\n",
    "       .to_markdown(tablefmt='rounded_grid', maxcolwidths=[None, 72]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading `mirror` AM scores\n",
      "\n",
      " Path: `assoc_df/adv_adj/ANYmirror/extra/AdvAdj_ALL_any-mirror_final-freq_min50x_extra.parq`\n",
      "\n",
      "\n",
      "_Bigrams with the highest `LRC` value for each adverb_\n",
      "\n",
      "\n",
      "|                              |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:-----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **exactly~alike**            |   7.793 |           0.147 |          0.210 |       0.997 |\n",
      "| **immediately~recognizable** |   5.980 |           0.069 |          0.090 |       0.992 |\n",
      "| **any~closer**               |   5.679 |           0.063 |          0.069 |       0.990 |\n",
      "| **terribly~wrong**           |   4.859 |           0.222 |          0.363 |       0.967 |\n",
      "| **inherently~wrong**         |   4.797 |           0.215 |          0.345 |       0.966 |\n",
      "| **particularly~noteworthy**  |   4.227 |           0.107 |          0.207 |       0.964 |\n",
      "| **ever~certain**             |   4.147 |           0.052 |          0.077 |       0.963 |\n",
      "| **that~great**               |   3.895 |           0.059 |          0.059 |       0.947 |\n",
      "| **necessarily~wrong**        |   3.455 |           0.096 |          0.183 |       0.937 |\n",
      "| **remotely~close**           |   3.339 |           0.062 |          0.106 |       0.929 |\n",
      "\n",
      "\n",
      "_Bigrams with the highest `deltaP_mean` value for each adverb_\n",
      "\n",
      "\n",
      "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **terribly~wrong**          |   4.859 |           0.222 |          0.363 |       0.967 |\n",
      "| **inherently~wrong**        |   4.797 |           0.215 |          0.345 |       0.966 |\n",
      "| **any~better**              |   5.061 |           0.186 |          0.342 |       0.976 |\n",
      "| **exactly~alike**           |   7.793 |           0.147 |          0.210 |       0.997 |\n",
      "| **immediately~available**   |   4.802 |           0.125 |          0.224 |       0.974 |\n",
      "| **particularly~noteworthy** |   4.227 |           0.107 |          0.207 |       0.964 |\n",
      "| **necessarily~wrong**       |   3.455 |           0.096 |          0.183 |       0.937 |\n",
      "| **remotely~close**          |   3.339 |           0.062 |          0.106 |       0.929 |\n",
      "| **that~great**              |   3.895 |           0.059 |          0.059 |       0.947 |\n",
      "| **ever~certain**            |   4.147 |           0.052 |          0.077 |       0.963 |\n",
      "\n",
      "\n",
      "_Bigrams with the highest `deltaP_max` value for each adverb_\n",
      "\n",
      "\n",
      "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **terribly~wrong**          |   4.859 |           0.222 |          0.363 |       0.967 |\n",
      "| **inherently~wrong**        |   4.797 |           0.215 |          0.345 |       0.966 |\n",
      "| **any~better**              |   5.061 |           0.186 |          0.342 |       0.976 |\n",
      "| **immediately~available**   |   4.802 |           0.125 |          0.224 |       0.974 |\n",
      "| **exactly~alike**           |   7.793 |           0.147 |          0.210 |       0.997 |\n",
      "| **particularly~noteworthy** |   4.227 |           0.107 |          0.207 |       0.964 |\n",
      "| **necessarily~wrong**       |   3.455 |           0.096 |          0.183 |       0.937 |\n",
      "| **remotely~close**          |   3.339 |           0.062 |          0.106 |       0.929 |\n",
      "| **that~good**               |   2.295 |           0.055 |          0.094 |       0.834 |\n",
      "| **ever~certain**            |   4.147 |           0.052 |          0.077 |       0.963 |\n",
      "\n",
      "\n",
      "_Bigrams with the highest `unexp_r` value for each adverb_\n",
      "\n",
      "\n",
      "|                              |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:-----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **exactly~alike**            |   7.793 |           0.147 |          0.210 |       0.997 |\n",
      "| **immediately~recognizable** |   5.980 |           0.069 |          0.090 |       0.992 |\n",
      "| **any~closer**               |   5.679 |           0.063 |          0.069 |       0.990 |\n",
      "| **terribly~amiss**           |   3.986 |           0.050 |          0.089 |       0.970 |\n",
      "| **inherently~wrong**         |   4.797 |           0.215 |          0.345 |       0.966 |\n",
      "| **particularly~noteworthy**  |   4.227 |           0.107 |          0.207 |       0.964 |\n",
      "| **ever~certain**             |   4.147 |           0.052 |          0.077 |       0.963 |\n",
      "| **that~great**               |   3.895 |           0.059 |          0.059 |       0.947 |\n",
      "| **necessarily~wrong**        |   3.455 |           0.096 |          0.183 |       0.937 |\n",
      "| **remotely~related**         |   2.970 |           0.024 |          0.029 |       0.935 |\n",
      "\n",
      "\n",
      "### Loading `direct` AM scores\n",
      "\n",
      " Path: `assoc_df/adv_adj/ANYdirect/extra/AdvAdj_ALL_any-direct_final-freq_min50x_extra.parq`\n",
      "\n",
      "\n",
      "_Bigrams with the highest `LRC` value for each adverb_\n",
      "\n",
      "\n",
      "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
      "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
      "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
      "| **immediately~accretive**   |   8.539 |           0.226 |          0.450 |       0.997 |\n",
      "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
      "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
      "| **ever~olympic**            |   8.231 |           0.222 |          0.442 |       0.996 |\n",
      "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
      "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
      "| **any~happier**             |   6.748 |           0.043 |          0.058 |       0.992 |\n",
      "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
      "\n",
      "\n",
      "_Bigrams with the highest `deltaP_mean` value for each adverb_\n",
      "\n",
      "\n",
      "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
      "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
      "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
      "| **immediately~appealable**  |   8.409 |           0.273 |          0.545 |       0.998 |\n",
      "| **ever~quarterly**          |   8.049 |           0.224 |          0.446 |       0.997 |\n",
      "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
      "| **any~better**              |   5.227 |           0.171 |          0.325 |       0.974 |\n",
      "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
      "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
      "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
      "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
      "\n",
      "\n",
      "_Bigrams with the highest `deltaP_max` value for each adverb_\n",
      "\n",
      "\n",
      "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
      "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
      "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
      "| **immediately~appealable**  |   8.409 |           0.273 |          0.545 |       0.998 |\n",
      "| **ever~quarterly**          |   8.049 |           0.224 |          0.446 |       0.997 |\n",
      "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
      "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
      "| **any~better**              |   5.227 |           0.171 |          0.325 |       0.974 |\n",
      "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
      "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
      "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
      "\n",
      "\n",
      "_Bigrams with the highest `unexp_r` value for each adverb_\n",
      "\n",
      "\n",
      "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
      "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
      "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
      "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
      "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
      "| **immediately~appealable**  |   8.409 |           0.273 |          0.545 |       0.998 |\n",
      "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
      "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
      "| **ever~quarterly**          |   8.049 |           0.224 |          0.446 |       0.997 |\n",
      "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
      "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
      "| **any~happier**             |   6.748 |           0.043 |          0.058 |       0.992 |\n",
      "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def peek_am(peek_metric, blamin):\n",
    "    peek = blamin.reset_index().groupby('l1').apply(\n",
    "        lambda x: x.nlargest(1, peek_metric)\n",
    "        ).reset_index(drop=True).set_index('key')\n",
    "    print(f'\\n_Bigrams with the highest `{peek_metric}` value for each adverb_\\n')\n",
    "    return peek.sort_values(peek_metric, ascending=False)\n",
    "\n",
    "blind_priority_cols = METRIC_PRIORITY_DICT[f'{TAG}_blind']\n",
    "blam_dfs = {}\n",
    "for blam_kind, blam_path in blam_dict.items():\n",
    "    print(f'\\n### Loading `{blam_kind}` AM scores\\n\\n Path: `{blam_path.relative_to(AM_DF_DIR.parent)}`\\n')\n",
    "    blamin = pd.read_parquet(\n",
    "        blam_path, engine='pyarrow',\n",
    "        filters=[('l1', 'in', adv_list)],\n",
    "        columns=FOCUS_DICT[TAG]['adv_adj'])\n",
    "    blamin['dataset']=blam_kind\n",
    "    blamin = catify(adjust_ams(blamin),\n",
    "                    reverse=True)\n",
    "    \n",
    "    for peek_metric in blind_priority_cols:\n",
    "        nb_show_table(peek_am(peek_metric, blamin).filter(blind_priority_cols), n_dec=3)\n",
    "    blamin.index = f'[_{blam_kind}_] ' + blamin.index\n",
    "    blam_dfs[blam_kind] = blamin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading `mirror` AM scores\n",
    "\n",
    " Path: `assoc_df/adv_adj/ANYmirror/extra/AdvAdj_ALL_any-mirror_final-freq_min50x_extra.parq`\n",
    "\n",
    "\n",
    "_Bigrams with the highest `LRC` value for each adverb_\n",
    "\n",
    "\n",
    "|                              |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:-----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **exactly~alike**            |   7.793 |           0.147 |          0.210 |       0.997 |\n",
    "| **immediately~recognizable** |   5.980 |           0.069 |          0.090 |       0.992 |\n",
    "| **any~closer**               |   5.679 |           0.063 |          0.069 |       0.990 |\n",
    "| **terribly~wrong**           |   4.859 |           0.222 |          0.363 |       0.967 |\n",
    "| **inherently~wrong**         |   4.797 |           0.215 |          0.345 |       0.966 |\n",
    "| **particularly~noteworthy**  |   4.227 |           0.107 |          0.207 |       0.964 |\n",
    "| **ever~certain**             |   4.147 |           0.052 |          0.077 |       0.963 |\n",
    "| **that~great**               |   3.895 |           0.059 |          0.059 |       0.947 |\n",
    "| **necessarily~wrong**        |   3.455 |           0.096 |          0.183 |       0.937 |\n",
    "| **remotely~close**           |   3.339 |           0.062 |          0.106 |       0.929 |\n",
    "\n",
    "\n",
    "_Bigrams with the highest `deltaP_mean` value for each adverb_\n",
    "\n",
    "\n",
    "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **terribly~wrong**          |   4.859 |           0.222 |          0.363 |       0.967 |\n",
    "| **inherently~wrong**        |   4.797 |           0.215 |          0.345 |       0.966 |\n",
    "| **any~better**              |   5.061 |           0.186 |          0.342 |       0.976 |\n",
    "| **exactly~alike**           |   7.793 |           0.147 |          0.210 |       0.997 |\n",
    "| **immediately~available**   |   4.802 |           0.125 |          0.224 |       0.974 |\n",
    "| **particularly~noteworthy** |   4.227 |           0.107 |          0.207 |       0.964 |\n",
    "| **necessarily~wrong**       |   3.455 |           0.096 |          0.183 |       0.937 |\n",
    "| **remotely~close**          |   3.339 |           0.062 |          0.106 |       0.929 |\n",
    "| **that~great**              |   3.895 |           0.059 |          0.059 |       0.947 |\n",
    "| **ever~certain**            |   4.147 |           0.052 |          0.077 |       0.963 |\n",
    "\n",
    "\n",
    "_Bigrams with the highest `deltaP_max` value for each adverb_\n",
    "\n",
    "\n",
    "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **terribly~wrong**          |   4.859 |           0.222 |          0.363 |       0.967 |\n",
    "| **inherently~wrong**        |   4.797 |           0.215 |          0.345 |       0.966 |\n",
    "| **any~better**              |   5.061 |           0.186 |          0.342 |       0.976 |\n",
    "| **immediately~available**   |   4.802 |           0.125 |          0.224 |       0.974 |\n",
    "| **exactly~alike**           |   7.793 |           0.147 |          0.210 |       0.997 |\n",
    "| **particularly~noteworthy** |   4.227 |           0.107 |          0.207 |       0.964 |\n",
    "| **necessarily~wrong**       |   3.455 |           0.096 |          0.183 |       0.937 |\n",
    "| **remotely~close**          |   3.339 |           0.062 |          0.106 |       0.929 |\n",
    "| **that~good**               |   2.295 |           0.055 |          0.094 |       0.834 |\n",
    "| **ever~certain**            |   4.147 |           0.052 |          0.077 |       0.963 |\n",
    "\n",
    "\n",
    "_Bigrams with the highest `unexp_r` value for each adverb_\n",
    "\n",
    "\n",
    "|                              |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:-----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **exactly~alike**            |   7.793 |           0.147 |          0.210 |       0.997 |\n",
    "| **immediately~recognizable** |   5.980 |           0.069 |          0.090 |       0.992 |\n",
    "| **any~closer**               |   5.679 |           0.063 |          0.069 |       0.990 |\n",
    "| **terribly~amiss**           |   3.986 |           0.050 |          0.089 |       0.970 |\n",
    "| **inherently~wrong**         |   4.797 |           0.215 |          0.345 |       0.966 |\n",
    "| **particularly~noteworthy**  |   4.227 |           0.107 |          0.207 |       0.964 |\n",
    "| **ever~certain**             |   4.147 |           0.052 |          0.077 |       0.963 |\n",
    "| **that~great**               |   3.895 |           0.059 |          0.059 |       0.947 |\n",
    "| **necessarily~wrong**        |   3.455 |           0.096 |          0.183 |       0.937 |\n",
    "| **remotely~related**         |   2.970 |           0.024 |          0.029 |       0.935 |\n",
    "\n",
    "\n",
    "### Loading `direct` AM scores\n",
    "\n",
    " Path: `assoc_df/adv_adj/ANYdirect/extra/AdvAdj_ALL_any-direct_final-freq_min50x_extra.parq`\n",
    "\n",
    "\n",
    "_Bigrams with the highest `LRC` value for each adverb_\n",
    "\n",
    "\n",
    "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
    "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
    "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
    "| **immediately~accretive**   |   8.539 |           0.226 |          0.450 |       0.997 |\n",
    "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
    "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
    "| **ever~olympic**            |   8.231 |           0.222 |          0.442 |       0.996 |\n",
    "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
    "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
    "| **any~happier**             |   6.748 |           0.043 |          0.058 |       0.992 |\n",
    "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
    "\n",
    "\n",
    "_Bigrams with the highest `deltaP_mean` value for each adverb_\n",
    "\n",
    "\n",
    "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
    "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
    "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
    "| **immediately~appealable**  |   8.409 |           0.273 |          0.545 |       0.998 |\n",
    "| **ever~quarterly**          |   8.049 |           0.224 |          0.446 |       0.997 |\n",
    "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
    "| **any~better**              |   5.227 |           0.171 |          0.325 |       0.974 |\n",
    "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
    "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
    "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
    "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
    "\n",
    "\n",
    "_Bigrams with the highest `deltaP_max` value for each adverb_\n",
    "\n",
    "\n",
    "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
    "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
    "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
    "| **immediately~appealable**  |   8.409 |           0.273 |          0.545 |       0.998 |\n",
    "| **ever~quarterly**          |   8.049 |           0.224 |          0.446 |       0.997 |\n",
    "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
    "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
    "| **any~better**              |   5.227 |           0.171 |          0.325 |       0.974 |\n",
    "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
    "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
    "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
    "\n",
    "\n",
    "_Bigrams with the highest `unexp_r` value for each adverb_\n",
    "\n",
    "\n",
    "|                             |   `LRC` |   `deltaP_mean` |   `deltaP_max` |   `unexp_r` |\n",
    "|:----------------------------|--------:|----------------:|---------------:|------------:|\n",
    "| **remotely~detonated**      |  12.691 |           0.440 |          0.876 |       1.000 |\n",
    "| **yet~unborn**              |  10.135 |           0.360 |          0.715 |       0.998 |\n",
    "| **inherently~governmental** |   8.919 |           0.169 |          0.332 |       0.998 |\n",
    "| **immediately~appealable**  |   8.409 |           0.273 |          0.545 |       0.998 |\n",
    "| **terribly~awry**           |   8.019 |           0.131 |          0.259 |       0.997 |\n",
    "| **exactly~alike**           |   8.461 |           0.145 |          0.243 |       0.997 |\n",
    "| **ever~quarterly**          |   8.049 |           0.224 |          0.446 |       0.997 |\n",
    "| **that~purported**          |   8.416 |           0.391 |          0.782 |       0.996 |\n",
    "| **necessarily~indicative**  |   8.025 |           0.100 |          0.171 |       0.996 |\n",
    "| **any~happier**             |   6.748 |           0.043 |          0.058 |       0.992 |\n",
    "| **particularly~hard-hit**   |   5.642 |           0.194 |          0.387 |       0.982 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Top 15 *context-blind* `LRC` values across all adverbs and datasets\n",
      "\n",
      "\n",
      "|                                        |   `f` |   `dP1` |   `LRC` |   `P1` |      `G2` | `l1`        | `l2`         |    `f1` |   `f2` |        `N` |   `exp_f` |   `unexp_f` |   `unexp_r` |   `odds_r_disc` |   `t` |   `MI` |   `dP2` |   `P2` |   `deltaP_max` |   `deltaP_mean` | `dataset`   |\n",
      "|:---------------------------------------|------:|--------:|--------:|-------:|----------:|:------------|:-------------|--------:|-------:|-----------:|----------:|------------:|------------:|----------------:|------:|-------:|--------:|-------:|---------------:|----------------:|:------------|\n",
      "| **[_direct_] remotely~detonated**      |    78 |    0.88 |   12.69 |   0.88 |  1,243.75 | remotely    | detonated    |  16,426 |     89 | 72,839,589 |      0.02 |       77.98 |        1.00 |            4.48 |  8.83 |   3.59 |    0.00 |   0.00 |           0.88 |            0.44 | direct      |\n",
      "| **[_direct_] yet~unborn**              |   372 |    0.72 |   10.14 |   0.72 |  4,319.00 | yet         | unborn       |  95,763 |    519 | 72,839,589 |      0.68 |      371.32 |        1.00 |            3.28 | 19.25 |   2.74 |    0.00 |   0.00 |           0.72 |            0.36 | direct      |\n",
      "| **[_direct_] inherently~governmental** |   253 |    0.33 |    8.92 |   0.33 |  2,742.59 | inherently  | governmental |  47,803 |    761 | 72,839,589 |      0.50 |      252.50 |        1.00 |            2.88 | 15.87 |   2.70 |    0.01 |   0.01 |           0.33 |            0.17 | direct      |\n",
      "| **[_direct_] remotely~exploitable**    |   145 |    0.15 |    8.79 |   0.15 |  1,613.38 | remotely    | exploitable  |  16,426 |    986 | 72,839,589 |      0.22 |      144.78 |        1.00 |            2.89 | 12.02 |   2.81 |    0.01 |   0.01 |           0.15 |            0.08 | direct      |\n",
      "| **[_direct_] immediately~accretive**   |   236 |    0.45 |    8.54 |   0.45 |  2,406.67 | immediately | accretive    |  96,973 |    523 | 72,839,589 |      0.70 |      235.30 |        1.00 |            2.79 | 15.32 |   2.53 |    0.00 |   0.00 |           0.45 |            0.23 | direct      |\n",
      "| **[_direct_] exactly~alike**           | 2,768 |    0.24 |    8.46 |   0.24 | 26,963.38 | exactly     | alike        |  58,643 | 11,375 | 72,839,589 |      9.16 |    2,758.84 |        1.00 |            2.62 | 52.44 |   2.48 |    0.05 |   0.05 |           0.24 |            0.14 | direct      |\n",
      "| **[_direct_] that~purported**          |    73 |    0.78 |    8.42 |   0.78 |    758.47 | that        | purported    | 208,262 |     93 | 72,839,589 |      0.27 |       72.73 |        1.00 |            3.10 |  8.51 |   2.44 |    0.00 |   0.00 |           0.78 |            0.39 | direct      |\n",
      "| **[_direct_] immediately~appealable**  |    76 |    0.55 |    8.41 |   0.55 |    815.23 | immediately | appealable   |  96,973 |    139 | 72,839,589 |      0.19 |       75.81 |        1.00 |            2.96 |  8.70 |   2.61 |    0.00 |   0.00 |           0.55 |            0.27 | direct      |\n",
      "| **[_direct_] immediately~adjacent**    | 1,595 |    0.33 |    8.31 |   0.34 | 15,089.64 | immediately | adjacent     |  96,973 |  4,756 | 72,839,589 |      6.33 |    1,588.67 |        1.00 |            2.59 | 39.78 |   2.40 |    0.02 |   0.02 |           0.33 |            0.18 | direct      |\n",
      "| **[_direct_] yet~unnamed**             |   736 |    0.35 |    8.29 |   0.35 |  7,048.15 | yet         | unnamed      |  95,763 |  2,107 | 72,839,589 |      2.77 |      733.23 |        1.00 |            2.61 | 27.03 |   2.42 |    0.01 |   0.01 |           0.35 |            0.18 | direct      |\n",
      "| **[_direct_] ever~olympic**            |   218 |    0.44 |    8.23 |   0.44 |  2,141.80 | ever        | olympic      | 114,075 |    492 | 72,839,589 |      0.77 |      217.23 |        1.00 |            2.71 | 14.71 |   2.45 |    0.00 |   0.00 |           0.44 |            0.22 | direct      |\n",
      "| **[_direct_] ever~quarterly**          |   137 |    0.45 |    8.05 |   0.45 |  1,349.65 | ever        | quarterly    | 114,075 |    306 | 72,839,589 |      0.48 |      136.52 |        1.00 |            2.71 | 11.66 |   2.46 |    0.00 |   0.00 |           0.45 |            0.22 | direct      |\n",
      "| **[_direct_] necessarily~indicative**  | 1,400 |    0.17 |    8.03 |   0.17 | 13,028.00 | necessarily | indicative   |  48,947 |  8,148 | 72,839,589 |      5.48 |    1,394.52 |        1.00 |            2.50 | 37.27 |   2.41 |    0.03 |   0.03 |           0.17 |            0.10 | direct      |\n",
      "| **[_direct_] terribly~awry**           |   180 |    0.26 |    8.02 |   0.26 |  1,770.97 | terribly    | awry         |  58,964 |    692 | 72,839,589 |      0.56 |      179.44 |        1.00 |            2.64 | 13.37 |   2.51 |    0.00 |   0.00 |           0.26 |            0.13 | direct      |\n",
      "| **[_direct_] yet~unspecified**         |   204 |    0.33 |    7.86 |   0.34 |  1,933.20 | yet         | unspecified  |  95,763 |    607 | 72,839,589 |      0.80 |      203.20 |        1.00 |            2.59 | 14.23 |   2.41 |    0.00 |   0.00 |           0.33 |            0.17 | direct      |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "blam_df = pd.concat(blam_dfs.values()).sort_values(blind_priority_cols[0], ascending=False)\n",
    "# adv_adj_am = adv_adj_am.loc[adv_adj_am.l1.isin(adv_list), :]\n",
    "print(f'### Top 15 *context-blind* `{blind_priority_cols[0]}` values across all adverbs and datasets\\n')\n",
    "nb_show_table(blam_df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 15 *context-blind* `LRC` values across all adverbs and datasets\n",
    "\n",
    "\n",
    "|                                        |   `f` |   `dP1` |   `LRC` |   `P1` |      `G2` | `l1`        | `l2`         |    `f1` |   `f2` |        `N` |   `exp_f` |   `unexp_f` |   `unexp_r` |   `odds_r_disc` |   `t` |   `MI` |   `dP2` |   `P2` |   `deltaP_max` |   `deltaP_mean` | `dataset`   |\n",
    "|:---------------------------------------|------:|--------:|--------:|-------:|----------:|:------------|:-------------|--------:|-------:|-----------:|----------:|------------:|------------:|----------------:|------:|-------:|--------:|-------:|---------------:|----------------:|:------------|\n",
    "| **[_direct_] remotely~detonated**      |    78 |    0.88 |   12.69 |   0.88 |  1,243.75 | remotely    | detonated    |  16,426 |     89 | 72,839,589 |      0.02 |       77.98 |        1.00 |            4.48 |  8.83 |   3.59 |    0.00 |   0.00 |           0.88 |            0.44 | direct      |\n",
    "| **[_direct_] yet~unborn**              |   372 |    0.72 |   10.14 |   0.72 |  4,319.00 | yet         | unborn       |  95,763 |    519 | 72,839,589 |      0.68 |      371.32 |        1.00 |            3.28 | 19.25 |   2.74 |    0.00 |   0.00 |           0.72 |            0.36 | direct      |\n",
    "| **[_direct_] inherently~governmental** |   253 |    0.33 |    8.92 |   0.33 |  2,742.59 | inherently  | governmental |  47,803 |    761 | 72,839,589 |      0.50 |      252.50 |        1.00 |            2.88 | 15.87 |   2.70 |    0.01 |   0.01 |           0.33 |            0.17 | direct      |\n",
    "| **[_direct_] remotely~exploitable**    |   145 |    0.15 |    8.79 |   0.15 |  1,613.38 | remotely    | exploitable  |  16,426 |    986 | 72,839,589 |      0.22 |      144.78 |        1.00 |            2.89 | 12.02 |   2.81 |    0.01 |   0.01 |           0.15 |            0.08 | direct      |\n",
    "| **[_direct_] immediately~accretive**   |   236 |    0.45 |    8.54 |   0.45 |  2,406.67 | immediately | accretive    |  96,973 |    523 | 72,839,589 |      0.70 |      235.30 |        1.00 |            2.79 | 15.32 |   2.53 |    0.00 |   0.00 |           0.45 |            0.23 | direct      |\n",
    "| **[_direct_] exactly~alike**           | 2,768 |    0.24 |    8.46 |   0.24 | 26,963.38 | exactly     | alike        |  58,643 | 11,375 | 72,839,589 |      9.16 |    2,758.84 |        1.00 |            2.62 | 52.44 |   2.48 |    0.05 |   0.05 |           0.24 |            0.14 | direct      |\n",
    "| **[_direct_] that~purported**          |    73 |    0.78 |    8.42 |   0.78 |    758.47 | that        | purported    | 208,262 |     93 | 72,839,589 |      0.27 |       72.73 |        1.00 |            3.10 |  8.51 |   2.44 |    0.00 |   0.00 |           0.78 |            0.39 | direct      |\n",
    "| **[_direct_] immediately~appealable**  |    76 |    0.55 |    8.41 |   0.55 |    815.23 | immediately | appealable   |  96,973 |    139 | 72,839,589 |      0.19 |       75.81 |        1.00 |            2.96 |  8.70 |   2.61 |    0.00 |   0.00 |           0.55 |            0.27 | direct      |\n",
    "| **[_direct_] immediately~adjacent**    | 1,595 |    0.33 |    8.31 |   0.34 | 15,089.64 | immediately | adjacent     |  96,973 |  4,756 | 72,839,589 |      6.33 |    1,588.67 |        1.00 |            2.59 | 39.78 |   2.40 |    0.02 |   0.02 |           0.33 |            0.18 | direct      |\n",
    "| **[_direct_] yet~unnamed**             |   736 |    0.35 |    8.29 |   0.35 |  7,048.15 | yet         | unnamed      |  95,763 |  2,107 | 72,839,589 |      2.77 |      733.23 |        1.00 |            2.61 | 27.03 |   2.42 |    0.01 |   0.01 |           0.35 |            0.18 | direct      |\n",
    "| **[_direct_] ever~olympic**            |   218 |    0.44 |    8.23 |   0.44 |  2,141.80 | ever        | olympic      | 114,075 |    492 | 72,839,589 |      0.77 |      217.23 |        1.00 |            2.71 | 14.71 |   2.45 |    0.00 |   0.00 |           0.44 |            0.22 | direct      |\n",
    "| **[_direct_] ever~quarterly**          |   137 |    0.45 |    8.05 |   0.45 |  1,349.65 | ever        | quarterly    | 114,075 |    306 | 72,839,589 |      0.48 |      136.52 |        1.00 |            2.71 | 11.66 |   2.46 |    0.00 |   0.00 |           0.45 |            0.22 | direct      |\n",
    "| **[_direct_] necessarily~indicative**  | 1,400 |    0.17 |    8.03 |   0.17 | 13,028.00 | necessarily | indicative   |  48,947 |  8,148 | 72,839,589 |      5.48 |    1,394.52 |        1.00 |            2.50 | 37.27 |   2.41 |    0.03 |   0.03 |           0.17 |            0.10 | direct      |\n",
    "| **[_direct_] terribly~awry**           |   180 |    0.26 |    8.02 |   0.26 |  1,770.97 | terribly    | awry         |  58,964 |    692 | 72,839,589 |      0.56 |      179.44 |        1.00 |            2.64 | 13.37 |   2.51 |    0.00 |   0.00 |           0.26 |            0.13 | direct      |\n",
    "| **[_direct_] yet~unspecified**         |   204 |    0.33 |    7.86 |   0.34 |  1,933.20 | yet         | unspecified  |  95,763 |    607 | 72,839,589 |      0.80 |      203.20 |        1.00 |            2.59 | 14.23 |   2.41 |    0.00 |   0.00 |           0.33 |            0.17 | direct      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f</th>\n",
       "      <th>dP1</th>\n",
       "      <th>LRC</th>\n",
       "      <th>P1</th>\n",
       "      <th>G2</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>N</th>\n",
       "      <th>...</th>\n",
       "      <th>unexp_f</th>\n",
       "      <th>unexp_r</th>\n",
       "      <th>odds_r_disc</th>\n",
       "      <th>t</th>\n",
       "      <th>MI</th>\n",
       "      <th>dP2</th>\n",
       "      <th>P2</th>\n",
       "      <th>deltaP_max</th>\n",
       "      <th>deltaP_mean</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[_mirror_] exactly~alike</th>\n",
       "      <td>88</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7.79</td>\n",
       "      <td>0.21</td>\n",
       "      <td>880.53</td>\n",
       "      <td>exactly</td>\n",
       "      <td>alike</td>\n",
       "      <td>1041</td>\n",
       "      <td>417</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>87.74</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.35</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] immediately~recognizable</th>\n",
       "      <td>58</td>\n",
       "      <td>0.09</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.09</td>\n",
       "      <td>456.86</td>\n",
       "      <td>immediately</td>\n",
       "      <td>recognizable</td>\n",
       "      <td>1195</td>\n",
       "      <td>640</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>57.55</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.56</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] any~closer</th>\n",
       "      <td>69</td>\n",
       "      <td>0.07</td>\n",
       "      <td>5.68</td>\n",
       "      <td>0.07</td>\n",
       "      <td>506.03</td>\n",
       "      <td>any</td>\n",
       "      <td>closer</td>\n",
       "      <td>1197</td>\n",
       "      <td>993</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>68.30</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.05</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] any~better</th>\n",
       "      <td>419</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2,497.00</td>\n",
       "      <td>any</td>\n",
       "      <td>better</td>\n",
       "      <td>1197</td>\n",
       "      <td>14013</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>409.14</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.83</td>\n",
       "      <td>19.99</td>\n",
       "      <td>1.63</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.19</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] terribly~wrong</th>\n",
       "      <td>1726</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.08</td>\n",
       "      <td>9,304.94</td>\n",
       "      <td>terribly</td>\n",
       "      <td>wrong</td>\n",
       "      <td>4610</td>\n",
       "      <td>20880</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>1,669.44</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.72</td>\n",
       "      <td>40.18</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.22</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] ever~available</th>\n",
       "      <td>51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.46</td>\n",
       "      <td>ever</td>\n",
       "      <td>available</td>\n",
       "      <td>5060</td>\n",
       "      <td>10284</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>20.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] terribly~important</th>\n",
       "      <td>79</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-15.18</td>\n",
       "      <td>terribly</td>\n",
       "      <td>important</td>\n",
       "      <td>4610</td>\n",
       "      <td>43671</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.29</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-4.42</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] that~different</th>\n",
       "      <td>78</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-14.90</td>\n",
       "      <td>that</td>\n",
       "      <td>different</td>\n",
       "      <td>5494</td>\n",
       "      <td>36166</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.75</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] particularly~close</th>\n",
       "      <td>145</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>13.09</td>\n",
       "      <td>particularly</td>\n",
       "      <td>close</td>\n",
       "      <td>13003</td>\n",
       "      <td>13874</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[_mirror_] particularly~important</th>\n",
       "      <td>155</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-122.90</td>\n",
       "      <td>particularly</td>\n",
       "      <td>important</td>\n",
       "      <td>13003</td>\n",
       "      <td>43671</td>\n",
       "      <td>1701929</td>\n",
       "      <td>...</td>\n",
       "      <td>-178.65</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-14.35</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>mirror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        f   dP1   LRC   P1       G2  \\\n",
       "key                                                                   \n",
       "[_mirror_] exactly~alike               88  0.21  7.79 0.21   880.53   \n",
       "[_mirror_] immediately~recognizable    58  0.09  5.98 0.09   456.86   \n",
       "[_mirror_] any~closer                  69  0.07  5.68 0.07   506.03   \n",
       "[_mirror_] any~better                 419  0.03  5.06 0.03 2,497.00   \n",
       "[_mirror_] terribly~wrong            1726  0.08  4.86 0.08 9,304.94   \n",
       "...                                   ...   ...   ...  ...      ...   \n",
       "[_mirror_] ever~available              51  0.00  0.00 0.00    11.46   \n",
       "[_mirror_] terribly~important          79 -0.00  0.00 0.00   -15.18   \n",
       "[_mirror_] that~different              78 -0.00  0.00 0.00   -14.90   \n",
       "[_mirror_] particularly~close         145  0.00  0.00 0.01    13.09   \n",
       "[_mirror_] particularly~important     155 -0.00 -0.54 0.00  -122.90   \n",
       "\n",
       "                                               l1            l2     f1     f2  \\\n",
       "key                                                                             \n",
       "[_mirror_] exactly~alike                  exactly         alike   1041    417   \n",
       "[_mirror_] immediately~recognizable   immediately  recognizable   1195    640   \n",
       "[_mirror_] any~closer                         any        closer   1197    993   \n",
       "[_mirror_] any~better                         any        better   1197  14013   \n",
       "[_mirror_] terribly~wrong                terribly         wrong   4610  20880   \n",
       "...                                           ...           ...    ...    ...   \n",
       "[_mirror_] ever~available                    ever     available   5060  10284   \n",
       "[_mirror_] terribly~important            terribly     important   4610  43671   \n",
       "[_mirror_] that~different                    that     different   5494  36166   \n",
       "[_mirror_] particularly~close        particularly         close  13003  13874   \n",
       "[_mirror_] particularly~important    particularly     important  13003  43671   \n",
       "\n",
       "                                           N  ...  unexp_f  unexp_r  \\\n",
       "key                                           ...                     \n",
       "[_mirror_] exactly~alike             1701929  ...    87.74     1.00   \n",
       "[_mirror_] immediately~recognizable  1701929  ...    57.55     0.99   \n",
       "[_mirror_] any~closer                1701929  ...    68.30     0.99   \n",
       "[_mirror_] any~better                1701929  ...   409.14     0.98   \n",
       "[_mirror_] terribly~wrong            1701929  ... 1,669.44     0.97   \n",
       "...                                      ...  ...      ...      ...   \n",
       "[_mirror_] ever~available            1701929  ...    20.42     0.40   \n",
       "[_mirror_] terribly~important        1701929  ...   -39.29    -0.50   \n",
       "[_mirror_] that~different            1701929  ...   -38.75    -0.50   \n",
       "[_mirror_] particularly~close        1701929  ...    39.00     0.27   \n",
       "[_mirror_] particularly~important    1701929  ...  -178.65    -1.15   \n",
       "\n",
       "                                     odds_r_disc      t    MI   dP2   P2  \\\n",
       "key                                                                        \n",
       "[_mirror_] exactly~alike                    2.68   9.35  2.54  0.08 0.08   \n",
       "[_mirror_] immediately~recognizable         2.18   7.56  2.11  0.05 0.05   \n",
       "[_mirror_] any~closer                       2.05   8.22  1.99  0.06 0.06   \n",
       "[_mirror_] any~better                       1.83  19.99  1.63  0.34 0.35   \n",
       "[_mirror_] terribly~wrong                   1.72  40.18  1.48  0.36 0.37   \n",
       "...                                          ...    ...   ...   ...  ...   \n",
       "[_mirror_] ever~available                   0.23   2.86  0.22  0.00 0.01   \n",
       "[_mirror_] terribly~important              -0.18  -4.42 -0.18 -0.01 0.02   \n",
       "[_mirror_] that~different                  -0.18  -4.39 -0.18 -0.01 0.01   \n",
       "[_mirror_] particularly~close               0.14   3.24  0.14  0.00 0.01   \n",
       "[_mirror_] particularly~important          -0.34 -14.35 -0.33 -0.01 0.01   \n",
       "\n",
       "                                     deltaP_max  deltaP_mean  dataset  \n",
       "key                                                                    \n",
       "[_mirror_] exactly~alike                   0.21         0.15   mirror  \n",
       "[_mirror_] immediately~recognizable        0.09         0.07   mirror  \n",
       "[_mirror_] any~closer                      0.07         0.06   mirror  \n",
       "[_mirror_] any~better                      0.34         0.19   mirror  \n",
       "[_mirror_] terribly~wrong                  0.36         0.22   mirror  \n",
       "...                                         ...          ...      ...  \n",
       "[_mirror_] ever~available                  0.00         0.00   mirror  \n",
       "[_mirror_] terribly~important             -0.00        -0.00   mirror  \n",
       "[_mirror_] that~different                 -0.00        -0.00   mirror  \n",
       "[_mirror_] particularly~close              0.00         0.00   mirror  \n",
       "[_mirror_] particularly~important         -0.00        -0.01   mirror  \n",
       "\n",
       "[117 rows x 21 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blam_df.filter(like='mirror', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving as parquet\n",
      "  partitioned by `['adv_form_lower']`...\n",
      "\n",
      "| adv_form_lower   |   count |\n",
      "|:-----------------|--------:|\n",
      "| particularly     | 101,341 |\n",
      "| that             |  53,554 |\n",
      "| yet              |  22,946 |\n",
      "| immediately      |  22,486 |\n",
      "| ever             |  21,513 |\n",
      "| exactly          |  15,047 |\n",
      "| necessarily      |  12,891 |\n",
      "| terribly         |  12,762 |\n",
      "| inherently       |   9,628 |\n",
      "| any              |   7,948 |\n",
      "| remotely         |   3,656 |\n",
      "\n",
      "> no more than 8,600 rows per individual `group-[#].parquet`\n",
      "  - max rows in writing batch = 4,301\n",
      "  - min rows in writing batch = 1,434\n",
      "\n",
      "✓ Sample of bigram tokens for `ALL-Top8[5000]`  successfully saved as  \n",
      "> \"/share/compling/projects/sanpi/results/top_AM/ALL/ALL-Top8/ALL-Top8adv_sample-hits_2024-07-29.parq\"\n",
      "* Total time to write partitioned parquet ⇾  `00:00:00.951`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_window</th>\n",
       "      <th>token_str</th>\n",
       "      <th>adv_form_lower</th>\n",
       "      <th>adj_form_lower</th>\n",
       "      <th>bigram_lower</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apw_eng_19970410_0298_7:6-7</th>\n",
       "      <td>no margin of error was immediately available .</td>\n",
       "      <td>no margin of error was immediately available .</td>\n",
       "      <td>immediately</td>\n",
       "      <td>available</td>\n",
       "      <td>immediately_available</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19970413_0437_16:14-15</th>\n",
       "      <td>it on the way to its first ever championship final appearance .</td>\n",
       "      <td>the Swiss won 2-1 , helping it on the way to its first ever Championship fin...</td>\n",
       "      <td>ever</td>\n",
       "      <td>championship</td>\n",
       "      <td>ever_championship</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19970413_0812_24:5-6</th>\n",
       "      <td>trainer shug mcgaughey was particularly proud of accelerator after four cons...</td>\n",
       "      <td>Trainer Shug McGaughey was particularly proud of Accelerator after four cons...</td>\n",
       "      <td>particularly</td>\n",
       "      <td>proud</td>\n",
       "      <td>particularly_proud</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19970414_0374_3:5-6</th>\n",
       "      <td>`` this project is particularly important because of the economic conditions in</td>\n",
       "      <td>`` This project is particularly important because of the economic conditions...</td>\n",
       "      <td>particularly</td>\n",
       "      <td>important</td>\n",
       "      <td>particularly_important</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19970414_0713_4:5-6</th>\n",
       "      <td>no other information was immediately available on the meeting between the two</td>\n",
       "      <td>no other information was immediately available on the meeting between the tw...</td>\n",
       "      <td>immediately</td>\n",
       "      <td>available</td>\n",
       "      <td>immediately_available</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_3.09057_x49184_235:13-15-16</th>\n",
       "      <td>times , but he 's not even that big of a threat to score the</td>\n",
       "      <td>He can be a maestro offensively at times , but he 's not even that big of a ...</td>\n",
       "      <td>that</td>\n",
       "      <td>big</td>\n",
       "      <td>that_big</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_3.10127_x50910_037:13-15-16</th>\n",
       "      <td>questions about things i 'm not even that interested in and i 'm somebody with</td>\n",
       "      <td>Then I get asked a lot of questions about things I 'm not even that interest...</td>\n",
       "      <td>that</td>\n",
       "      <td>interested</td>\n",
       "      <td>that_interested</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_3.10936_x52202_02:20-21-22</th>\n",
       "      <td>and moving for just 3 is n't exactly healthy or in your best interest as</td>\n",
       "      <td>Sitting for over 13 hours a day , sleeping for 8 more , and moving for just ...</td>\n",
       "      <td>exactly</td>\n",
       "      <td>healthy</td>\n",
       "      <td>exactly_healthy</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_3.02086_x37921_02:4-5-6</th>\n",
       "      <td>the code is n't particularly complicated , and uses mostly ideas which</td>\n",
       "      <td>The code is n't particularly complicated , and uses mostly ideas which have ...</td>\n",
       "      <td>particularly</td>\n",
       "      <td>complicated</td>\n",
       "      <td>particularly_complicated</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_3.03493_x40234_100:4-5-6</th>\n",
       "      <td>if you are n't exactly sure or nothing really sticks out ,</td>\n",
       "      <td>If you are n't exactly sure or nothing really sticks out , open up your EQ o...</td>\n",
       "      <td>exactly</td>\n",
       "      <td>sure</td>\n",
       "      <td>exactly_sure</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283772 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                             text_window  \\\n",
       "hit_id                                                                                                                     \n",
       "apw_eng_19970410_0298_7:6-7                                               no margin of error was immediately available .   \n",
       "apw_eng_19970413_0437_16:14-15                           it on the way to its first ever championship final appearance .   \n",
       "apw_eng_19970413_0812_24:5-6             trainer shug mcgaughey was particularly proud of accelerator after four cons...   \n",
       "apw_eng_19970414_0374_3:5-6              `` this project is particularly important because of the economic conditions in   \n",
       "apw_eng_19970414_0713_4:5-6                no other information was immediately available on the meeting between the two   \n",
       "...                                                                                                                  ...   \n",
       "pcc_eng_val_3.09057_x49184_235:13-15-16                     times , but he 's not even that big of a threat to score the   \n",
       "pcc_eng_val_3.10127_x50910_037:13-15-16   questions about things i 'm not even that interested in and i 'm somebody with   \n",
       "pcc_eng_val_3.10936_x52202_02:20-21-22          and moving for just 3 is n't exactly healthy or in your best interest as   \n",
       "pcc_eng_val_3.02086_x37921_02:4-5-6               the code is n't particularly complicated , and uses mostly ideas which   \n",
       "pcc_eng_val_3.03493_x40234_100:4-5-6                          if you are n't exactly sure or nothing really sticks out ,   \n",
       "\n",
       "                                                                                                               token_str  \\\n",
       "hit_id                                                                                                                     \n",
       "apw_eng_19970410_0298_7:6-7                                               no margin of error was immediately available .   \n",
       "apw_eng_19970413_0437_16:14-15           the Swiss won 2-1 , helping it on the way to its first ever Championship fin...   \n",
       "apw_eng_19970413_0812_24:5-6             Trainer Shug McGaughey was particularly proud of Accelerator after four cons...   \n",
       "apw_eng_19970414_0374_3:5-6              `` This project is particularly important because of the economic conditions...   \n",
       "apw_eng_19970414_0713_4:5-6              no other information was immediately available on the meeting between the tw...   \n",
       "...                                                                                                                  ...   \n",
       "pcc_eng_val_3.09057_x49184_235:13-15-16  He can be a maestro offensively at times , but he 's not even that big of a ...   \n",
       "pcc_eng_val_3.10127_x50910_037:13-15-16  Then I get asked a lot of questions about things I 'm not even that interest...   \n",
       "pcc_eng_val_3.10936_x52202_02:20-21-22   Sitting for over 13 hours a day , sleeping for 8 more , and moving for just ...   \n",
       "pcc_eng_val_3.02086_x37921_02:4-5-6      The code is n't particularly complicated , and uses mostly ideas which have ...   \n",
       "pcc_eng_val_3.03493_x40234_100:4-5-6     If you are n't exactly sure or nothing really sticks out , open up your EQ o...   \n",
       "\n",
       "                                        adv_form_lower adj_form_lower  \\\n",
       "hit_id                                                                  \n",
       "apw_eng_19970410_0298_7:6-7                immediately      available   \n",
       "apw_eng_19970413_0437_16:14-15                    ever   championship   \n",
       "apw_eng_19970413_0812_24:5-6              particularly          proud   \n",
       "apw_eng_19970414_0374_3:5-6               particularly      important   \n",
       "apw_eng_19970414_0713_4:5-6                immediately      available   \n",
       "...                                                ...            ...   \n",
       "pcc_eng_val_3.09057_x49184_235:13-15-16           that            big   \n",
       "pcc_eng_val_3.10127_x50910_037:13-15-16           that     interested   \n",
       "pcc_eng_val_3.10936_x52202_02:20-21-22         exactly        healthy   \n",
       "pcc_eng_val_3.02086_x37921_02:4-5-6       particularly    complicated   \n",
       "pcc_eng_val_3.03493_x40234_100:4-5-6           exactly           sure   \n",
       "\n",
       "                                                     bigram_lower polarity  \n",
       "hit_id                                                                      \n",
       "apw_eng_19970410_0298_7:6-7                 immediately_available      pos  \n",
       "apw_eng_19970413_0437_16:14-15                  ever_championship      pos  \n",
       "apw_eng_19970413_0812_24:5-6                   particularly_proud      pos  \n",
       "apw_eng_19970414_0374_3:5-6                particularly_important      pos  \n",
       "apw_eng_19970414_0713_4:5-6                 immediately_available      pos  \n",
       "...                                                           ...      ...  \n",
       "pcc_eng_val_3.09057_x49184_235:13-15-16                  that_big      neg  \n",
       "pcc_eng_val_3.10127_x50910_037:13-15-16           that_interested      neg  \n",
       "pcc_eng_val_3.10936_x52202_02:20-21-22            exactly_healthy      neg  \n",
       "pcc_eng_val_3.02086_x37921_02:4-5-6      particularly_complicated      neg  \n",
       "pcc_eng_val_3.03493_x40234_100:4-5-6                 exactly_sure      neg  \n",
       "\n",
       "[283772 rows x 6 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "hits_df = load_hit_table(\n",
    "    adv_list, pos_hits=POS_HITS_PATH, neg_hits=NEG_HITS_PATH)\n",
    "hits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving as parquet\n",
    "  partitioned by `['adv_form_lower']`...\n",
    "\n",
    "| adv_form_lower   |   count |\n",
    "|:-----------------|--------:|\n",
    "| particularly     | 101,341 |\n",
    "| that             |  53,554 |\n",
    "| yet              |  22,946 |\n",
    "| immediately      |  22,486 |\n",
    "| ever             |  21,513 |\n",
    "| exactly          |  15,047 |\n",
    "| necessarily      |  12,891 |\n",
    "| terribly         |  12,762 |\n",
    "| inherently       |   9,628 |\n",
    "| any              |   7,948 |\n",
    "| remotely         |   3,656 |\n",
    "\n",
    "> no more than 8,600 rows per individual `group-[#].parquet`\n",
    "  - max rows in writing batch = 4,301\n",
    "  - min rows in writing batch = 1,434\n",
    "\n",
    "✓ Sample of bigram tokens for `ALL-Top8[5000]`  successfully saved as  \n",
    "> \"/share/compling/projects/sanpi/results/top_AM/ALL/ALL-Top8/ALL-Top8adv_sample-hits_2024-07-29.parq\"\n",
    "* Total time to write partitioned parquet ⇾  `00:00:00.113`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_show_table(hits_df.sort_index(axis=1).sample(13).iloc[:, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "nb_show_table(hits_df.sort_index(axis=1).sample(13).iloc[:, :4])\n",
    "```\n",
    "\n",
    "|                                               | `adj_form_lower` | `adv_form_lower` | `bigrlower`             | `text_window`                                                                                     |\n",
    "|:----------------------------------------------|:-----------------|:-----------------|:------------------------|:--------------------------------------------------------------------------------------------------|\n",
    "| **pcc_eng_26_068.6567_x1093885_3:5-6**        | popular          | ever             | ever_popular            | spacious mid terrace in ever popular location                                                     |\n",
    "| **pcc_eng_28_027.3336_x0425352_21:4-5-6**     | hea              | necessarily      | necessarily_hea         | the end is not necessarily hea , but it 's a solid                                                |\n",
    "| **nyt_eng_19950702_0006_49:8-9**              | ugly             | particularly     | particularly_ugly       | he 'll make a face after a particularly ugly swing .                                              |\n",
    "| **pcc_eng_24_010.6893_x0156421_04:12-13**     | effective        | particularly     | particularly_effective  | analog to album pre-sales , and are particularly effective for streaming fans who may have        |\n",
    "| **nyt_eng_19960708_0547_13:26-27-28**         | addictive        | necessarily      | necessarily_addictive   | and -rrb- `` says cigarettes are n't necessarily addictive ; some say milk is bad                 |\n",
    "| **nyt_eng_20040127_0066_8:25-26**             | disturbing       | particularly     | particularly_disturbing | police department employees , but it is particularly disturbing when it involves a scientist who  |\n",
    "| **pcc_eng_25_007.0915_x0098850_12:14-15-16**  | useful           | particularly     | particularly_useful     | value and blog traffic but are not particularly useful for true leaders , who invent              |\n",
    "| **pcc_eng_14_041.1082_x0648041_080:08-09-10** | unique           | exactly          | exactly_unique          | award - winning development project is not exactly unique .                                       |\n",
    "| **pcc_eng_00_050.2537_x0795891_224:10-11**    | concerned        | particularly     | particularly_concerned  | light of climate change , i am particularly concerned about the power that corporations have      |\n",
    "| **pcc_eng_22_077.2483_x1232367_62:4-5**       | true             | particularly     | particularly_true       | that could prove particularly true for \" black ops . \"                                            |\n",
    "| **nyt_eng_20060308_0284_27:4-5-6**            | different        | that             | that_different          | `` it 's not that different from playing shortstop . ''                                           |\n",
    "| **nyt_eng_20070425_0212_10:1-2-3**            | wrong            | necessarily      | necessarily_wrong       | nothing necessarily wrong with that .                                                             |\n",
    "| **pcc_eng_07_027.5704_x0429654_14:27-28**     | vulnerable       | particularly     | particularly_vulnerable | to construct sewers and reduce flooding in particularly vulnerable areas in the borough of queens |\n",
    "\n",
    "\n",
    "\n",
    "## Set adverb and collect specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_SET</th>\n",
       "      <th>f_SET</th>\n",
       "      <th>dP1_SET</th>\n",
       "      <th>P1_SET</th>\n",
       "      <th>LRC_SET</th>\n",
       "      <th>G2_SET</th>\n",
       "      <th>MI_SET</th>\n",
       "      <th>odds_r_disc_SET</th>\n",
       "      <th>t_SET</th>\n",
       "      <th>N_SET</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_t</th>\n",
       "      <th>mean_N</th>\n",
       "      <th>mean_f1</th>\n",
       "      <th>mean_f2</th>\n",
       "      <th>mean_expF</th>\n",
       "      <th>mean_unexpF</th>\n",
       "      <th>r_f_MIR</th>\n",
       "      <th>r_N_MIR</th>\n",
       "      <th>r_f1_MIR</th>\n",
       "      <th>r_f2_MIR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adv</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>necessarily</th>\n",
       "      <td>NEGany~necessarily</td>\n",
       "      <td>42595</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.87</td>\n",
       "      <td>7.10</td>\n",
       "      <td>230,257.34</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.17</td>\n",
       "      <td>196.05</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>110.48</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>25,027.00</td>\n",
       "      <td>1,161.20</td>\n",
       "      <td>20,617.80</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>NEGany~that</td>\n",
       "      <td>164768</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6.34</td>\n",
       "      <td>831,137.25</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.94</td>\n",
       "      <td>383.56</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>217.42</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>106,878.00</td>\n",
       "      <td>5,007.91</td>\n",
       "      <td>79,530.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exactly</th>\n",
       "      <td>NEGany~exactly</td>\n",
       "      <td>43813</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.94</td>\n",
       "      <td>210,126.60</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.82</td>\n",
       "      <td>197.11</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>109.68</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>29,842.00</td>\n",
       "      <td>1,366.77</td>\n",
       "      <td>20,946.23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>NEGany~any</td>\n",
       "      <td>15384</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4.07</td>\n",
       "      <td>50,880.96</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>111.95</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>69.16</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>17,789.50</td>\n",
       "      <td>851.61</td>\n",
       "      <td>7,373.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remotely</th>\n",
       "      <td>NEGany~remotely</td>\n",
       "      <td>5661</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>3.40</td>\n",
       "      <td>15,284.49</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.06</td>\n",
       "      <td>65.73</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>49.63</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>9,383.50</td>\n",
       "      <td>558.48</td>\n",
       "      <td>3,192.02</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever</th>\n",
       "      <td>NEGany~ever</td>\n",
       "      <td>5932</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>183.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>12.49</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>34.23</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>59,567.50</td>\n",
       "      <td>2,918.83</td>\n",
       "      <td>2,401.67</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yet</th>\n",
       "      <td>NEGany~yet</td>\n",
       "      <td>51867</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4.65</td>\n",
       "      <td>197,610.98</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.42</td>\n",
       "      <td>209.42</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>109.75</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>48,289.00</td>\n",
       "      <td>2,156.07</td>\n",
       "      <td>23,937.43</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immediately</th>\n",
       "      <td>NEGany~immediately</td>\n",
       "      <td>56099</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.86</td>\n",
       "      <td>224,059.55</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.49</td>\n",
       "      <td>219.01</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>114.44</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>49,084.00</td>\n",
       "      <td>2,215.00</td>\n",
       "      <td>26,036.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>particularly</th>\n",
       "      <td>NEGany~particularly</td>\n",
       "      <td>55527</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.38</td>\n",
       "      <td>37,272.74</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>140.66</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>106.81</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>263,335.50</td>\n",
       "      <td>12,304.83</td>\n",
       "      <td>20,080.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inherently</th>\n",
       "      <td>NEGany~inherently</td>\n",
       "      <td>6743</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.75</td>\n",
       "      <td>7,022.02</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>56.75</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>46.91</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>26,468.00</td>\n",
       "      <td>1,481.33</td>\n",
       "      <td>3,322.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terribly</th>\n",
       "      <td>NEGany~terribly</td>\n",
       "      <td>17949</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>3.19</td>\n",
       "      <td>43,741.44</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.98</td>\n",
       "      <td>114.80</td>\n",
       "      <td>72839589</td>\n",
       "      <td>...</td>\n",
       "      <td>67.21</td>\n",
       "      <td>37270759</td>\n",
       "      <td>1732696</td>\n",
       "      <td>31,787.00</td>\n",
       "      <td>1,679.65</td>\n",
       "      <td>8,078.35</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          key_SET   f_SET  dP1_SET  P1_SET  LRC_SET  \\\n",
       "adv                                                                   \n",
       "necessarily    NEGany~necessarily   42595     0.83    0.87     7.10   \n",
       "that                  NEGany~that  164768     0.75    0.79     6.34   \n",
       "exactly            NEGany~exactly   43813     0.70    0.75     5.94   \n",
       "any                    NEGany~any   15384     0.40    0.45     4.07   \n",
       "remotely          NEGany~remotely    5661     0.30    0.34     3.40   \n",
       "ever                  NEGany~ever    5932     0.01    0.05     0.16   \n",
       "yet                    NEGany~yet   51867     0.50    0.54     4.65   \n",
       "immediately    NEGany~immediately   56099     0.54    0.58     4.86   \n",
       "particularly  NEGany~particularly   55527     0.07    0.11     1.38   \n",
       "inherently      NEGany~inherently    6743     0.10    0.14     1.75   \n",
       "terribly          NEGany~terribly   17949     0.26    0.30     3.19   \n",
       "\n",
       "                 G2_SET  MI_SET  odds_r_disc_SET  t_SET     N_SET  ...  \\\n",
       "adv                                                                ...   \n",
       "necessarily  230,257.34    1.30             2.17 196.05  72839589  ...   \n",
       "that         831,137.25    1.26             1.94 383.56  72839589  ...   \n",
       "exactly      210,126.60    1.23             1.82 197.11  72839589  ...   \n",
       "any           50,880.96    1.01             1.25 111.95  72839589  ...   \n",
       "remotely      15,284.49    0.90             1.06  65.73  72839589  ...   \n",
       "ever             183.92    0.08             0.08  12.49  72839589  ...   \n",
       "yet          197,610.98    1.09             1.42 209.42  72839589  ...   \n",
       "immediately  224,059.55    1.12             1.49 219.01  72839589  ...   \n",
       "particularly  37,272.74    0.39             0.43 140.66  72839589  ...   \n",
       "inherently     7,022.02    0.51             0.56  56.75  72839589  ...   \n",
       "terribly      43,741.44    0.84             0.98 114.80  72839589  ...   \n",
       "\n",
       "              mean_t    mean_N  mean_f1    mean_f2 mean_expF mean_unexpF  \\\n",
       "adv                                                                        \n",
       "necessarily   110.48  37270759  1732696  25,027.00  1,161.20   20,617.80   \n",
       "that          217.42  37270759  1732696 106,878.00  5,007.91   79,530.09   \n",
       "exactly       109.68  37270759  1732696  29,842.00  1,366.77   20,946.23   \n",
       "any            69.16  37270759  1732696  17,789.50    851.61    7,373.39   \n",
       "remotely       49.63  37270759  1732696   9,383.50    558.48    3,192.02   \n",
       "ever           34.23  37270759  1732696  59,567.50  2,918.83    2,401.67   \n",
       "yet           109.75  37270759  1732696  48,289.00  2,156.07   23,937.43   \n",
       "immediately   114.44  37270759  1732696  49,084.00  2,215.00   26,036.00   \n",
       "particularly  106.81  37270759  1732696 263,335.50 12,304.83   20,080.17   \n",
       "inherently     46.91  37270759  1732696  26,468.00  1,481.33    3,322.17   \n",
       "terribly       67.21  37270759  1732696  31,787.00  1,679.65    8,078.35   \n",
       "\n",
       "              r_f_MIR  r_N_MIR  r_f1_MIR  r_f2_MIR  \n",
       "adv                                                 \n",
       "necessarily      0.02     0.02      0.09      0.02  \n",
       "that             0.03     0.02      0.09      0.03  \n",
       "exactly          0.02     0.02      0.09      0.02  \n",
       "any              0.07     0.02      0.09      0.03  \n",
       "remotely         0.33     0.02      0.09      0.14  \n",
       "ever             0.79     0.02      0.09      0.04  \n",
       "yet              0.01     0.02      0.09      0.01  \n",
       "immediately      0.01     0.02      0.09      0.01  \n",
       "particularly     0.17     0.02      0.09      0.03  \n",
       "inherently       0.42     0.02      0.09      0.11  \n",
       "terribly         0.09     0.02      0.09      0.08  \n",
       "\n",
       "[11 rows x 47 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv_adj_am' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madv_adj_am\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adv_adj_am' is not defined"
     ]
    }
   ],
   "source": [
    "adv_adj_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## *necessarily*\n",
      "\n",
      "\n",
      "|                                |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |   `f1` |    `f2` | `l1`        | `l2`           |\n",
      "|:-------------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|-------:|--------:|:------------|:---------------|\n",
      "| **necessarily~indicative**     | 1,400 |      5.48 |    1,394.52 |    0.17 |   0.17 |    0.03 |   0.03 |    8.03 | 13,028.00 | 72,839,589 | 48,947 |   8,148 | necessarily | indicative     |\n",
      "| **necessarily~cause**          |    52 |      0.50 |       51.50 |    0.07 |   0.07 |    0.00 |   0.00 |    5.46 |    383.88 | 72,839,589 | 48,947 |     743 | necessarily | cause          |\n",
      "| **necessarily~representative** |   492 |     12.33 |      479.67 |    0.03 |   0.03 |    0.01 |   0.01 |    4.97 |  2,685.16 | 72,839,589 | 48,947 |  18,355 | necessarily | representative |\n",
      "| **necessarily~predictive**     |    58 |      1.63 |       56.37 |    0.02 |   0.02 |    0.00 |   0.00 |    3.95 |    303.20 | 72,839,589 | 48,947 |   2,421 | necessarily | predictive     |\n",
      "| **necessarily~incompatible**   |   112 |      3.58 |      108.42 |    0.02 |   0.02 |    0.00 |   0.00 |    4.14 |    556.70 | 72,839,589 | 48,947 |   5,332 | necessarily | incompatible   |\n",
      "| **necessarily~synonymous**     |   167 |      5.54 |      161.46 |    0.02 |   0.02 |    0.00 |   0.00 |    4.25 |    818.37 | 72,839,589 | 48,947 |   8,245 | necessarily | synonymous     |\n",
      "| **necessarily~reflective**     |   183 |      7.55 |      175.45 |    0.02 |   0.02 |    0.00 |   0.00 |    3.97 |    819.22 | 72,839,589 | 48,947 |  11,237 | necessarily | reflective     |\n",
      "| **necessarily~incomplete**     |   124 |      5.13 |      118.87 |    0.02 |   0.02 |    0.00 |   0.00 |    3.81 |    554.34 | 72,839,589 | 48,947 |   7,634 | necessarily | incomplete     |\n",
      "| **necessarily~true**           | 3,460 |    155.66 |    3,304.34 |    0.01 |   0.01 |    0.07 |   0.07 |    4.35 | 15,129.32 | 72,839,589 | 48,947 | 231,639 | necessarily | true           |\n",
      "| **necessarily~wiser**          |    51 |      2.44 |       48.56 |    0.01 |   0.01 |    0.00 |   0.00 |    3.07 |    213.67 | 72,839,589 | 48,947 |   3,630 | necessarily | wiser          |\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10 entries, necessarily~indicative to necessarily~wiser\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   f        10 non-null     Int32  \n",
      " 1   exp_f    10 non-null     Float64\n",
      " 2   unexp_f  10 non-null     Float64\n",
      " 3   dP1      10 non-null     Float32\n",
      " 4   P1       10 non-null     Float32\n",
      " 5   dP2      10 non-null     Float32\n",
      " 6   P2       10 non-null     Float32\n",
      " 7   LRC      10 non-null     Float32\n",
      " 8   G2       10 non-null     Float64\n",
      " 9   N        10 non-null     Int32  \n",
      " 10  f1       10 non-null     Int32  \n",
      " 11  f2       10 non-null     Int32  \n",
      " 12  l1       10 non-null     string \n",
      " 13  l2       10 non-null     string \n",
      "dtypes: Float32(5), Float64(3), Int32(4), string(2)\n",
      "memory usage: 960.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10 entries, necessarily~indicative to necessarily~wiser\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   f        10 non-null     Int32  \n",
      " 1   exp_f    10 non-null     Float64\n",
      " 2   unexp_f  10 non-null     Float64\n",
      " 3   dP1      10 non-null     Float32\n",
      " 4   P1       10 non-null     Float32\n",
      " 5   dP2      10 non-null     Float32\n",
      " 6   P2       10 non-null     Float32\n",
      " 7   LRC      10 non-null     Float32\n",
      " 8   G2       10 non-null     Float64\n",
      " 9   N        10 non-null     Int32  \n",
      " 10  f1       10 non-null     Int32  \n",
      " 11  f2       10 non-null     Int32  \n",
      " 12  l1       10 non-null     string \n",
      " 13  l2       10 non-null     string \n",
      "dtypes: Float32(5), Float64(3), Int32(4), string(2)\n",
      "memory usage: 960.0+ bytes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adverb \u001b[38;5;129;01min\u001b[39;00m adv_am\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43msample_adv_bigrams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43madverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTAG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madv_adj_am\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhits_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhits_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_top_bigrams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigram_floor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBIGRAM_F_FLOOR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sanpi/notebooks/am_notebooks.py:559\u001b[0m, in \u001b[0;36msample_adv_bigrams\u001b[0;34m(adverb, data_tag, amdf, hits_df, n_top_bigrams, verbose, bigram_floor)\u001b[0m\n\u001b[1;32m    555\u001b[0m n_ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_top_bigrams \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# examples = collect_examples(this_adv_am, hits_df, adv=adverb, metric='LRC')\u001b[39;00m\n\u001b[1;32m    557\u001b[0m examples \u001b[38;5;241m=\u001b[39m collect_adv_bigram_ex(\n\u001b[1;32m    558\u001b[0m     amdf\u001b[38;5;241m=\u001b[39mthis_adv_am, hits_df\u001b[38;5;241m=\u001b[39mhits_df, adv\u001b[38;5;241m=\u001b[39madverb,\n\u001b[0;32m--> 559\u001b[0m     metric_selection\u001b[38;5;241m=\u001b[39m\u001b[43mMETRIC_PRIORITY_DICT\u001b[49m[data_tag][:\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    560\u001b[0m     n_examples\u001b[38;5;241m=\u001b[39mn_ex, n_bigrams\u001b[38;5;241m=\u001b[39mn_top_bigrams, \n\u001b[1;32m    561\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose, output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaving Samples in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    564\u001b[0m paths \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/projects/sanpi/notebooks/am_notebooks.py:559\u001b[0m, in \u001b[0;36msample_adv_bigrams\u001b[0;34m(adverb, data_tag, amdf, hits_df, n_top_bigrams, verbose, bigram_floor)\u001b[0m\n\u001b[1;32m    555\u001b[0m n_ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_top_bigrams \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# examples = collect_examples(this_adv_am, hits_df, adv=adverb, metric='LRC')\u001b[39;00m\n\u001b[1;32m    557\u001b[0m examples \u001b[38;5;241m=\u001b[39m collect_adv_bigram_ex(\n\u001b[1;32m    558\u001b[0m     amdf\u001b[38;5;241m=\u001b[39mthis_adv_am, hits_df\u001b[38;5;241m=\u001b[39mhits_df, adv\u001b[38;5;241m=\u001b[39madverb,\n\u001b[0;32m--> 559\u001b[0m     metric_selection\u001b[38;5;241m=\u001b[39m\u001b[43mMETRIC_PRIORITY_DICT\u001b[49m[data_tag][:\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    560\u001b[0m     n_examples\u001b[38;5;241m=\u001b[39mn_ex, n_bigrams\u001b[38;5;241m=\u001b[39mn_top_bigrams, \n\u001b[1;32m    561\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose, output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaving Samples in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    564\u001b[0m paths \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev-sanpi/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for adverb in adv_am.index:\n",
    "    sample_adv_bigrams(\n",
    "        adverb, data_tag=TAG, verbose=True,\n",
    "        amdf=adv_adj_am, hits_df=hits_df,\n",
    "        n_top_bigrams=BK, bigram_floor=BIGRAM_F_FLOOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for adverb in adv_am.index: \n",
    "    sample_adv_bigrams(adverb, adv_adj_am, hits_df)\n",
    "```\n",
    "\n",
    "## *necessarily*\n",
    "\n",
    "\n",
    "|                                |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |   `f1` |    `f2` | `l1`        | `l2`           |\n",
    "|:-------------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|-------:|--------:|:------------|:---------------|\n",
    "| **necessarily~indicative**     | 1,400 |      5.48 |    1,394.52 |    0.17 |   0.17 |    0.03 |   0.03 |    8.03 | 13,028.00 | 72,839,589 | 48,947 |   8,148 | necessarily | indicative     |\n",
    "| **necessarily~cause**          |    52 |      0.50 |       51.50 |    0.07 |   0.07 |    0.00 |   0.00 |    5.46 |    383.88 | 72,839,589 | 48,947 |     743 | necessarily | cause          |\n",
    "| **necessarily~representative** |   492 |     12.33 |      479.67 |    0.03 |   0.03 |    0.01 |   0.01 |    4.97 |  2,685.16 | 72,839,589 | 48,947 |  18,355 | necessarily | representative |\n",
    "| **necessarily~true**           | 3,460 |    155.66 |    3,304.34 |    0.01 |   0.01 |    0.07 |   0.07 |    4.35 | 15,129.32 | 72,839,589 | 48,947 | 231,639 | necessarily | true           |\n",
    "| **necessarily~synonymous**     |   167 |      5.54 |      161.46 |    0.02 |   0.02 |    0.00 |   0.00 |    4.25 |    818.37 | 72,839,589 | 48,947 |   8,245 | necessarily | synonymous     |\n",
    "| **necessarily~incompatible**   |   112 |      3.58 |      108.42 |    0.02 |   0.02 |    0.00 |   0.00 |    4.14 |    556.70 | 72,839,589 | 48,947 |   5,332 | necessarily | incompatible   |\n",
    "| **necessarily~reflective**     |   183 |      7.55 |      175.45 |    0.02 |   0.02 |    0.00 |   0.00 |    3.97 |    819.22 | 72,839,589 | 48,947 |  11,237 | necessarily | reflective     |\n",
    "| **necessarily~predictive**     |    58 |      1.63 |       56.37 |    0.02 |   0.02 |    0.00 |   0.00 |    3.95 |    303.20 | 72,839,589 | 48,947 |   2,421 | necessarily | predictive     |\n",
    "| **necessarily~incomplete**     |   124 |      5.13 |      118.87 |    0.02 |   0.02 |    0.00 |   0.00 |    3.81 |    554.34 | 72,839,589 | 48,947 |   7,634 | necessarily | incomplete     |\n",
    "| **necessarily~evil**           |   224 |     15.26 |      208.74 |    0.01 |   0.01 |    0.00 |   0.00 |    3.30 |    788.90 | 72,839,589 | 48,947 |  22,706 | necessarily | evil           |\n",
    "\n",
    "\n",
    "1. necessarily_indicative\n",
    "   >  While these lows do n't occur like clockwork every four years , and past performance is not necessarily indicative of future results , the historical record is rather impressive .\n",
    "\n",
    "2. necessarily_cause\n",
    "   >  This is not necessarily cause for alarm .\n",
    "\n",
    "3. necessarily_representative\n",
    "   >  Let 's just be clear : what some \" Mormon Apologists \" write is not necessarily representative of what most \" Mormons \" think or believe .\n",
    "\n",
    "4. necessarily_true\n",
    "   >  \" I must finally conclude that this proposition , I am , I exist , is necessarily true whenever it is put forward by me or conceived in my mind . \" -\n",
    "\n",
    "5. necessarily_synonymous\n",
    "   >  And this discussion always seems to come back to what we find attractive and appealing , instead of what we know to be healthy ( which research shows is not necessarily synonymous with thin ) .\n",
    "\n",
    "6. necessarily_incompatible\n",
    "   >  The philosophies of habits and intensity are n't necessarily incompatible .\n",
    "\n",
    "7. necessarily_reflective\n",
    "   >  After reading a novel by Adichie , I feel more educated and more aware of the fact that my way of viewing the world is incomplete and not necessarily reflective of everyone 's experiences .\n",
    "\n",
    "8. necessarily_predictive\n",
    "   >  There are varying degrees of offensive prowess sprinkled throughout the list of the hardest-hitters in baseball , so it is n't necessarily predictive of anything other than solid-average offense - but it 's a good sign nevertheless .\n",
    "\n",
    "9. necessarily_incomplete\n",
    "   >  The TPC 's analysis is necessarily incomplete ; it considers only the rate reductions Romney has made public and not the tax preferences that he has said he will eliminate but has n't yet identified .\n",
    "\n",
    "10. necessarily_evil\n",
    "   >  Vampires are n't necessarily evil .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_indicative_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_cause_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_representative_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_true_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_synonymous_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_incompatible_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_reflective_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_predictive_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_incomplete_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/necessarily/necessarily_evil_50ex.csv\n",
    "\n",
    "## *that*\n",
    "\n",
    "\n",
    "|                     |    `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |    `f1` |    `f2` | `l1`   | `l2`       |\n",
    "|:--------------------|-------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|--------:|--------:|:-------|:-----------|\n",
    "| **that~purported**  |     73 |      0.27 |       72.73 |    0.78 |   0.78 |    0.00 |   0.00 |    8.42 |    758.47 | 72,839,589 | 208,262 |      93 | that   | purported  |\n",
    "| **that~uncommon**   |    804 |     32.34 |      771.66 |    0.07 |   0.07 |    0.00 |   0.00 |    4.43 |  3,680.42 | 72,839,589 | 208,262 |  11,312 | that   | uncommon   |\n",
    "| **that~dissimilar** |    307 |     13.17 |      293.83 |    0.06 |   0.07 |    0.00 |   0.00 |    4.13 |  1,365.56 | 72,839,589 | 208,262 |   4,605 | that   | dissimilar |\n",
    "| **that~bad**        | 19,708 |  1,228.13 |   18,479.87 |    0.04 |   0.05 |    0.09 |   0.09 |    4.01 | 74,955.43 | 72,839,589 | 208,262 | 429,537 | that   | bad        |\n",
    "| **that~farfetched** |     93 |      3.79 |       89.21 |    0.07 |   0.07 |    0.00 |   0.00 |    3.75 |    423.24 | 72,839,589 | 208,262 |   1,324 | that   | farfetched |\n",
    "| **that~great**      | 11,781 |    884.23 |   10,896.77 |    0.04 |   0.04 |    0.05 |   0.06 |    3.71 | 40,195.15 | 72,839,589 | 208,262 | 309,258 | that   | great      |\n",
    "| **that~hard**       | 10,380 |    996.32 |    9,383.68 |    0.03 |   0.03 |    0.05 |   0.05 |    3.34 | 30,573.43 | 72,839,589 | 208,262 | 348,463 | that   | hard       |\n",
    "| **that~stupid**     |  1,437 |    145.02 |    1,291.98 |    0.03 |   0.03 |    0.01 |   0.01 |    3.12 |  4,048.67 | 72,839,589 | 208,262 |  50,722 | that   | stupid     |\n",
    "| **that~big**        |  7,073 |    865.10 |    6,207.90 |    0.02 |   0.02 |    0.03 |   0.03 |    2.96 | 17,624.62 | 72,839,589 | 208,262 | 302,567 | that   | big        |\n",
    "| **that~easy**       | 12,825 |  1,657.83 |   11,167.17 |    0.02 |   0.02 |    0.05 |   0.06 |    2.91 | 30,976.21 | 72,839,589 | 208,262 | 579,827 | that   | easy       |\n",
    "\n",
    "\n",
    "1. that_purported\n",
    "   >  Benham noted that in Davis ' most recent appeals , his lawyers put into evidence affidavits that purported to show that executions by electrocution have been plagued by `` shocking and grotesque errors '' and that there is a substantial risk they result in `` unnecessary infliction of pain and disfigurement . ''\n",
    "\n",
    "2. that_uncommon\n",
    "   >  Perhaps such a practice was not that uncommon in Japan , especially during the Middle Ages .\n",
    "\n",
    "3. that_dissimilar\n",
    "   >  Many of the prisoners she met were \" not that dissimilar from entrepreneurs , \" she tells Chu .\n",
    "\n",
    "4. that_bad\n",
    "   >  The preview I watched was n't that bad although I was left a bit confused by the end with the development of the characters .\n",
    "\n",
    "5. that_farfetched\n",
    "   >  However , it makes me wonder how much of this ' farfetched ' film is really that farfetched .\n",
    "\n",
    "6. that_great\n",
    "   >  Is hard to believe that the support was not that great , but we did not give up , and we raised enough money to BUY and ultrasound .\n",
    "\n",
    "7. that_hard\n",
    "   >  I mean really , it 's not that hard to soak a pound of dry beans , and I love beans .\n",
    "\n",
    "8. that_stupid\n",
    "   >  I 'm not that stupid , seeing as obviously he has a thing for you ... \" she said , smirking as a slight flush went over Kasumi 's cheeks .\n",
    "\n",
    "9. that_big\n",
    "   >  Kemp's Ridley turtles get up to 100 feet and 2 feet long which is not that big for a turtle .\n",
    "\n",
    "10. that_easy\n",
    "   >  However , it is not that easy to find the suitable terms to characterize a wine .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_purported_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_uncommon_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_dissimilar_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_bad_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_farfetched_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_great_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_hard_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_stupid_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_big_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/that/that_easy_50ex.csv\n",
    "\n",
    "## *exactly*\n",
    "\n",
    "\n",
    "|                           |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |   `f1` |    `f2` | `l1`    | `l2`          |\n",
    "|:--------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|-------:|--------:|:--------|:--------------|\n",
    "| **exactly~alike**         | 2,768 |      9.16 |    2,758.84 |    0.24 |   0.24 |    0.05 |   0.05 |    8.46 | 26,963.38 | 72,839,589 | 58,643 |  11,375 | exactly | alike         |\n",
    "| **exactly~opposite**      |   464 |      6.84 |      457.16 |    0.05 |   0.05 |    0.01 |   0.01 |    5.76 |  3,028.33 | 72,839,589 | 58,643 |   8,491 | exactly | opposite      |\n",
    "| **exactly~right**         | 6,323 |    115.21 |    6,207.79 |    0.04 |   0.04 |    0.11 |   0.11 |    5.74 | 39,191.65 | 72,839,589 | 58,643 | 143,095 | exactly | right         |\n",
    "| **exactly~sure**          | 9,157 |    211.60 |    8,945.40 |    0.03 |   0.03 |    0.15 |   0.16 |    5.40 | 52,863.19 | 72,839,589 | 58,643 | 262,825 | exactly | sure          |\n",
    "| **exactly~zero**          |   305 |      7.65 |      297.35 |    0.03 |   0.03 |    0.01 |   0.01 |    4.86 |  1,664.57 | 72,839,589 | 58,643 |   9,500 | exactly | zero          |\n",
    "| **exactly~parallel**      |   205 |      5.22 |      199.78 |    0.03 |   0.03 |    0.00 |   0.00 |    4.72 |  1,111.99 | 72,839,589 | 58,643 |   6,488 | exactly | parallel      |\n",
    "| **exactly~stellar**       |   172 |      4.33 |      167.67 |    0.03 |   0.03 |    0.00 |   0.00 |    4.68 |    936.96 | 72,839,589 | 58,643 |   5,379 | exactly | stellar       |\n",
    "| **exactly~analogous**     |   103 |      2.47 |      100.53 |    0.03 |   0.03 |    0.00 |   0.00 |    4.53 |    571.33 | 72,839,589 | 58,643 |   3,062 | exactly | analogous     |\n",
    "| **exactly~perpendicular** |    50 |      1.00 |       49.00 |    0.04 |   0.04 |    0.00 |   0.00 |    4.34 |    295.37 | 72,839,589 | 58,643 |   1,240 | exactly | perpendicular |\n",
    "| **exactly~conducive**     |   208 |      7.33 |      200.67 |    0.02 |   0.02 |    0.00 |   0.00 |    4.25 |    995.32 | 72,839,589 | 58,643 |   9,110 | exactly | conducive     |\n",
    "\n",
    "\n",
    "1. exactly_alike\n",
    "   >  No two of your characters will be exactly alike .\n",
    "\n",
    "2. exactly_opposite\n",
    "   >  Similar laws are in place for the quark and antiquark that make up a meson : their respective colors must be exactly opposite .\n",
    "\n",
    "3. exactly_right\n",
    "   >  All that is exactly right , because deciding on a university is often the first really major decision of a young adult 's life .\n",
    "\n",
    "4. exactly_sure\n",
    "   >  Not exactly sure what a Prof is ?\n",
    "\n",
    "5. exactly_zero\n",
    "   >  I did sign up for a trip to Pai Canyon to watch the sunset , and was able to chat with a group of girls from the UK on the way up and the way back , so my social interaction was n't exactly zero for Day 1 solo on the roads .\n",
    "\n",
    "6. exactly_parallel\n",
    "   >  The most common obstacle is binding which can result when mounting surfaces are n't exactly parallel .\n",
    "\n",
    "7. exactly_stellar\n",
    "   >  It was a long day for them and the Browns are n't exactly stellar in the passing game .\n",
    "\n",
    "8. exactly_analogous\n",
    "   >  In short , the cases are almost exactly analogous .\n",
    "\n",
    "9. exactly_perpendicular\n",
    "   >  In this context , the rotation of the ground floor white volume is included in order to be located exactly perpendicular to the south .\n",
    "\n",
    "10. exactly_conducive\n",
    "   >  Not exactly conducive to being a rock star .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_alike_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_opposite_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_right_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_sure_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_zero_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_parallel_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_stellar_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_analogous_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_perpendicular_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/exactly/exactly_conducive_50ex.csv\n",
    "\n",
    "## *any*\n",
    "\n",
    "\n",
    "|                 |    `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |   `f1` |    `f2` | `l1`   | `l2`    |\n",
    "|:----------------|-------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|-------:|--------:|:-------|:--------|\n",
    "| **any~happier** |    963 |      7.84 |      955.16 |    0.06 |   0.06 |    0.03 |   0.03 |    6.75 |  7,438.55 | 72,839,589 | 34,382 |  16,606 | any    | happier |\n",
    "| **any~clearer** |    608 |      5.51 |      602.49 |    0.05 |   0.05 |    0.02 |   0.02 |    6.51 |  4,556.17 | 72,839,589 | 34,382 |  11,680 | any    | clearer |\n",
    "| **any~closer**  |  1,738 |     29.02 |    1,708.98 |    0.03 |   0.03 |    0.05 |   0.05 |    5.74 | 10,942.35 | 72,839,589 | 34,382 |  61,475 | any    | closer  |\n",
    "| **any~wiser**   |    141 |      1.71 |      139.29 |    0.04 |   0.04 |    0.00 |   0.00 |    5.66 |    971.10 | 72,839,589 | 34,382 |   3,630 | any    | wiser   |\n",
    "| **any~cuter**   |     80 |      0.86 |       79.14 |    0.04 |   0.04 |    0.00 |   0.00 |    5.56 |    570.11 | 72,839,589 | 34,382 |   1,828 | any    | cuter   |\n",
    "| **any~safer**   |    626 |     10.77 |      615.23 |    0.03 |   0.03 |    0.02 |   0.02 |    5.56 |  3,883.22 | 72,839,589 | 34,382 |  22,826 | any    | safer   |\n",
    "| **any~worse**   |  3,673 |     84.50 |    3,588.50 |    0.02 |   0.02 |    0.10 |   0.11 |    5.33 | 20,994.30 | 72,839,589 | 34,382 | 179,012 | any    | worse   |\n",
    "| **any~better**  | 11,460 |    295.36 |   11,164.64 |    0.02 |   0.02 |    0.32 |   0.33 |    5.23 | 65,861.94 | 72,839,589 | 34,382 | 625,733 | any    | better  |\n",
    "| **any~truer**   |     56 |      0.67 |       55.33 |    0.04 |   0.04 |    0.00 |   0.00 |    5.16 |    386.71 | 72,839,589 | 34,382 |   1,427 | any    | truer   |\n",
    "| **any~nearer**  |     66 |      0.87 |       65.13 |    0.04 |   0.04 |    0.00 |   0.00 |    5.15 |    444.12 | 72,839,589 | 34,382 |   1,836 | any    | nearer  |\n",
    "\n",
    "\n",
    "1. any_happier\n",
    "   >  Even though he arrived a week before my due date Frankie weighed 7lb 8oz , was in perfect health and we could n't have been any happier !\n",
    "\n",
    "2. any_clearer\n",
    "   >  The dramaturgy cannot be any clearer in trying to highlight where our focus should be .\n",
    "\n",
    "3. any_closer\n",
    "   >  Asked if she was any closer to giving an endorsement , Palin told the Fox News host that she could only tell him what she 'd do if she were a South Carolinian .\n",
    "\n",
    "4. any_wiser\n",
    "   >  Is burning binary bits of information any wiser ?\n",
    "\n",
    "5. any_cuter\n",
    "   >  It could n't be any cuter .\n",
    "\n",
    "6. any_safer\n",
    "   >  In Kate Thompson 's Creature of the Night Bobby 's mum is running away , taking her family with her , but is the country any safer than the city ? *\n",
    "\n",
    "7. any_worse\n",
    "   >  \" Can this day get any worse ? \"\n",
    "\n",
    "8. any_better\n",
    "   >  Do you think his odds would have been any better if he 'd been publishing his work today ?\n",
    "\n",
    "9. any_truer\n",
    "   >  In the case of Alan , this saying could not have been any truer .\n",
    "\n",
    "10. any_nearer\n",
    "   >  No one lives any nearer than town .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_happier_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_clearer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_closer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_wiser_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_cuter_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_safer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_worse_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_better_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_truer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/any/any_nearer_50ex.csv\n",
    "\n",
    "## *remotely*\n",
    "\n",
    "\n",
    "|                          |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |     `G2` |        `N` |   `f1` |    `f2` | `l1`     | `l2`        |\n",
    "|:-------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|---------:|-----------:|-------:|--------:|:---------|:------------|\n",
    "| **remotely~detonated**   |    78 |      0.02 |       77.98 |    0.88 |   0.88 |    0.00 |   0.00 |   12.69 | 1,243.75 | 72,839,589 | 16,426 |      89 | remotely | detonated   |\n",
    "| **remotely~exploitable** |   145 |      0.22 |      144.78 |    0.15 |   0.15 |    0.01 |   0.01 |    8.79 | 1,613.38 | 72,839,589 | 16,426 |     986 | remotely | exploitable |\n",
    "| **remotely~comparable**  |   277 |      2.76 |      274.24 |    0.02 |   0.02 |    0.02 |   0.02 |    6.15 | 2,015.00 | 72,839,589 | 16,426 |  12,252 | remotely | comparable  |\n",
    "| **remotely~plausible**   |   183 |      3.96 |      179.04 |    0.01 |   0.01 |    0.01 |   0.01 |    4.89 | 1,048.46 | 72,839,589 | 16,426 |  17,571 | remotely | plausible   |\n",
    "| **remotely~related**     |   617 |     23.76 |      593.24 |    0.01 |   0.01 |    0.04 |   0.04 |    4.36 | 2,857.41 | 72,839,589 | 16,426 | 105,375 | remotely | related     |\n",
    "| **remotely~close**       | 1,597 |     92.76 |    1,504.24 |    0.00 |   0.00 |    0.09 |   0.10 |    3.90 | 6,229.80 | 72,839,589 | 16,426 | 411,329 | remotely | close       |\n",
    "| **remotely~interested**  | 1,062 |     59.65 |    1,002.35 |    0.00 |   0.00 |    0.06 |   0.06 |    3.90 | 4,177.56 | 72,839,589 | 16,426 | 264,528 | remotely | interested  |\n",
    "| **remotely~credible**    |   100 |      3.68 |       96.32 |    0.01 |   0.01 |    0.01 |   0.01 |    3.87 |   468.95 | 72,839,589 | 16,426 |  16,318 | remotely | credible    |\n",
    "| **remotely~possible**    |   727 |     55.31 |      671.69 |    0.00 |   0.00 |    0.04 |   0.04 |    3.41 | 2,431.85 | 72,839,589 | 16,426 | 245,272 | remotely | possible    |\n",
    "| **remotely~believable**  |    68 |      3.12 |       64.88 |    0.00 |   0.00 |    0.00 |   0.00 |    3.33 |   290.03 | 72,839,589 | 16,426 |  13,823 | remotely | believable  |\n",
    "\n",
    "\n",
    "1. remotely_detonated\n",
    "   >  the bomb hit the car only a few miles from its destination , officials said , and may have been remotely detonated .\n",
    "\n",
    "2. remotely_exploitable\n",
    "   >  Sun Systems products get 23 fixes , seven of which are remotely exploitable .\n",
    "\n",
    "3. remotely_comparable\n",
    "   >  Yes , yes , I know , the cases are not remotely comparable .\n",
    "\n",
    "4. remotely_plausible\n",
    "   >  Which reminds me , how was it even remotely plausible that Sally Yates was convinced that Mike Flynn was compromised because he had n't been completely forthright in disclosing his conversation with the Russian ambassador ?\n",
    "\n",
    "5. remotely_related\n",
    "   >  If you like Treme and restaurants , or restaurants only , or Bourdain or , well , any other things even remotely related to the whole thing , read it .\n",
    "\n",
    "6. remotely_close\n",
    "   >  \" In only one case did I find anything even remotely close where an officer wrote that the victim did not resist to the best of her ability . \"\n",
    "\n",
    "7. remotely_interested\n",
    "   >  Since the game is free-to - play , anybody who 's even remotely interested in tank warfare or shooting down birds out of the sky , would be missing out should they choose not give it a go .\n",
    "\n",
    "8. remotely_credible\n",
    "   >  Michiko Kakutani , writing in The New York Times , bridled at the book 's `` grotesque , voyeuristic scenes '' and found the female characters not `` remotely credible . ''\n",
    "\n",
    "9. remotely_possible\n",
    "   >  The guy getting credit for revealing the fraud actually did a lot of work that is not remotely possible for the average retail investor , or even the extraordinary retail investor , including going to China to meet the company .\n",
    "\n",
    "10. remotely_believable\n",
    "   >  although there is nothing remotely believable about this drawn-out cat-and-mouse game of a movie crossed with a whodunit , that 's almost the point .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_detonated_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_exploitable_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_comparable_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_plausible_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_related_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_close_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_interested_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_credible_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_possible_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/remotely/remotely_believable_50ex.csv\n",
    "\n",
    "## *ever*\n",
    "\n",
    "\n",
    "|                      |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |    `f1` |   `f2` | `l1`   | `l2`        |\n",
    "|:---------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|--------:|-------:|:-------|:------------|\n",
    "| **ever~olympic**     |   218 |      0.77 |      217.23 |    0.44 |   0.44 |    0.00 |   0.00 |    8.23 |  2,141.80 | 72,839,589 | 114,075 |    492 | ever   | olympic     |\n",
    "| **ever~quarterly**   |   137 |      0.48 |      136.52 |    0.45 |   0.45 |    0.00 |   0.00 |    8.05 |  1,349.65 | 72,839,589 | 114,075 |    306 | ever   | quarterly   |\n",
    "| **ever~watchful**    |   416 |      2.92 |      413.08 |    0.22 |   0.22 |    0.00 |   0.00 |    7.05 |  3,399.87 | 72,839,589 | 114,075 |  1,866 | ever   | watchful    |\n",
    "| **ever~closer**      | 6,307 |     96.28 |    6,210.72 |    0.10 |   0.10 |    0.05 |   0.06 |    6.08 | 41,328.73 | 72,839,589 | 114,075 | 61,475 | ever   | closer      |\n",
    "| **ever~joint**       |   195 |      2.29 |      192.71 |    0.13 |   0.13 |    0.00 |   0.00 |    5.95 |  1,375.51 | 72,839,589 | 114,075 |  1,460 | ever   | joint       |\n",
    "| **ever~nearer**      |   223 |      2.88 |      220.12 |    0.12 |   0.12 |    0.00 |   0.00 |    5.84 |  1,528.28 | 72,839,589 | 114,075 |  1,836 | ever   | nearer      |\n",
    "| **ever~vigilant**    |   923 |     14.94 |      908.06 |    0.10 |   0.10 |    0.01 |   0.01 |    5.80 |  5,892.45 | 72,839,589 | 114,075 |  9,541 | ever   | vigilant    |\n",
    "| **ever~diminishing** |    71 |      0.70 |       70.30 |    0.16 |   0.16 |    0.00 |   0.00 |    5.75 |    527.77 | 72,839,589 | 114,075 |    445 | ever   | diminishing |\n",
    "| **ever~scarcer**     |    82 |      0.85 |       81.15 |    0.15 |   0.15 |    0.00 |   0.00 |    5.75 |    599.19 | 72,839,589 | 114,075 |    545 | ever   | scarcer     |\n",
    "| **ever~shrinking**   |   120 |      1.55 |      118.45 |    0.12 |   0.12 |    0.00 |   0.00 |    5.60 |    822.02 | 72,839,589 | 114,075 |    989 | ever   | shrinking   |\n",
    "\n",
    "\n",
    "1. ever_olympic\n",
    "   >  Naidan Tuvshinbayar won Mongolia 's first ever Olympic gold medal when claiming the men's 100 kg title .\n",
    "\n",
    "2. ever_quarterly\n",
    "   >  Losses of Tw $ 9.8 billion in the last three months of 2017 represented its worst ever quarterly results .\n",
    "\n",
    "3. ever_watchful\n",
    "   >  Owls are a rowdy bunch with fantasy colored feathers in bright oranges , greens , reds , yellows , blues and purples , with feathers that stick out in all directions and their signature yellow eyes ever watchful for an opportunity to clown around .\n",
    "\n",
    "4. ever_closer\n",
    "   >  CW : Since the US / EU stance on the two above-territories since the collapse of the USSR has effectively achieved the precise opposite of what that stance was designed to achieve ( by pushing the regions into ever closer ties with Russia ) , how can there be any logical defence of continuing that policy ?\n",
    "\n",
    "5. ever_joint\n",
    "   >  The Nepal Army has said that Kathmandu and Beijing will conduct their first ever joint military exercise from April 16 .\n",
    "\n",
    "6. ever_nearer\n",
    "   >  At Wednesday 's ceremony , the 2012 medals will be shown to the world for the first time , yet another reminder that the Games are drawing ever nearer .\n",
    "\n",
    "7. ever_vigilant\n",
    "   >  \" One after the other , each of us must be ever vigilant , to relentlessly overcome each challenge , to reach higher , set more innovative goals , and bring joy to our customers .\n",
    "\n",
    "8. ever_diminishing\n",
    "   >  Miners create a block after a time frame which is worth an ever diminishing amount of currency or some kind of reward in order to ensure the shortfall .\n",
    "\n",
    "9. ever_scarcer\n",
    "   >  As The City 's rents rise even higher and real estate becomes ever scarcer with more people flocking here to take advantage of the tech -fueled economic boom , a new kind of street person is emerging : older , gay , and living with HIV or AIDS .\n",
    "\n",
    "10. ever_shrinking\n",
    "   >  In our complicated and ever shrinking world , the power of different bodies to make or administer law is often unclear .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_olympic_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_quarterly_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_watchful_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_closer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_joint_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_nearer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_vigilant_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_diminishing_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_scarcer_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/ever/ever_shrinking_50ex.csv\n",
    "\n",
    "## *yet*\n",
    "\n",
    "\n",
    "|                      |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |     `G2` |        `N` |   `f1` |   `f2` | `l1`   | `l2`         |\n",
    "|:---------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|---------:|-----------:|-------:|-------:|:-------|:-------------|\n",
    "| **yet~unborn**       |   372 |      0.68 |      371.32 |    0.72 |   0.72 |    0.00 |   0.00 |   10.14 | 4,319.00 | 72,839,589 | 95,763 |    519 | yet    | unborn       |\n",
    "| **yet~unnamed**      |   736 |      2.77 |      733.23 |    0.35 |   0.35 |    0.01 |   0.01 |    8.29 | 7,048.15 | 72,839,589 | 95,763 |  2,107 | yet    | unnamed      |\n",
    "| **yet~unspecified**  |   204 |      0.80 |      203.20 |    0.33 |   0.34 |    0.00 |   0.00 |    7.86 | 1,933.20 | 72,839,589 | 95,763 |    607 | yet    | unspecified  |\n",
    "| **yet~undetermined** |   297 |      1.45 |      295.55 |    0.27 |   0.27 |    0.00 |   0.00 |    7.55 | 2,658.03 | 72,839,589 | 95,763 |  1,104 | yet    | undetermined |\n",
    "| **yet~untitled**     |   121 |      0.54 |      120.46 |    0.29 |   0.29 |    0.00 |   0.00 |    7.37 | 1,107.51 | 72,839,589 | 95,763 |    412 | yet    | untitled     |\n",
    "| **yet~unidentified** |   336 |      2.48 |      333.52 |    0.18 |   0.18 |    0.00 |   0.00 |    6.85 | 2,694.33 | 72,839,589 | 95,763 |  1,890 | yet    | unidentified |\n",
    "| **yet~unformed**     |    53 |      0.33 |       52.67 |    0.21 |   0.21 |    0.00 |   0.00 |    6.28 |   445.94 | 72,839,589 | 95,763 |    249 | yet    | unformed     |\n",
    "| **yet~final**        |   643 |      7.70 |      635.30 |    0.11 |   0.11 |    0.01 |   0.01 |    6.20 | 4,494.99 | 72,839,589 | 95,763 |  5,860 | yet    | final        |\n",
    "| **yet~unannounced**  |   129 |      1.16 |      127.84 |    0.14 |   0.15 |    0.00 |   0.00 |    6.19 |   979.33 | 72,839,589 | 95,763 |    883 | yet    | unannounced  |\n",
    "| **yet~undiscovered** |   278 |      2.99 |      275.01 |    0.12 |   0.12 |    0.00 |   0.00 |    6.18 | 2,006.07 | 72,839,589 | 95,763 |  2,272 | yet    | undiscovered |\n",
    "\n",
    "\n",
    "1. yet_unborn\n",
    "   >  You may spill your blood as a purely symbolic gesture , in service to those humans yet unborn .\n",
    "\n",
    "2. yet_unnamed\n",
    "   >  That will be powered by a yet unnamed 1.6 Ghz octa-core processor , supported by 2GB RAM .\n",
    "\n",
    "3. yet_unspecified\n",
    "   >  most of the justices are troubled by outsized verdicts , and the court has ruled that the constitutional guarantee of due process of law places some , as yet unspecified , limits on punitive damages .\n",
    "\n",
    "4. yet_undetermined\n",
    "   >  Clashes had five wounded Saturday night in circumstances yet undetermined .\n",
    "\n",
    "5. yet_untitled\n",
    "   >  Actor-filmmaker Kamal Haasan will reportedly join hands with his erstwhile assistant Rajesh M Selva , director of his last Tamil outing \" Thoongaavanam \" , for a yet untitled Tamil actioner .\n",
    "\n",
    "6. yet_unidentified\n",
    "   >  They are focusing genetic testing on families , like Crissy 's , where breast cancer is hereditary and develops from a yet unidentified gene mutation .\n",
    "\n",
    "7. yet_unformed\n",
    "   >  Whilst earth , as yet unformed and void ,\n",
    "\n",
    "8. yet_final\n",
    "   >  The duties are not yet final , so we will continue to use all the means available to us to ensure a fair outcome to these investigations , \" the sources explained .\n",
    "\n",
    "9. yet_unannounced\n",
    "   >  3 . A yet unannounced candidate takes the White House\n",
    "\n",
    "10. yet_undiscovered\n",
    "   >  `` Countless seabirds , dolphins , fishes , corals and tiny things as yet undiscovered could survive as a result , free of the threats that are eliminating them elsewhere . ''\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_unborn_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_unnamed_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_unspecified_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_undetermined_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_untitled_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_unidentified_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_unformed_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_final_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_unannounced_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/yet/yet_undiscovered_50ex.csv\n",
    "\n",
    "## *immediately*\n",
    "\n",
    "\n",
    "|                              |    `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |       `G2` |        `N` |   `f1` |    `f2` | `l1`        | `l2`         |\n",
    "|:-----------------------------|-------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|-----------:|-----------:|-------:|--------:|:------------|:-------------|\n",
    "| **immediately~accretive**    |    236 |      0.70 |      235.30 |    0.45 |   0.45 |    0.00 |   0.00 |    8.54 |   2,406.67 | 72,839,589 | 96,973 |     523 | immediately | accretive    |\n",
    "| **immediately~appealable**   |     76 |      0.19 |       75.81 |    0.55 |   0.55 |    0.00 |   0.00 |    8.41 |     815.23 | 72,839,589 | 96,973 |     139 | immediately | appealable   |\n",
    "| **immediately~adjacent**     |  1,595 |      6.33 |    1,588.67 |    0.33 |   0.34 |    0.02 |   0.02 |    8.31 |  15,089.64 | 72,839,589 | 96,973 |   4,756 | immediately | adjacent     |\n",
    "| **immediately~apparent**     |  4,971 |     72.22 |    4,898.78 |    0.09 |   0.09 |    0.05 |   0.05 |    6.12 |  32,982.98 | 72,839,589 | 96,973 |  54,246 | immediately | apparent     |\n",
    "| **immediately~clear**        | 26,038 |    464.92 |   25,573.08 |    0.07 |   0.07 |    0.26 |   0.27 |    5.86 | 167,884.92 | 72,839,589 | 96,973 | 349,214 | immediately | clear        |\n",
    "| **immediately~actionable**   |    173 |      2.64 |      170.36 |    0.09 |   0.09 |    0.00 |   0.00 |    5.47 |   1,121.82 | 72,839,589 | 96,973 |   1,983 | immediately | actionable   |\n",
    "| **immediately~recognizable** |  1,736 |     44.60 |    1,691.40 |    0.05 |   0.05 |    0.02 |   0.02 |    5.15 |   9,447.17 | 72,839,589 | 96,973 |  33,499 | immediately | recognizable |\n",
    "| **immediately~available**    | 29,351 |    887.87 |   28,463.13 |    0.04 |   0.04 |    0.29 |   0.30 |    5.06 | 159,088.56 | 72,839,589 | 96,973 | 666,909 | immediately | available    |\n",
    "| **immediately~subsequent**   |     59 |      0.96 |       58.04 |    0.08 |   0.08 |    0.00 |   0.00 |    4.81 |     374.58 | 72,839,589 | 96,973 |     722 | immediately | subsequent   |\n",
    "| **immediately~recognisable** |    349 |     10.92 |      338.08 |    0.04 |   0.04 |    0.00 |   0.00 |    4.59 |   1,757.24 | 72,839,589 | 96,973 |   8,204 | immediately | recognisable |\n",
    "\n",
    "\n",
    "1. immediately_accretive\n",
    "   >  \" Further , we expect to leverage our purchasing , sales , distribution and administrative capabilities to improve the profitability of this business , and we expect this acquisition to be immediately accretive to Drew 's earnings . \"\n",
    "\n",
    "2. immediately_appealable\n",
    "   >  Generally , the denial of a motion to dismiss under Rule 12 ( b ) ( 6 ) , SCRCP , is not immediately appealable .\n",
    "\n",
    "3. immediately_adjacent\n",
    "   >  Sometimes the parking is in its own big ugly building , and sometimes it 's designed so that it 's immediately adjacent to the apartment or townhouse of its occupant .\n",
    "\n",
    "4. immediately_apparent\n",
    "   >  \" Research can be hard , and its benefits are n't always immediately apparent , \" she shares .\n",
    "\n",
    "5. immediately_clear\n",
    "   >  the cause of the fire was not immediately clear but investigators say they believe it was an accident .\n",
    "\n",
    "6. immediately_actionable\n",
    "   >  I facilitate teacher workshops and online courses that deliver practical and immediately actionable advice on teachers from early childhood to Primary can develop student ICT capability in their lessons .\n",
    "\n",
    "7. immediately_recognizable\n",
    "   >  It 's an appropriate motto for the hitmaker savant , who has managed to fit a number of wildly disparate elements under that pop umbrella , crafting an immediately recognizable sound out of a multitude of genres .\n",
    "\n",
    "8. immediately_available\n",
    "   >  Financial details of the agreement were not immediately available , but Simon was more than satisfied .\n",
    "\n",
    "9. immediately_subsequent\n",
    "   >  At issue , as FHQ has discussed , is the discrepancy between the longstanding New Hampshire election law that requires seven days between the primary in the Granite state and the immediately subsequent primary or caucus and a newly -enacted Nevada Republican Party resolution tethering the party 's caucuses to the Saturday following the New Hampshire primary .\n",
    "\n",
    "10. immediately_recognisable\n",
    "   >  Its vastness and power have supplied immediately recognisable symbols for moral and religious works of art .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_accretive_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_appealable_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_adjacent_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_apparent_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_clear_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_actionable_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_recognizable_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_available_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_subsequent_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/immediately/immediately_recognisable_50ex.csv\n",
    "\n",
    "## *particularly*\n",
    "\n",
    "\n",
    "|                              |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |    `f1` |   `f2` | `l1`         | `l2`        |\n",
    "|:-----------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|--------:|-------:|:-------------|:------------|\n",
    "| **particularly~hard-hit**    |   154 |      2.76 |      151.24 |    0.39 |   0.39 |    0.00 |   0.00 |    5.64 |  1,005.08 | 72,839,589 | 513,668 |    391 | particularly | hard-hit    |\n",
    "| **particularly~galling**     |   540 |     15.33 |      524.67 |    0.24 |   0.25 |    0.00 |   0.00 |    5.12 |  2,937.15 | 72,839,589 | 513,668 |  2,174 | particularly | galling     |\n",
    "| **particularly~well-suited** |    62 |      1.22 |       60.78 |    0.35 |   0.36 |    0.00 |   0.00 |    4.91 |    390.17 | 72,839,589 | 513,668 |    173 | particularly | well-suited |\n",
    "| **particularly~acute**       | 2,809 |    109.23 |    2,699.77 |    0.17 |   0.18 |    0.01 |   0.01 |    4.79 | 13,361.56 | 72,839,589 | 513,668 | 15,489 | particularly | acute       |\n",
    "| **particularly~noteworthy**  | 2,294 |    112.66 |    2,181.34 |    0.14 |   0.14 |    0.00 |   0.00 |    4.37 |  9,788.31 | 72,839,589 | 513,668 | 15,975 | particularly | noteworthy  |\n",
    "| **particularly~thorny**      |   308 |     12.43 |      295.57 |    0.17 |   0.17 |    0.00 |   0.00 |    4.36 |  1,439.60 | 72,839,589 | 513,668 |  1,762 | particularly | thorny      |\n",
    "| **particularly~fond**        | 4,179 |    229.28 |    3,949.72 |    0.12 |   0.13 |    0.01 |   0.01 |    4.24 | 16,897.66 | 72,839,589 | 513,668 | 32,513 | particularly | fond        |\n",
    "| **particularly~nettlesome**  |    63 |      2.03 |       60.97 |    0.21 |   0.22 |    0.00 |   0.00 |    4.02 |    324.87 | 72,839,589 | 513,668 |    288 | particularly | nettlesome  |\n",
    "| **particularly~nasty**       | 2,143 |    135.27 |    2,007.73 |    0.10 |   0.11 |    0.00 |   0.00 |    3.96 |  8,052.99 | 72,839,589 | 513,668 | 19,181 | particularly | nasty       |\n",
    "| **particularly~egregious**   | 1,705 |    107.12 |    1,597.88 |    0.11 |   0.11 |    0.00 |   0.00 |    3.94 |  6,421.57 | 72,839,589 | 513,668 | 15,190 | particularly | egregious   |\n",
    "\n",
    "\n",
    "1. particularly_hard-hit\n",
    "   >  shares of financial stocks were particularly hard-hit .\n",
    "\n",
    "2. particularly_galling\n",
    "   >  the brazen presence in Poti has been particularly galling for Georgia because it is hundreds of kilometers -LRB- miles -RRB- from South Ossetia , where the war broke out and where most of the fighting occurred .\n",
    "\n",
    "3. particularly_well-suited\n",
    "   >  but the plutonium produced by its two breeders , Monju and Joyo , is a kind that is particularly well-suited to bombs , Greenpeace said .\n",
    "\n",
    "4. particularly_acute\n",
    "   >  The risks were particularly acute for correspondents in the north and east and those covering political events .\n",
    "\n",
    "5. particularly_noteworthy\n",
    "   >  Bushey recalled how , shortly after learning of what he called `` a particularly noteworthy crime , '' he directed one of his subordinates to inform Simpson `` just as soon as was humanly possible . ''\n",
    "\n",
    "6. particularly_thorny\n",
    "   >  With $ 15,000 payouts for first place and particularly thorny fields , the Championships have traditionally been considered among pool 's domestic majors .\n",
    "\n",
    "7. particularly_fond\n",
    "   >  For example , I love mixing the herbal Honeybush Vanilla [ 19 ] with a black English Breakfast [ 20 ] tea. Adagio [ 21 ] allows users to upload their favourite blends so that other tea drinkers can purchase them ( I am particularly fond of the Captain America [ 22 ] and TARDIS [ 23 ] blends ) .\n",
    "\n",
    "8. particularly_nettlesome\n",
    "   >  The court in East Bridge Lofts described that issue as \" particularly nettlesome , \" but it also found that South Carolina ( whose law governed this issue ) applies what is known as the \" automatic waiver \" rule :\n",
    "\n",
    "9. particularly_nasty\n",
    "   >  Hunter , known for his in-your-face style , gave a particularly nasty time to Red Wings captain Steve Yzerman .\n",
    "\n",
    "10. particularly_egregious\n",
    "   >  Veldhuis ' case is particularly egregious , because he was fired while his classes were still in session .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_hard-hit_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_galling_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_well-suited_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_acute_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_noteworthy_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_thorny_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_fond_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_nettlesome_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_nasty_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/particularly/particularly_egregious_50ex.csv\n",
    "\n",
    "## *inherently*\n",
    "\n",
    "\n",
    "|                                  |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |     `G2` |        `N` |   `f1` |   `f2` | `l1`       | `l2`              |\n",
    "|:---------------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|---------:|-----------:|-------:|-------:|:-----------|:------------------|\n",
    "| **inherently~governmental**      |   253 |      0.50 |      252.50 |    0.33 |   0.33 |    0.01 |   0.01 |    8.92 | 2,742.59 | 72,839,589 | 47,803 |    761 | inherently | governmental      |\n",
    "| **inherently~unequal**           |   390 |      3.69 |      386.31 |    0.07 |   0.07 |    0.01 |   0.01 |    6.38 | 2,893.14 | 72,839,589 | 47,803 |  5,621 | inherently | unequal           |\n",
    "| **inherently~evil**              | 1,123 |     14.90 |    1,108.10 |    0.05 |   0.05 |    0.02 |   0.02 |    6.05 | 7,572.62 | 72,839,589 | 47,803 | 22,706 | inherently | evil              |\n",
    "| **inherently~coercive**          |    94 |      0.82 |       93.18 |    0.07 |   0.08 |    0.00 |   0.00 |    5.98 |   711.86 | 72,839,589 | 47,803 |  1,253 | inherently | coercive          |\n",
    "| **inherently~unstable**          |   806 |     14.80 |      791.20 |    0.04 |   0.04 |    0.02 |   0.02 |    5.52 | 4,903.18 | 72,839,589 | 47,803 | 22,546 | inherently | unstable          |\n",
    "| **inherently~sinful**            |   145 |      2.01 |      142.99 |    0.05 |   0.05 |    0.00 |   0.00 |    5.49 |   962.39 | 72,839,589 | 47,803 |  3,059 | inherently | sinful            |\n",
    "| **inherently~flawed**            |   899 |     18.13 |      880.87 |    0.03 |   0.03 |    0.02 |   0.02 |    5.39 | 5,301.74 | 72,839,589 | 47,803 | 27,628 | inherently | flawed            |\n",
    "| **inherently~untrustworthy**     |    69 |      0.79 |       68.21 |    0.06 |   0.06 |    0.00 |   0.00 |    5.39 |   483.61 | 72,839,589 | 47,803 |  1,211 | inherently | untrustworthy     |\n",
    "| **inherently~interdisciplinary** |    94 |      1.25 |       92.75 |    0.05 |   0.05 |    0.00 |   0.00 |    5.35 |   631.33 | 72,839,589 | 47,803 |  1,906 | inherently | interdisciplinary |\n",
    "| **inherently~discriminatory**    |   175 |      2.94 |      172.06 |    0.04 |   0.04 |    0.00 |   0.00 |    5.27 | 1,093.34 | 72,839,589 | 47,803 |  4,481 | inherently | discriminatory    |\n",
    "\n",
    "\n",
    "1. inherently_governmental\n",
    "   >  \" When it comes to security in war zones , what is considered ' inherently governmental ? '\n",
    "\n",
    "2. inherently_unequal\n",
    "   >  And this is when the Supreme Court of the U.S. ruled that the \" separate but equal \" policy was inherently unequal .\n",
    "\n",
    "3. inherently_evil\n",
    "   >  The layperson 's affirmation of this proposition as necessarily true is linked essentially to some pretty basic \" inside \" feelings that rape is inherently evil .\n",
    "\n",
    "4. inherently_coercive\n",
    "   >  While direct contact is somewhat more intrusive , it is not inherently coercive .\n",
    "\n",
    "5. inherently_unstable\n",
    "   >  \" Vitamin C is inherently unstable and easily oxidizes in air , which makes it ineffective .\n",
    "\n",
    "6. inherently_sinful\n",
    "   >  The position taken by most Anglican churches in Africa , which see homosexuality as inherently sinful ( at best ) is in the end irreconcilable with the liberal views which predominate in North America and increasingly ( though far from uniformly ) in the Church of England itself .\n",
    "\n",
    "7. inherently_flawed\n",
    "   >  A man's means may become his end as a result of conscious and reasoned choice , and there is nothing inherently flawed in this approach .\n",
    "\n",
    "8. inherently_untrustworthy\n",
    "   >  As such , UN agencies like the WTO , Human Rights Council , and World Bank are inevitably unethical and inherently untrustworthy .\n",
    "\n",
    "9. inherently_interdisciplinary\n",
    "   >  The problems are inherently interdisciplinary .\n",
    "\n",
    "10. inherently_discriminatory\n",
    "   >  some analysts said whites , often the chief architect of standardized tests and the products of better schools , tend to perform better than minorities _ making reliance on the test inherently discriminatory .\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_governmental_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_unequal_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_evil_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_coercive_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_unstable_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_sinful_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_flawed_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_untrustworthy_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_interdisciplinary_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/inherently/inherently_discriminatory_50ex.csv\n",
    "\n",
    "## *terribly*\n",
    "\n",
    "\n",
    "|                          |   `f` |   `exp_f` |   `unexp_f` |   `dP1` |   `P1` |   `dP2` |   `P2` |   `LRC` |      `G2` |        `N` |   `f1` |    `f2` | `l1`     | `l2`        |\n",
    "|:-------------------------|------:|----------:|------------:|--------:|-------:|--------:|-------:|--------:|----------:|-----------:|-------:|--------:|:---------|:------------|\n",
    "| **terribly~awry**        |   180 |      0.56 |      179.44 |    0.26 |   0.26 |    0.00 |   0.00 |    8.02 |  1,770.97 | 72,839,589 | 58,964 |     692 | terribly | awry        |\n",
    "| **terribly~wrong**       | 6,349 |    120.67 |    6,228.33 |    0.04 |   0.04 |    0.11 |   0.11 |    5.67 | 38,814.14 | 72,839,589 | 58,964 | 149,064 | terribly | wrong       |\n",
    "| **terribly~amiss**       |    64 |      0.73 |       63.27 |    0.07 |   0.07 |    0.00 |   0.00 |    5.37 |    451.25 | 72,839,589 | 58,964 |     898 | terribly | amiss       |\n",
    "| **terribly~sorry**       | 1,262 |     50.65 |    1,211.35 |    0.02 |   0.02 |    0.02 |   0.02 |    4.43 |  5,741.83 | 72,839,589 | 58,964 |  62,573 | terribly | sorry       |\n",
    "| **terribly~misguided**   |   101 |      3.24 |       97.76 |    0.02 |   0.03 |    0.00 |   0.00 |    4.09 |    501.57 | 72,839,589 | 58,964 |   4,008 | terribly | misguided   |\n",
    "| **terribly~inefficient** |   198 |      7.89 |      190.11 |    0.02 |   0.02 |    0.00 |   0.00 |    4.05 |    900.41 | 72,839,589 | 58,964 |   9,744 | terribly | inefficient |\n",
    "| **terribly~surprising**  |   964 |     57.10 |      906.90 |    0.01 |   0.01 |    0.02 |   0.02 |    3.82 |  3,660.97 | 72,839,589 | 58,964 |  70,540 | terribly | surprising  |\n",
    "| **terribly~sad**         | 1,241 |     80.21 |    1,160.79 |    0.01 |   0.01 |    0.02 |   0.02 |    3.73 |  4,513.50 | 72,839,589 | 58,964 |  99,081 | terribly | sad         |\n",
    "| **terribly~unfair**      |   367 |     20.86 |      346.14 |    0.01 |   0.01 |    0.01 |   0.01 |    3.71 |  1,419.19 | 72,839,589 | 58,964 |  25,769 | terribly | unfair      |\n",
    "| **terribly~frightened**  |   137 |      6.77 |      130.23 |    0.02 |   0.02 |    0.00 |   0.00 |    3.60 |    565.89 | 72,839,589 | 58,964 |   8,364 | terribly | frightened  |\n",
    "\n",
    "\n",
    "1. terribly_awry\n",
    "   >  As tends to happen , the experiment goes terribly awry .\n",
    "\n",
    "2. terribly_wrong\n",
    "   >  But he spoke with the confidence of someone who thinks his country has got things right , while others have got them terribly wrong .\n",
    "\n",
    "3. terribly_amiss\n",
    "   >  \" It was made blatantly obvious to the millions of people watching the post parade that something was terribly amiss ( with Life At Ten ) , \" De Bartolo 's letter said .\n",
    "\n",
    "4. terribly_sorry\n",
    "   >  I am terribly sorry to have to leave you after knowing you for such a short time .\n",
    "\n",
    "5. terribly_misguided\n",
    "   >  well , he 's a genius again , though I fear a terribly misguided one .\n",
    "\n",
    "6. terribly_inefficient\n",
    "   >  Well , I did write , but it was terribly inefficient to pile up the letters telling you I was drinking and studying and talking with folks in all sorts of interesting tongues .\n",
    "\n",
    "7. terribly_surprising\n",
    "   >  This story is a few days old ( and probably not terribly surprising ) , but I just wanted to mention Indiana University professor Julia R. Fox 's study that concludes that The Daily Show is just as substantive as nightly network news .\n",
    "\n",
    "8. terribly_sad\n",
    "   >  \" Although I am terribly sad that we just lost her , I think that I am privileged to have known her , \" Suen said .\n",
    "\n",
    "9. terribly_unfair\n",
    "   >  It ca n't be terribly unfair to the United States if there is zero consequence for the United States not meeting its targets .\n",
    "\n",
    "10. terribly_frightened\n",
    "   >  We are now terribly angry at your Patient ; now that we are no longer so terribly frightened about him !\n",
    "\n",
    "Saving Samples in /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/...\n",
    "\n",
    "Samples saved as...\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_awry_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_wrong_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_amiss_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_sorry_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_misguided_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_inefficient_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_surprising_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_sad_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_unfair_50ex.csv\n",
    "+ /share/compling/projects/sanpi/results/top_AM/any_bigram_examples/terribly/terribly_frightened_50ex.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-sanpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
