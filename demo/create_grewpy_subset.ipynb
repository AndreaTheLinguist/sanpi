{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\"developed from https://github.com/grew-nlp/grewpy/blob/master/examples/test_corpus.py\"\"\"\n",
    "import enum\n",
    "import os\n",
    "import sys\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from grewpy import Corpus, Request\n",
    "from grewpy.grew import GrewError as GrewError\n",
    "\n",
    "# sys.path.insert(0, os.path.abspath(os.path.join( os.path.dirname(__file__), \"../\"))) # Use local grew lib"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "connected to port: 8888\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Define functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "_META_TUP = namedtuple(\n",
    "    'meta_info',\n",
    "    ['sent_id', 'doc_id', 'sent_int', 'sent_text', 'prev_id', 'prev_text', 'next_id', 'next_text'])\n",
    "\n",
    "\n",
    "def corpus_from_path(path):\n",
    "    return Corpus(str(path))\n",
    "\n",
    "\n",
    "def grewpize_pat(raw_text):\n",
    "    return ''.join(line.strip() for line in raw_text.split('{', 1)[1].split('}', 1)[0].strip().splitlines())\n",
    "\n",
    "def parse_sent(sent_id):\n",
    "    doc_id, ordinal_str = sent_id.rsplit('_', 1)\n",
    "    ordinal_int = int(ordinal_str)\n",
    "\n",
    "    row = (sent_id, doc_id, ordinal_int, co.get(sent_id).meta['text'])\n",
    "    for context_ix in (ordinal_int + i for i in (-1, 1)):\n",
    "        c_text = ''\n",
    "        c_id = ''\n",
    "        # > conllu doc sentence numbering starts at 1\n",
    "        if context_ix > 0:\n",
    "            c_id = f'{doc_id}_{context_ix}'\n",
    "            try:\n",
    "                c_obj = co.get(c_id)\n",
    "            except GrewError:\n",
    "                c_id = ''\n",
    "            else:\n",
    "                c_text = c_obj.meta['text']\n",
    "        row += (c_id, c_text)\n",
    "\n",
    "    yield _META_TUP._make(row)\n",
    "\n",
    "\n",
    "def pprint_pat(request):\n",
    "    print(str(request).replace(';', ';\\n\\t '))\n",
    "\n",
    "def table_by_1(corpus: Corpus, pattern_request: Request, cluster: list, total_hits):\n",
    "\n",
    "    df = pd.Series(corpus.count(pattern_request, cluster)).to_frame().rename(\n",
    "        columns={0: 'total'})\n",
    "    df = df.assign(percent=(df.total / total_hits * 100).round(1))\n",
    "    df = df.sort_values('total', ascending=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def table_by_2(corpus, request, cluster, total_hits):\n",
    "    df = pd.json_normalize(corpus.count(request, cluster), sep='_').transpose(\n",
    "    ).rename(columns={0: 'total'})\n",
    "    df = df.assign(percent=(df.total / total_hits * 100).round(1))\n",
    "    df = df.sort_values('total', ascending=False)\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Program to Get file size in human-readable units like KB, MB, GB, TB\n",
    "\n",
    "class sizeUnit(enum.Enum):\n",
    "    # class to store the various units\n",
    "    BYTES = 1\n",
    "    KB = 2\n",
    "    MB = 3\n",
    "    GB = 4\n",
    "\n",
    "\n",
    "def unitConvertor(sizeInBytes, unit):\n",
    "    # Cinverts the file unit\n",
    "    if unit == sizeUnit.KB:\n",
    "        return sizeInBytes/1024\n",
    "    elif unit == sizeUnit.MB:\n",
    "        return sizeInBytes/(1024*1024)\n",
    "    elif unit == sizeUnit.GB:\n",
    "        return sizeInBytes/(1024*1024*1024)\n",
    "    else:\n",
    "        return sizeInBytes\n",
    "\n",
    "\n",
    "def fileSize(filePath, size_type, decimals=1):\n",
    "    \"\"\"File size in KB, MB and GB\"\"\"\n",
    "    size = os.path.getsize(filePath)\n",
    "    return round(unitConvertor(size, size_type), decimals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Should be able to set this up to have `conllu_path` (and `pat_path`?)\n",
    "       as input, and run it in parallel on a list of files,\n",
    "       even files from different directories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# TODO: add argument parsing (at least `argv` if not `argparse`)\n",
    "pat_path = Path('Pat/advadj/all-RB-JJs.pat')\n",
    "\n",
    "# pat_paths = Path.cwd().glob('Pat/[acsr]*/*.pat')\n",
    "# for pat_path in pat_paths:\n",
    "#     print(str(pat_path))\n",
    "\n",
    "conllu_path = Path(\n",
    "    # \"/home/arh234/data/puddin/PccVa.conll/pcc_eng_val-03.conllu\"\n",
    "    # \"/home/arh234/projects/sanpi/demo/data/corpora/gitrepo_puddin/2smallest.conll/apw_eng_199911.conllu\"\n",
    "    # \"data/corpora/gitrepo_puddin/2smallest.conll/apw_eng_199911.conllu\"\n",
    "    \"./data/corpora/gitrepo_puddin/2smallest.conll/nyt_eng_200405.conllu\"\n",
    ")\n",
    "file_size = fileSize(conllu_path,sizeUnit.MB)\n",
    "print(f'Loading corpus from {conllu_path} ({file_size} MB)...')\n",
    "co = corpus_from_path(conllu_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading corpus from data/corpora/gitrepo_puddin/2smallest.conll/nyt_eng_200405.conllu (10.3 MB)...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Describing the corpus input"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "counts_df = pd.DataFrame(index=['total'], columns=[\n",
    "                         'file_size', 'sentences', 'tokens', 'ADV', 'ADJ', 'NEG'])\n",
    "counts_df['file_size'] = f'{file_size} MB'\n",
    "counts_df['sentences'] = len(co)\n",
    "counts_df['tokens'] = sum(len(s) for s in co)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for name, spec in (\n",
    "    ('ADV', 'xpos=re\"RB.*\"'),\n",
    "    ('ADJ', 'xpos=re\"JJ.*\"'),\n",
    "    ('NEG', ('lemma=\"not\"|\"hardly\"|\"scarcely\"|\"never\"|\"rarely\"|\"barely\"|\"seldom\"|'\n",
    "             '\"no\"|\"nothing\"|\"none\"|\"nobody\"|\"neither\"|\"without\"|\"few\"|\"nor\"'),\n",
    "     # TODO: add neg raising lemma node\n",
    "     #  'N-R', ()\n",
    "     )):\n",
    "\n",
    "    req = Request(f'X[{spec}]')\n",
    "    total_count = co.count(req)\n",
    "    counts_df[name] = total_count\n",
    "\n",
    "    print(f\"\\nTotal {name} in {conllu_path.name} by exact POS:\\n\")\n",
    "    print(table_by_1(co, req, [\"X.xpos\"], total_count).to_markdown(), '\\n')\n",
    "\n",
    "    print(f\"\\nTop 10 {name} lemma in {conllu_path.name}\\n\")\n",
    "    print(table_by_1(co, req, [\"X.lemma\"], total_count).nlargest(\n",
    "        10, 'total').to_markdown(), '\\n')\n",
    "\n",
    "print(f\"# {conllu_path.name} overview\\n\")\n",
    "print(counts_df.transpose().to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Total ADV in nyt_eng_200405.conllu by exact POS:\n",
      "\n",
      "|     |   total |   percent |\n",
      "|:----|--------:|----------:|\n",
      "| RB  |    8102 |      94.7 |\n",
      "| RBR |     338 |       3.9 |\n",
      "| RBS |     118 |       1.4 | \n",
      "\n",
      "\n",
      "Top 10 ADV lemma in nyt_eng_200405.conllu\n",
      "\n",
      "|      |   total |   percent |\n",
      "|:-----|--------:|----------:|\n",
      "| not  |    1381 |      16.1 |\n",
      "| also |     304 |       3.6 |\n",
      "| just |     300 |       3.5 |\n",
      "| more |     227 |       2.7 |\n",
      "| now  |     223 |       2.6 |\n",
      "| so   |     210 |       2.5 |\n",
      "| even |     190 |       2.2 |\n",
      "| only |     190 |       2.2 |\n",
      "| as   |     183 |       2.1 |\n",
      "| here |     162 |       1.9 | \n",
      "\n",
      "\n",
      "Total ADJ in nyt_eng_200405.conllu by exact POS:\n",
      "\n",
      "|     |   total |   percent |\n",
      "|:----|--------:|----------:|\n",
      "| JJ  |   15103 |      93.2 |\n",
      "| JJR |     670 |       4.1 |\n",
      "| JJS |     439 |       2.7 | \n",
      "\n",
      "\n",
      "Top 10 ADJ lemma in nyt_eng_200405.conllu\n",
      "\n",
      "|          |   total |   percent |\n",
      "|:---------|--------:|----------:|\n",
      "| other    |     301 |       1.9 |\n",
      "| new      |     280 |       1.7 |\n",
      "| last     |     273 |       1.7 |\n",
      "| more     |     272 |       1.7 |\n",
      "| first    |     255 |       1.6 |\n",
      "| many     |     167 |       1   |\n",
      "| good     |     155 |       1   |\n",
      "| such     |     148 |       0.9 |\n",
      "| american |     139 |       0.9 |\n",
      "| former   |     106 |       0.7 | \n",
      "\n",
      "\n",
      "Total NEG in nyt_eng_200405.conllu by exact POS:\n",
      "\n",
      "|     |   total |   percent |\n",
      "|:----|--------:|----------:|\n",
      "| RB  |    1551 |      75.8 |\n",
      "| DT  |     216 |      10.6 |\n",
      "| IN  |      87 |       4.3 |\n",
      "| JJ  |      82 |       4   |\n",
      "| NN  |      72 |       3.5 |\n",
      "| CC  |      22 |       1.1 |\n",
      "| UH  |      14 |       0.7 |\n",
      "| NNS |       1 |       0   | \n",
      "\n",
      "\n",
      "Top 10 NEG lemma in nyt_eng_200405.conllu\n",
      "\n",
      "|         |   total |   percent |\n",
      "|:--------|--------:|----------:|\n",
      "| not     |    1381 |      67.5 |\n",
      "| no      |     264 |      12.9 |\n",
      "| never   |     104 |       5.1 |\n",
      "| without |      87 |       4.3 |\n",
      "| few     |      82 |       4   |\n",
      "| nothing |      52 |       2.5 |\n",
      "| nor     |      16 |       0.8 |\n",
      "| hardly  |      14 |       0.7 |\n",
      "| neither |      13 |       0.6 |\n",
      "| nobody  |      11 |       0.5 | \n",
      "\n",
      "# nyt_eng_200405.conllu overview\n",
      "\n",
      "|           | total   |\n",
      "|:----------|:--------|\n",
      "| file_size | 10.3 MB |\n",
      "| sentences | 11599   |\n",
      "| tokens    | 275113  |\n",
      "| ADV       | 8558    |\n",
      "| ADJ       | 16212   |\n",
      "| NEG       | 2045    |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## `ADV ADJ` bigrams/collocations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "req = Request(grewpize_pat(pat_path.read_text(encoding='utf8')))\n",
    "pprint_pat(req)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pattern {ADJ [xpos=re\"JJ.?\"];\n",
      "\t mod: ADJ -[advmod]-> ADV;\n",
      "\t ADV < ADJ}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "total_hits = co.count(req)\n",
    "print(\"\\n## `ADV ADJ` bigrams in\", conllu_path.name)\n",
    "print(f\"total `ADV ADJ` bigrams in {conllu_path.name}: {total_hits}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "## `ADV ADJ` bigrams in nyt_eng_200405.conllu\n",
      "total `ADV ADJ` bigrams in nyt_eng_200405.conllu: 1158\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(f\"\\n## Count bigrams in {conllu_path.name} with clustering\")\n",
    "print(f\"Bigrams by ADV lemma: Top 10\")\n",
    "print(table_by_1(co, req, [\"ADV.lemma\"], total_hits).nlargest(\n",
    "    10, 'total').to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "## Count bigrams in nyt_eng_200405.conllu with clustering\n",
      "Bigrams by ADV lemma: Top 10\n",
      "|        |   total |   percent |\n",
      "|:-------|--------:|----------:|\n",
      "| more   |     148 |      12.8 |\n",
      "| very   |      93 |       8   |\n",
      "| most   |      87 |       7.5 |\n",
      "| as     |      73 |       6.3 |\n",
      "| too    |      69 |       6   |\n",
      "| so     |      60 |       5.2 |\n",
      "| how    |      40 |       3.5 |\n",
      "| really |      26 |       2.2 |\n",
      "| less   |      26 |       2.2 |\n",
      "| pretty |      23 |       2   |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(f\"Top 5 `ADV ADJ` bigrams in {conllu_path.name}\")\n",
    "print(table_by_2(co, req, [\"ADV.lemma\", \"ADJ.lemma\"],\n",
    "      total_hits).nlargest(5, 'total').to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top 5 `ADV ADJ` bigrams in nyt_eng_200405.conllu\n",
      "|          |   total |   percent |\n",
      "|:---------|--------:|----------:|\n",
      "| how_much |      11 |       0.9 |\n",
      "| so_much  |      11 |       0.9 |\n",
      "| too_much |      11 |       0.9 |\n",
      "| so_many  |      10 |       0.9 |\n",
      "| as_much  |      10 |       0.9 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Collect context info"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "context_info = pd.concat(pd.DataFrame(parse_sent(\n",
    "                         match['sent_id'])) for match in co.search(req))\n",
    "context_info = context_info.assign(conllu_id=conllu_path.stem).set_index('sent_id')\n",
    "\n",
    "context_info = context_info[['conllu_id', 'doc_id', 'sent_int',\n",
    "                       'prev_id', 'next_id', 'prev_text', 'sent_text', 'next_text']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Save `context_info` dataframe as .psv\n",
    " Example row:\n",
    " |           | apw_eng_19991101_0021_10                                                                                                                                                                                                                                     |\n",
    " |:----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    " | conllu_id | apw_eng_199911                                                                                                                                                                                                                                               |\n",
    " | doc_id    | apw_eng_19991101_0021                                                                                                                                                                                                                                        |\n",
    " | sent_int  | 10                                                                                                                                                                                                                                                           |\n",
    " | prev_id   | apw_eng_19991101_0021_9                                                                                                                                                                                                                                      |\n",
    " | next_id   | apw_eng_19991101_0021_11                                                                                                                                                                                                                                     |\n",
    " | prev_text | -LBQ-  That 's called logical , consistent thinking , which unfortunately is not the way of Washington , D.C. , right now ,  -RDQ-  said A. Michael Noll , a University of Southern California communications professor and a former AT & employee .         |\n",
    " | sent_text | AT & claims that limiting the number of access providers may not be so bad , because a company that can plan its investment in all the equipment it requires to run the Internet through TV cables can offer service faster , cheaper and more efficiently . |\n",
    " | next_text | -LBQ-  We 're moving into an information-based economy .                                                                                                                                                                                                     |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "subset_dir = conllu_path.parent.joinpath(f'subset_{pat_path.parent.stem}')\n",
    "if not subset_dir.is_dir():\n",
    "    subset_dir.mkdir()\n",
    "label = pat_path.stem\n",
    "# TODO: make `label` an input option\n",
    "context_path = subset_dir.joinpath(\n",
    "    f'{label}:{conllu_path.stem}.context.psv')\n",
    "context_info.to_csv(context_path, sep='|')\n",
    "print(f'✔️ context info for {label} subset of {conllu_path.name} saved as:\\n'\n",
    "      f'     {context_path}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✔️ context info for all-RB-JJs subset of nyt_eng_200405.conllu saved as:\n",
      "     data/corpora/gitrepo_puddin/2smallest.conll/subset_advadj/all-RB-JJs:nyt_eng_200405.context.psv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ## Create subset conllu and save to file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "subset_path = subset_dir.joinpath(f'{label}:{conllu_path.name}')\n",
    "subset_path.write_text('\\n'.join(co.get(id).to_conll() for id in context_info.index), encoding='utf8')\n",
    "\n",
    "print(f'✔️ {label} subset of {conllu_path.name} saved as:\\n'\n",
    "      f'     {subset_path}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✔️ all-RB-JJs subset of nyt_eng_200405.conllu saved as:\n",
      "     data/corpora/gitrepo_puddin/2smallest.conll/subset_advadj/all-RB-JJs:nyt_eng_200405.conllu\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}