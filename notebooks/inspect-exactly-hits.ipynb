{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting pickled `exactly` hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions\n",
    "These are copied from `./source/analyze/utils/{dataframes, general}.py`, but jupyter won't import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(data_dir: Path, fname_glob: str, verbose: bool = False):\n",
    "    path_iter = data_dir.rglob(fname_glob)\n",
    "    if verbose:\n",
    "        path_iter = tuple(path_iter)\n",
    "        print_iter(\n",
    "            [f'../{p.relative_to(data_dir)}' for p in path_iter], bullet='-',\n",
    "            header=f'### {len(path_iter)} paths matching {fname_glob} found in {data_dir}')\n",
    "    return path_iter\n",
    "\n",
    "\n",
    "def print_iter(iter_obj,\n",
    "               bullet: str = '▸',\n",
    "            #//    logger: logging.Logger = None,\n",
    "            #//    level: int = 20,\n",
    "               header: str = ''):\n",
    "\n",
    "    bullet_str = f'\\n{bullet} '\n",
    "\n",
    "    iter_str = bullet_str.join(f'{i}' for i in iter_obj)\n",
    "\n",
    "    msg_str = f'\\n{header}{bullet_str}{iter_str}'\n",
    "    msg_str = msg_str.replace('\\n\\n', '\\n').strip(f'{bullet} ')\n",
    "\n",
    "    print(msg_str)\n",
    "    \n",
    "\n",
    "def balance_sample(full_df: pd.DataFrame,\n",
    "                   column_name: str = 'category',\n",
    "                   sample_per_value: int = 5,\n",
    "                   verbose: bool = False) -> tuple:\n",
    "    '''\n",
    "    create sample with no more than n rows satisfying each unique value\n",
    "    of the given column. A value of -1 for `sample_per_value` will limit\n",
    "    all values' results to the minimum count per value.\n",
    "    '''\n",
    "    info_message = ''\n",
    "    subsamples = []\n",
    "    for __, col_val_df in full_df.groupby(column_name):\n",
    "        # take sample if 1+ and less than length of full dataframe\n",
    "        if len(col_val_df) > sample_per_value > 0:\n",
    "            subsample_df = col_val_df.sample(sample_per_value)\n",
    "            subsamples.append(subsample_df)\n",
    "        else: \n",
    "            subsamples.append(col_val_df)\n",
    "\n",
    "    # > trim all \"by column\" sub dfs to length of shortest if -1 given\n",
    "    if sample_per_value == -1:\n",
    "        trim_len = int(min(len(sdf) for sdf in subsamples))\n",
    "        subsamples = [sdf.sample(trim_len)\n",
    "                       for sdf in subsamples]\n",
    "\n",
    "    b_sample = pd.concat(subsamples)\n",
    "\n",
    "    if verbose:\n",
    "        subset_info_table = (\n",
    "            b_sample\n",
    "            .value_counts(subset=column_name)\n",
    "            .to_frame(name='count')\n",
    "            .assign(percentage=b_sample\n",
    "                    .value_counts(column_name, normalize=True)\n",
    "                    .round(2) * 100)\n",
    "            .to_markdown())\n",
    "        label = (full_df.hits_df_pkl[0].stem + ' '\n",
    "                 if 'hits_df_pkl' in full_df.columns\n",
    "                 else '')\n",
    "        info_message = (f'\\n## {column_name} representation in {label}sample\\n'\n",
    "                        + subset_info_table)\n",
    "\n",
    "    return b_sample, info_message\n",
    "\n",
    "\n",
    "def concat_pkls(data_dir: Path = Path('/share/compling/data/sanpi/2_hit_tables'),\n",
    "                fname_glob: str = '*.pkl.gz',\n",
    "                pickles=None,\n",
    "                convert_dtypes=False,\n",
    "                verbose: bool = True) -> pd.DataFrame:\n",
    "    if not pickles:\n",
    "        pickles = find_files(Path(data_dir), fname_glob, verbose)\n",
    "\n",
    "    # tested and found that it is faster to assign `corpus` intermittently\n",
    "    df = pd.concat((pd.read_pickle(p).assign(corpus=p.stem.rsplit('_', 2)[0])\n",
    "                    for p in pickles))\n",
    "\n",
    "    dup_check_cols = cols_by_str(df, end_str=('text', 'id', 'sent'))\n",
    "    df = (df.loc[~df.duplicated(subset = dup_check_cols), :])\n",
    "    df = df.convert_dtypes()\n",
    "    df = make_cats(df, (['corpus'] + cols_by_str(df, start_str=('nr', 'neg', 'adv'),\n",
    "                                                 end_str=('lemma', 'form'))))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def cols_by_str(df: pd.DataFrame, start_str=None, end_str=None) -> list:\n",
    "    if end_str:\n",
    "        cols = df.columns[df.columns.str.endswith(end_str)]\n",
    "        if start_str:\n",
    "            cols = cols[cols.str.startswith(start_str)]\n",
    "    elif start_str:\n",
    "        cols = df.columns[df.columns.str.startswith(start_str)]\n",
    "    else:\n",
    "        cols = df.columns\n",
    "\n",
    "    return cols.to_list()\n",
    "\n",
    "\n",
    "def make_cats(orig_df:pd.DataFrame, columns: list = None) -> pd.DataFrame:\n",
    "    df = orig_df.copy()\n",
    "    if columns is None:\n",
    "        cat_suff = (\"code\", \"name\", \"path\", \"stem\")\n",
    "        columns = df.columns.str.endswith(cat_suff)\n",
    "\n",
    "    df.loc[:, columns] = df.loc[:, columns].astype(\n",
    "        'string').fillna('_').astype('category')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is copied from `./source/analyze_deps.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _optimize_df(df:pd.DataFrame) -> pd.DataFrame: \n",
    "    \n",
    "    print('Original Dataframe:')\n",
    "    df.info(memory_usage='deep')\n",
    "    \n",
    "    # * clean up dataframe a bit\n",
    "    # drop unneeded string columns\n",
    "    # was:\n",
    "    #   for c in udf.cols_by_str(df, start_str=('context', 'text', 'sent_text', 'token')):\n",
    "    for c in cols_by_str(df, start_str=('context', 'sent_text', 'token')):\n",
    "        df.pop(c)\n",
    "    # select only non-`object` dtype columns\n",
    "    relevant_cols = df.columns[~df.dtypes.astype(\n",
    "        'string').str.endswith(('object'))]\n",
    "    # limit df to `relevant_cols`\n",
    "    df = df[relevant_cols]\n",
    "    \n",
    "    # create empty dataframe with `relevant_cols` as index/rows\n",
    "    df_info = pd.DataFrame(index=relevant_cols)\n",
    "\n",
    "    df_info = df_info.assign(\n",
    "        mem0=df.memory_usage(deep=True),\n",
    "        dtype0=df.dtypes.astype('string'),\n",
    "        defined_values=df.count(),\n",
    "        unique_values=df.apply(pd.unique, axis=0).apply(len))\n",
    "    df_info = df_info.assign(\n",
    "        ratio_unique = (df_info.unique_values/df_info.defined_values).round(2))\n",
    "\n",
    "    cat_candidates = df_info.loc[df_info.ratio_unique < 0.8, :].loc[df_info.dtype0!='category'].index.to_list()\n",
    "    # catted_df = udf.make_cats(df.copy(), cat_candidates)\n",
    "    catted_df = make_cats(df.copy(), cat_candidates)\n",
    "    \n",
    "    df_info = df_info.assign(dtype1=catted_df.dtypes, mem1=catted_df.memory_usage(deep=True))\n",
    "    df_info = df_info.assign(mem_change= df_info.mem1-df_info.mem0)\n",
    "    print(df_info.sort_values(['mem_change', 'ratio_unique', 'dtype0']).to_markdown())\n",
    "    mem_improved = df_info.loc[df_info.mem_change < 0, :].index.to_list()\n",
    "    for c in df.columns[~df.columns.isin(mem_improved)]: \n",
    "        print(c, '\\t', df.loc[:, c].dtype)\n",
    "    df.loc[:, mem_improved] = catted_df.loc[:, mem_improved]\n",
    "    print('Category Converted dataframe:')\n",
    "    df.info(memory_usage='deep')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 12 paths matching exactly*hits+deps.pkl.gz found in /share/compling/data/sanpi/3_dep_info\n",
      "- ../raised/exactly_apw_neg-raised_hits+deps.pkl.gz\n",
      "- ../raised/exactly_nyt_neg-raised_hits+deps.pkl.gz\n",
      "- ../raised/exactly_puddin_neg-raised_hits+deps.pkl.gz\n",
      "- ../scoped/exactly_apw_with-relay_hits+deps.pkl.gz\n",
      "- ../scoped/exactly_nyt_with-relay_hits+deps.pkl.gz\n",
      "- ../scoped/exactly_puddin_with-relay_hits+deps.pkl.gz\n",
      "- ../contig/exactly_apw_sans-relay_hits+deps.pkl.gz\n",
      "- ../contig/exactly_nyt_sans-relay_hits+deps.pkl.gz\n",
      "- ../contig/exactly_puddin_sans-relay_hits+deps.pkl.gz\n",
      "- ../advadj/exactly_apw_all-RB-JJs_hits+deps.pkl.gz\n",
      "- ../advadj/exactly_nyt_all-RB-JJs_hits+deps.pkl.gz\n",
      "- ../advadj/exactly_puddin_all-RB-JJs_hits+deps.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "ddf = concat_pkls(data_dir=Path('/share/compling/data/sanpi/3_dep_info'), \n",
    "                  fname_glob='exactly*hits+deps.pkl.gz', \n",
    "                  convert_dtypes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataframe:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150437 entries, apw_eng_20030918_0697_20:4-5-8-9 to pcc_eng_09_047.0803_x0745587_19:3-4\n",
      "Data columns (total 43 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   colloc             150437 non-null  string  \n",
      " 1   sent_text          150437 non-null  string  \n",
      " 2   nr_form            150437 non-null  category\n",
      " 3   neg_form           150437 non-null  category\n",
      " 4   adv_form           150437 non-null  category\n",
      " 5   adj_form           150437 non-null  string  \n",
      " 6   hit_text           150437 non-null  string  \n",
      " 7   text_window        150437 non-null  string  \n",
      " 8   sent_id            150437 non-null  string  \n",
      " 9   match_id           150437 non-null  string  \n",
      " 10  colloc_id          150437 non-null  string  \n",
      " 11  token_str          150437 non-null  string  \n",
      " 12  lemma_str          150437 non-null  string  \n",
      " 13  context_prev_id    150437 non-null  string  \n",
      " 14  context_prev_sent  150437 non-null  string  \n",
      " 15  context_next_id    150437 non-null  string  \n",
      " 16  context_next_sent  150437 non-null  string  \n",
      " 17  nr_lemma           150437 non-null  category\n",
      " 18  neg_lemma          150437 non-null  category\n",
      " 19  adv_lemma          150437 non-null  category\n",
      " 20  adj_lemma          150437 non-null  string  \n",
      " 21  nr_index           457 non-null     UInt16  \n",
      " 22  neg_index          51496 non-null   UInt16  \n",
      " 23  adv_index          150437 non-null  UInt16  \n",
      " 24  adj_index          150437 non-null  UInt16  \n",
      " 25  dep_negraise       457 non-null     object  \n",
      " 26  dep_neg            51496 non-null   object  \n",
      " 27  dep_mod            150437 non-null  object  \n",
      " 28  json_source        150437 non-null  object  \n",
      " 29  utt_len            150437 non-null  UInt16  \n",
      " 30  category           150437 non-null  string  \n",
      " 31  hits_df_path       150437 non-null  object  \n",
      " 32  dep_str            150437 non-null  string  \n",
      " 33  dep_str_ix         150437 non-null  string  \n",
      " 34  dep_str_full       150437 non-null  string  \n",
      " 35  dep_str_rel        150437 non-null  string  \n",
      " 36  dep_str_mask       150437 non-null  string  \n",
      " 37  dep_str_mask_rel   150437 non-null  string  \n",
      " 38  corpus             150437 non-null  category\n",
      " 39  relay_form         2891 non-null    string  \n",
      " 40  relay_lemma        2891 non-null    string  \n",
      " 41  relay_index        2891 non-null    UInt8   \n",
      " 42  dep_relay          2891 non-null    object  \n",
      "dtypes: UInt16(5), UInt8(1), category(7), object(6), string(24)\n",
      "memory usage: 434.7 MB\n",
      "|                  |     mem0 | dtype0   |   defined_values |   unique_values |   ratio_unique | dtype1   |     mem1 |   mem_change |\n",
      "|:-----------------|---------:|:---------|-----------------:|----------------:|---------------:|:---------|---------:|-------------:|\n",
      "| lemma_str        | 29759457 | string   |           150437 |           78190 |           0.52 | category | 17540971 |    -12218486 |\n",
      "| dep_str_mask_rel | 11599471 | string   |           150437 |             291 |           0    | category |   337911 |    -11261560 |\n",
      "| dep_str_rel      | 14388422 | string   |           150437 |           26315 |           0.17 | category |  3891252 |    -10497170 |\n",
      "| dep_str_mask     |  9774606 | string   |           150437 |             190 |           0    | category |   319579 |     -9455027 |\n",
      "| category         |  9477531 | string   |           150437 |               4 |           0    | category |   150861 |     -9326670 |\n",
      "| dep_str          | 12563557 | string   |           150437 |           25541 |           0.17 | category |  3509565 |     -9053992 |\n",
      "| match_id         |  9338334 | string   |           150437 |            1233 |           0.01 | category |   415156 |     -8923178 |\n",
      "| adj_lemma        |  9517494 | string   |           150437 |            6134 |           0.04 | category |   835914 |     -8681580 |\n",
      "| adj_form         |  9520638 | string   |           150437 |            6395 |           0.04 | category |   984752 |     -8535886 |\n",
      "| colloc           | 10659162 | string   |           150437 |           19746 |           0.13 | category |  2252302 |     -8406860 |\n",
      "| hit_text         | 10964161 | string   |           150437 |           29493 |           0.2  | category |  3581143 |     -7383018 |\n",
      "| dep_str_full     | 15620880 | string   |           150437 |           64474 |           0.43 | category |  9461404 |     -6159476 |\n",
      "| relay_lemma      |  6080996 | string   |             2891 |             651 |           0.23 | category |   358482 |     -5722514 |\n",
      "| relay_form       |  6082598 | string   |             2891 |             707 |           0.24 | category |   362552 |     -5720046 |\n",
      "| dep_str_ix       | 13796015 | string   |           150437 |           63392 |           0.42 | category |  8573477 |     -5222538 |\n",
      "| sent_id          | 13185809 | string   |           150437 |           85959 |           0.57 | category | 10249989 |     -2935820 |\n",
      "| colloc_id        | 13959965 | string   |           150437 |           98943 |           0.66 | category | 11901361 |     -2058604 |\n",
      "| nr_index         |   451311 | UInt16   |              457 |              48 |           0.11 | category |   154331 |      -296980 |\n",
      "| neg_index        |   451311 | UInt16   |            51496 |             125 |           0    | category |   161997 |      -289314 |\n",
      "| relay_index      |   300874 | UInt8    |             2891 |              56 |           0.02 | category |   155834 |      -145040 |\n",
      "| utt_len          |   451311 | UInt16   |           150437 |             230 |           0    | category |   322867 |      -128444 |\n",
      "| adv_index        |   451311 | UInt16   |           150437 |             307 |           0    | category |   327521 |      -123790 |\n",
      "| adj_index        |   451311 | UInt16   |           150437 |             307 |           0    | category |   327523 |      -123788 |\n",
      "| nr_form          |   153661 | category |           150437 |              34 |           0    | category |   153661 |            0 |\n",
      "| neg_form         |   153358 | category |           150437 |              30 |           0    | category |   153358 |            0 |\n",
      "| nr_lemma         |   152182 | category |           150437 |              19 |           0    | category |   152182 |            0 |\n",
      "| neg_lemma        |   151857 | category |           150437 |              14 |           0    | category |   151857 |            0 |\n",
      "| corpus           |   150752 | category |           150437 |               3 |           0    | category |   150752 |            0 |\n",
      "| adv_form         |   415485 | category |           150437 |            1241 |           0.01 | category |   415485 |            0 |\n",
      "| adv_lemma        |   407135 | category |           150437 |            1110 |           0.01 | category |   407135 |            0 |\n",
      "| text_window      | 14500517 | string   |           150437 |          122093 |           0.81 | string   | 14500517 |            0 |\n",
      "nr_form \t category\n",
      "neg_form \t category\n",
      "adv_form \t category\n",
      "text_window \t string\n",
      "nr_lemma \t category\n",
      "neg_lemma \t category\n",
      "adv_lemma \t category\n",
      "corpus \t category\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3257568/450405812.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, mem_improved] = catted_df.loc[:, mem_improved]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Converted dataframe:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 150437 entries, apw_eng_20030918_0697_20:4-5-8-9 to pcc_eng_09_047.0803_x0745587_19:3-4\n",
      "Data columns (total 31 columns):\n",
      " #   Column            Non-Null Count   Dtype   \n",
      "---  ------            --------------   -----   \n",
      " 0   colloc            150437 non-null  category\n",
      " 1   nr_form           150437 non-null  category\n",
      " 2   neg_form          150437 non-null  category\n",
      " 3   adv_form          150437 non-null  category\n",
      " 4   adj_form          150437 non-null  category\n",
      " 5   hit_text          150437 non-null  category\n",
      " 6   text_window       150437 non-null  string  \n",
      " 7   sent_id           150437 non-null  category\n",
      " 8   match_id          150437 non-null  category\n",
      " 9   colloc_id         150437 non-null  category\n",
      " 10  lemma_str         150437 non-null  category\n",
      " 11  nr_lemma          150437 non-null  category\n",
      " 12  neg_lemma         150437 non-null  category\n",
      " 13  adv_lemma         150437 non-null  category\n",
      " 14  adj_lemma         150437 non-null  category\n",
      " 15  nr_index          150437 non-null  category\n",
      " 16  neg_index         150437 non-null  category\n",
      " 17  adv_index         150437 non-null  category\n",
      " 18  adj_index         150437 non-null  category\n",
      " 19  utt_len           150437 non-null  category\n",
      " 20  category          150437 non-null  category\n",
      " 21  dep_str           150437 non-null  category\n",
      " 22  dep_str_ix        150437 non-null  category\n",
      " 23  dep_str_full      150437 non-null  category\n",
      " 24  dep_str_rel       150437 non-null  category\n",
      " 25  dep_str_mask      150437 non-null  category\n",
      " 26  dep_str_mask_rel  150437 non-null  category\n",
      " 27  corpus            150437 non-null  category\n",
      " 28  relay_form        150437 non-null  category\n",
      " 29  relay_lemma       150437 non-null  category\n",
      " 30  relay_index       150437 non-null  category\n",
      "dtypes: category(30), string(1)\n",
      "memory usage: 101.4 MB\n"
     ]
    }
   ],
   "source": [
    "odf = _optimize_df(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = odf.columns[~odf.columns.isin(cols_by_str(odf, start_str=('dep_m', 'dep_n', 'dep_r', 'context')))].to_list()\n",
    "columns.sort()\n",
    "odf = odf.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(odf) == len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vdf = odf.loc[odf.category != 'advadj', ['category', 'neg_lemma', 'colloc', 'hit_text', 'text_window']]\n",
    "# vdf, info = balance_sample(vdf, sample_per_value=50, verbose=True)\n",
    "# print(info)\n",
    "# vdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_counts(df, columns): \n",
    "    return df.value_counts(columns).to_frame().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corpus_group</th>\n",
       "      <th>news</th>\n",
       "      <th>puddin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>advadj</th>\n",
       "      <td>4917</td>\n",
       "      <td>60971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contig</th>\n",
       "      <td>3500</td>\n",
       "      <td>42908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scoped</th>\n",
       "      <td>78</td>\n",
       "      <td>2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raised</th>\n",
       "      <td>13</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       \n",
       "corpus_group  news puddin\n",
       "category                 \n",
       "advadj        4917  60971\n",
       "contig        3500  42908\n",
       "scoped          78   2750\n",
       "raised          13    327"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf = odf.loc[odf.adv_lemma=='exactly', :]\n",
    "odf.loc[odf.corpus.str.endswith('puddin'), 'corpus_group'] = 'puddin'\n",
    "odf.loc[odf.corpus.str.endswith(('nyt', 'apw')), 'corpus_group'] = 'news'\n",
    "general_counts = show_counts(odf, ['category', 'corpus_group']).unstack().sort_values(('count', 'puddin'), ascending=False) # type: ignore\n",
    "general_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(odf) < len(ddf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: ☝️ `odf` is shorter (fewer rows) than original loaded hits because adverbs other than `'exactly'` have been dropped.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_cols(tdf:pd.DataFrame): \n",
    "    \n",
    "    summary = tdf.describe().transpose()\n",
    "    summary = summary.assign(top_percent = (((pd.to_numeric(summary.freq) / len(tdf)))*100).round(2))\n",
    "    summary = summary.rename(columns={'top': 'top_value', 'freq':'top_freq'})\n",
    "    \n",
    "    return summary.convert_dtypes().sort_values('unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total \"exactly\" hits for all patterns: 115464\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top_value</th>\n",
       "      <th>top_freq</th>\n",
       "      <th>top_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adv_lemma</th>\n",
       "      <td>115464</td>\n",
       "      <td>1</td>\n",
       "      <td>exactly</td>\n",
       "      <td>115464</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>115464</td>\n",
       "      <td>4</td>\n",
       "      <td>advadj</td>\n",
       "      <td>65888</td>\n",
       "      <td>57.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_lemma</th>\n",
       "      <td>115464</td>\n",
       "      <td>14</td>\n",
       "      <td>_</td>\n",
       "      <td>65888</td>\n",
       "      <td>57.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr_lemma</th>\n",
       "      <td>115464</td>\n",
       "      <td>17</td>\n",
       "      <td>_</td>\n",
       "      <td>115124</td>\n",
       "      <td>99.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relay_lemma</th>\n",
       "      <td>115464</td>\n",
       "      <td>622</td>\n",
       "      <td>_</td>\n",
       "      <td>112636</td>\n",
       "      <td>97.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_id</th>\n",
       "      <td>115464</td>\n",
       "      <td>978</td>\n",
       "      <td>4-5</td>\n",
       "      <td>9361</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_lemma</th>\n",
       "      <td>115464</td>\n",
       "      <td>4240</td>\n",
       "      <td>sure</td>\n",
       "      <td>19181</td>\n",
       "      <td>16.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conllu_id</th>\n",
       "      <td>115464</td>\n",
       "      <td>6611</td>\n",
       "      <td>pcc_eng_22_037</td>\n",
       "      <td>65</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_text</th>\n",
       "      <td>115464</td>\n",
       "      <td>12602</td>\n",
       "      <td>exactly sure</td>\n",
       "      <td>9620</td>\n",
       "      <td>8.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma_str</th>\n",
       "      <td>115464</td>\n",
       "      <td>59565</td>\n",
       "      <td>because we understand that not every purchase ...</td>\n",
       "      <td>1942</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_id</th>\n",
       "      <td>115464</td>\n",
       "      <td>65702</td>\n",
       "      <td>pcc_eng_02_072.4537_x1155490_12</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colloc_id</th>\n",
       "      <td>115464</td>\n",
       "      <td>65890</td>\n",
       "      <td>pcc_eng_18_097.0237_x1555301_14:24-25</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_window</th>\n",
       "      <td>115464</td>\n",
       "      <td>89816</td>\n",
       "      <td>every purchase is exactly right , we 've</td>\n",
       "      <td>971</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count  unique  \\\n",
       "adv_lemma    115464       1   \n",
       "category     115464       4   \n",
       "neg_lemma    115464      14   \n",
       "nr_lemma     115464      17   \n",
       "relay_lemma  115464     622   \n",
       "match_id     115464     978   \n",
       "adj_lemma    115464    4240   \n",
       "conllu_id    115464    6611   \n",
       "hit_text     115464   12602   \n",
       "lemma_str    115464   59565   \n",
       "sent_id      115464   65702   \n",
       "colloc_id    115464   65890   \n",
       "text_window  115464   89816   \n",
       "\n",
       "                                                     top_value  top_freq  \\\n",
       "adv_lemma                                              exactly    115464   \n",
       "category                                                advadj     65888   \n",
       "neg_lemma                                                    _     65888   \n",
       "nr_lemma                                                     _    115124   \n",
       "relay_lemma                                                  _    112636   \n",
       "match_id                                                   4-5      9361   \n",
       "adj_lemma                                                 sure     19181   \n",
       "conllu_id                                       pcc_eng_22_037        65   \n",
       "hit_text                                          exactly sure      9620   \n",
       "lemma_str    because we understand that not every purchase ...      1942   \n",
       "sent_id                        pcc_eng_02_072.4537_x1155490_12         6   \n",
       "colloc_id                pcc_eng_18_097.0237_x1555301_14:24-25         4   \n",
       "text_window           every purchase is exactly right , we 've       971   \n",
       "\n",
       "             top_percent  \n",
       "adv_lemma          100.0  \n",
       "category           57.06  \n",
       "neg_lemma          57.06  \n",
       "nr_lemma           99.71  \n",
       "relay_lemma        97.55  \n",
       "match_id            8.11  \n",
       "adj_lemma          16.61  \n",
       "conllu_id           0.06  \n",
       "hit_text            8.33  \n",
       "lemma_str           1.68  \n",
       "sent_id             0.01  \n",
       "colloc_id            0.0  \n",
       "text_window         0.84  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf = odf.assign(conllu_id=odf.sent_id.str.rsplit('_', 2).str.get(0).str.split('.').str.get(0).astype('string').astype('category')) # type: ignore\n",
    "tdf = odf[cols_by_str(odf, end_str=('lemma','id', 'text', 'window', 'category', 'Pol')) + ['lemma_str']]\n",
    "print(f'Total \"exactly\" hits for all patterns: {len(tdf)}')\n",
    "summary_tdf = summarize_text_cols(tdf)\n",
    "summary_tdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $PosPol$ context dataset\n",
    "\n",
    "### Option A\n",
    "bare collocation tokens (`advadj.all-RB-JJs` pattern match) which do not appear as matches for any other pattern match (i.e. $NegPol$ contexts).\n",
    "\n",
    "*That is, the `colloc_id` (unique `ADV` & `ADJ` nodes in unique sentence tokens) is not duplicated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfp_a = tdf.loc[(tdf.category=='advadj') & (~tdf.duplicated(subset='colloc_id', keep=False)), :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B\n",
    "categorize $NegPol$ set first (`tdfn`), then compute complement of that (i.e. $ALL - NegPol$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top_value</th>\n",
       "      <th>top_freq</th>\n",
       "      <th>top_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adv_lemma</th>\n",
       "      <td>49576</td>\n",
       "      <td>1</td>\n",
       "      <td>exactly</td>\n",
       "      <td>49576</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>49576</td>\n",
       "      <td>3</td>\n",
       "      <td>contig</td>\n",
       "      <td>46408</td>\n",
       "      <td>93.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_lemma</th>\n",
       "      <td>49576</td>\n",
       "      <td>13</td>\n",
       "      <td>not</td>\n",
       "      <td>45771</td>\n",
       "      <td>92.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr_lemma</th>\n",
       "      <td>49576</td>\n",
       "      <td>17</td>\n",
       "      <td>_</td>\n",
       "      <td>49236</td>\n",
       "      <td>99.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relay_lemma</th>\n",
       "      <td>49576</td>\n",
       "      <td>622</td>\n",
       "      <td>_</td>\n",
       "      <td>46748</td>\n",
       "      <td>94.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_id</th>\n",
       "      <td>49576</td>\n",
       "      <td>833</td>\n",
       "      <td>3-4-5</td>\n",
       "      <td>7889</td>\n",
       "      <td>15.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_lemma</th>\n",
       "      <td>49576</td>\n",
       "      <td>3743</td>\n",
       "      <td>sure</td>\n",
       "      <td>9548</td>\n",
       "      <td>19.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conllu_id</th>\n",
       "      <td>49576</td>\n",
       "      <td>5908</td>\n",
       "      <td>pcc_eng_01_001</td>\n",
       "      <td>29</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_text</th>\n",
       "      <td>49576</td>\n",
       "      <td>8174</td>\n",
       "      <td>not exactly sure</td>\n",
       "      <td>6420</td>\n",
       "      <td>12.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_window</th>\n",
       "      <td>49576</td>\n",
       "      <td>43673</td>\n",
       "      <td>we understand that not every purchase is exact...</td>\n",
       "      <td>971</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma_str</th>\n",
       "      <td>49576</td>\n",
       "      <td>45040</td>\n",
       "      <td>because we understand that not every purchase ...</td>\n",
       "      <td>971</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_id</th>\n",
       "      <td>49576</td>\n",
       "      <td>49160</td>\n",
       "      <td>pcc_eng_08_044.5355_x0704514_38</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colloc_id</th>\n",
       "      <td>49576</td>\n",
       "      <td>49280</td>\n",
       "      <td>pcc_eng_18_097.0237_x1555301_14:24-25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count  unique                                          top_value  \\\n",
       "adv_lemma    49576       1                                            exactly   \n",
       "category     49576       3                                             contig   \n",
       "neg_lemma    49576      13                                                not   \n",
       "nr_lemma     49576      17                                                  _   \n",
       "relay_lemma  49576     622                                                  _   \n",
       "match_id     49576     833                                              3-4-5   \n",
       "adj_lemma    49576    3743                                               sure   \n",
       "conllu_id    49576    5908                                     pcc_eng_01_001   \n",
       "hit_text     49576    8174                                   not exactly sure   \n",
       "text_window  49576   43673  we understand that not every purchase is exact...   \n",
       "lemma_str    49576   45040  because we understand that not every purchase ...   \n",
       "sent_id      49576   49160                    pcc_eng_08_044.5355_x0704514_38   \n",
       "colloc_id    49576   49280              pcc_eng_18_097.0237_x1555301_14:24-25   \n",
       "\n",
       "             top_freq  top_percent  \n",
       "adv_lemma       49576        100.0  \n",
       "category        46408        93.61  \n",
       "neg_lemma       45771        92.32  \n",
       "nr_lemma        49236        99.31  \n",
       "relay_lemma     46748         94.3  \n",
       "match_id         7889        15.91  \n",
       "adj_lemma        9548        19.26  \n",
       "conllu_id          29         0.06  \n",
       "hit_text         6420        12.95  \n",
       "text_window       971         1.96  \n",
       "lemma_str         971         1.96  \n",
       "sent_id             3         0.01  \n",
       "colloc_id           3         0.01  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tdfn = tdf.loc[tdf.neg_lemma!='_', :]\n",
    "tdfp_b = tdf.loc[~tdf.colloc_id.isin(tdfn.colloc_id), :]\n",
    "summarize_text_cols(tdfn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(tdfp_a.index == tdfp_b.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options A and B are identical\n",
    "so since $NegPol$ is more directly defined, and has to be separated out anyway, it's simpler to just get the \"complement\", (`tdfp_b` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top_value</th>\n",
       "      <th>top_freq</th>\n",
       "      <th>top_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adv_lemma</th>\n",
       "      <td>16610</td>\n",
       "      <td>1</td>\n",
       "      <td>exactly</td>\n",
       "      <td>16610</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>16610</td>\n",
       "      <td>1</td>\n",
       "      <td>advadj</td>\n",
       "      <td>16610</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_lemma</th>\n",
       "      <td>16610</td>\n",
       "      <td>1</td>\n",
       "      <td>_</td>\n",
       "      <td>16610</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr_lemma</th>\n",
       "      <td>16610</td>\n",
       "      <td>1</td>\n",
       "      <td>_</td>\n",
       "      <td>16610</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relay_lemma</th>\n",
       "      <td>16610</td>\n",
       "      <td>1</td>\n",
       "      <td>_</td>\n",
       "      <td>16610</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_id</th>\n",
       "      <td>16610</td>\n",
       "      <td>122</td>\n",
       "      <td>3-4</td>\n",
       "      <td>2074</td>\n",
       "      <td>12.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_lemma</th>\n",
       "      <td>16610</td>\n",
       "      <td>1346</td>\n",
       "      <td>right</td>\n",
       "      <td>6517</td>\n",
       "      <td>39.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_text</th>\n",
       "      <td>16610</td>\n",
       "      <td>1446</td>\n",
       "      <td>exactly right</td>\n",
       "      <td>6450</td>\n",
       "      <td>38.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conllu_id</th>\n",
       "      <td>16610</td>\n",
       "      <td>4373</td>\n",
       "      <td>pcc_eng_24_085</td>\n",
       "      <td>18</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_window</th>\n",
       "      <td>16610</td>\n",
       "      <td>13345</td>\n",
       "      <td>That 's exactly right .</td>\n",
       "      <td>454</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemma_str</th>\n",
       "      <td>16610</td>\n",
       "      <td>14548</td>\n",
       "      <td>that be exactly right .</td>\n",
       "      <td>537</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_id</th>\n",
       "      <td>16610</td>\n",
       "      <td>16556</td>\n",
       "      <td>pcc_eng_04_093.4894_x1494321_12</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colloc_id</th>\n",
       "      <td>16610</td>\n",
       "      <td>16610</td>\n",
       "      <td>pcc_eng_13_103.2841_x1652612_23:5-6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count  unique                            top_value  top_freq  \\\n",
       "adv_lemma    16610       1                              exactly     16610   \n",
       "category     16610       1                               advadj     16610   \n",
       "neg_lemma    16610       1                                    _     16610   \n",
       "nr_lemma     16610       1                                    _     16610   \n",
       "relay_lemma  16610       1                                    _     16610   \n",
       "match_id     16610     122                                  3-4      2074   \n",
       "adj_lemma    16610    1346                                right      6517   \n",
       "hit_text     16610    1446                        exactly right      6450   \n",
       "conllu_id    16610    4373                       pcc_eng_24_085        18   \n",
       "text_window  16610   13345              That 's exactly right .       454   \n",
       "lemma_str    16610   14548              that be exactly right .       537   \n",
       "sent_id      16610   16556      pcc_eng_04_093.4894_x1494321_12         3   \n",
       "colloc_id    16610   16610  pcc_eng_13_103.2841_x1652612_23:5-6         1   \n",
       "\n",
       "             top_percent  \n",
       "adv_lemma          100.0  \n",
       "category           100.0  \n",
       "neg_lemma          100.0  \n",
       "nr_lemma           100.0  \n",
       "relay_lemma        100.0  \n",
       "match_id           12.49  \n",
       "adj_lemma          39.24  \n",
       "hit_text           38.83  \n",
       "conllu_id           0.11  \n",
       "text_window         2.73  \n",
       "lemma_str           3.23  \n",
       "sent_id             0.02  \n",
       "colloc_id           0.01  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdfp = tdf.loc[~tdf.colloc_id.isin(tdfn.colloc_id)]\n",
    "summarize_text_cols(tdfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total `exactly` collocations identified: 65888\n",
      "PosPol: 14% : 16610 hits\n",
      "NegPol: 43% : 49576 hits\n"
     ]
    }
   ],
   "source": [
    "print(f'Total `exactly` collocations identified: {odf.category.value_counts()[\"advadj\"]}')\n",
    "print(f'PosPol: {round(100*len(tdfp)/len(tdf))}% : {len(tdfp)} hits')\n",
    "print(f'NegPol: {str(round(100*len(tdfn)/len(tdf))).zfill(2)}% : {len(tdfn)} hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfp = tdfp.assign(polarity='positive')\n",
    "select_cols = ['adj_lemma', 'text_window', 'lemma_str'] + cols_by_str(tdfp, end_str=('_id', 'corpus'))\n",
    "pos_text_info = tdfp.loc[:, select_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfn = tdfn.assign(polarity='negative')\n",
    "tdf_with_overlap = tdf\n",
    "tdf = pd.concat([tdfp, tdfn]).sort_values('colloc_id')\n",
    "tdf.to_pickle('/share/compling/projects/sanpi/notebooks/exactly_out/all-exactly-hits_text+polarity.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>polarity</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>SUM</th>\n",
       "      <th>neg_ratio</th>\n",
       "      <th>pos_ratio</th>\n",
       "      <th>neg_bin</th>\n",
       "      <th>pos_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_lemma</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SUM</th>\n",
       "      <td>49576</td>\n",
       "      <td>16610</td>\n",
       "      <td>66186</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>9548</td>\n",
       "      <td>118</td>\n",
       "      <td>9666</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1769</td>\n",
       "      <td>6517</td>\n",
       "      <td>8286</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alike</th>\n",
       "      <td>2410</td>\n",
       "      <td>1107</td>\n",
       "      <td>3517</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear</th>\n",
       "      <td>1755</td>\n",
       "      <td>146</td>\n",
       "      <td>1901</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doughy</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picture-perfect</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dour</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dovish</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left-wing</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4241 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "polarity         negative  positive    SUM  neg_ratio  pos_ratio  neg_bin  \\\n",
       "adj_lemma                                                                   \n",
       "SUM                 49576     16610  66186      0.749      0.251      0.7   \n",
       "sure                 9548       118   9666      0.988      0.012      1.0   \n",
       "right                1769      6517   8286      0.213      0.787      0.2   \n",
       "alike                2410      1107   3517      0.685      0.315      0.7   \n",
       "clear                1755       146   1901      0.923      0.077      0.9   \n",
       "...                   ...       ...    ...        ...        ...      ...   \n",
       "doughy                  1         0      1      1.000      0.000      1.0   \n",
       "picture-perfect         1         0      1      1.000      0.000      1.0   \n",
       "dour                    0         1      1      0.000      1.000      0.0   \n",
       "dovish                  1         0      1      1.000      0.000      1.0   \n",
       "left-wing               1         0      1      1.000      0.000      1.0   \n",
       "\n",
       "polarity         pos_bin  \n",
       "adj_lemma                 \n",
       "SUM                  0.3  \n",
       "sure                 0.0  \n",
       "right                0.8  \n",
       "alike                0.3  \n",
       "clear                0.1  \n",
       "...                  ...  \n",
       "doughy               0.0  \n",
       "picture-perfect      0.0  \n",
       "dour                 1.0  \n",
       "dovish               0.0  \n",
       "left-wing            0.0  \n",
       "\n",
       "[4241 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist = pd.crosstab(tdf.adj_lemma, tdf.polarity,\n",
    "                        margins=True, margins_name='SUM')\n",
    "freq_dist = freq_dist.assign(neg_ratio=(freq_dist.negative/freq_dist.SUM).round(3),\n",
    "                             pos_ratio=(freq_dist.positive/freq_dist.SUM).round(3))\n",
    "freq_dist = freq_dist.assign(neg_bin=freq_dist.neg_ratio.round(1),\n",
    "                             pos_bin=freq_dist.pos_ratio.round(1))\n",
    "\n",
    "freq_dist.sort_values('SUM', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-and               0\n",
       "post-dubstep       0\n",
       "poster-size        0\n",
       "poster-children    0\n",
       "postcard           0\n",
       "                  ..\n",
       "first-choice       0\n",
       "first              0\n",
       "firmer             0\n",
       "firm               0\n",
       "~healthy           0\n",
       "Name: adj_lemma, Length: 6134, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdfp.adj_lemma[~tdfp.adj_lemma.isin(freq_dist.index)].value_counts()\n",
    "#TODO?? why does it say there is a mismatch between the crosstab and pospol adj set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text_info = pos_text_info.assign(\n",
    "    neg_ratio = pos_text_info.adj_lemma.apply(lambda a: freq_dist.loc[a, 'neg_ratio'] if a in freq_dist.index else None)) # type: ignore\n",
    "pos_text_info.sort_values(['neg_ratio', 'conllu_id'], ascending=False)\n",
    "pos_text_info.to_csv('/share/compling/projects/sanpi/notebooks/exactly_out/pos_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh5 = freq_dist.loc[freq_dist.SUM >= 5, :]\n",
    "freq_thresh5.sort_values(['neg_bin', 'SUM', 'neg_ratio'], ascending=False).to_csv('/share/compling/projects/sanpi/notebooks/exactly_out/freq_thresh5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh100 = freq_dist.loc[freq_dist.SUM >= 100, :]\n",
    "freq_thresh100.sort_values(['neg_bin', 'SUM', 'neg_ratio' ], ascending=False).to_csv('/share/compling/projects/sanpi/notebooks/exactly_out/freq_thresh100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_thresh200 = freq_dist.loc[freq_dist.SUM >= 200, :]\n",
    "freq_thresh200.sort_values(['neg_bin', 'SUM', 'neg_ratio' ], ascending=False).to_csv('/share/compling/projects/sanpi/notebooks/exactly_out/freq_thresh200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bsamp, __ = balance_sample(tdf, column_name='polarity', sample_per_value=3)\n",
    "# bsamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th>adj_lemma</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <th>sure</th>\n",
       "      <td>9548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <th>right</th>\n",
       "      <td>6517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">negative</th>\n",
       "      <th>alike</th>\n",
       "      <td>2410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>1769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear</th>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outdoorsy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outrageous</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outre</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outsize</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <th>zippy</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5089 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count\n",
       "polarity adj_lemma        \n",
       "negative sure         9548\n",
       "positive right        6517\n",
       "negative alike        2410\n",
       "         right        1769\n",
       "         clear        1755\n",
       "...                    ...\n",
       "         outdoorsy       1\n",
       "         outrageous      1\n",
       "         outre           1\n",
       "         outsize         1\n",
       "positive zippy           1\n",
       "\n",
       "[5089 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_counts(tdf, ['polarity', 'adj_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_lemma</th>\n",
       "      <th>nr_lemma</th>\n",
       "      <th>relay_lemma</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <th>_</th>\n",
       "      <th>_</th>\n",
       "      <td>45459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_</th>\n",
       "      <th>_</th>\n",
       "      <th>_</th>\n",
       "      <td>16610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">no</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">_</th>\n",
       "      <th>two</th>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <th>_</th>\n",
       "      <th>_</th>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">no</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">_</th>\n",
       "      <th>development</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>designer</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>designation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without</th>\n",
       "      <th>_</th>\n",
       "      <th>support</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                count\n",
       "neg_lemma nr_lemma relay_lemma       \n",
       "not       _        _            45459\n",
       "_         _        _            16610\n",
       "no        _        two            700\n",
       "                   one            355\n",
       "never     _        _              349\n",
       "...                               ...\n",
       "no        _        development      1\n",
       "                   designer         1\n",
       "                   designation      1\n",
       "                   description      1\n",
       "without   _        support          1\n",
       "\n",
       "[697 rows x 1 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_counts(tdf, ['neg_lemma', 'nr_lemma', 'relay_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     | relay_lemma    | nr_lemma   |   count |\n",
      "|----:|:---------------|:-----------|--------:|\n",
      "|   0 | _              | _          |   63018 |\n",
      "|   1 | two            | _          |     701 |\n",
      "|   2 | one            | _          |     358 |\n",
      "|   3 | piece          | _          |     190 |\n",
      "|   4 | _              | think      |     126 |\n",
      "|   5 | arrangement    | _          |     100 |\n",
      "|   6 | people         | _          |     100 |\n",
      "|   7 | _              | look       |      81 |\n",
      "|   8 | _              | seem       |      58 |\n",
      "|   9 | case           | _          |      38 |\n",
      "|  10 | item           | _          |      38 |\n",
      "|  11 | situation      | _          |      36 |\n",
      "|  12 | _              | want       |      31 |\n",
      "|  13 | business       | _          |      30 |\n",
      "|  14 | patient        | _          |      22 |\n",
      "|  15 | pair           | _          |      17 |\n",
      "|  16 | project        | _          |      16 |\n",
      "|  17 | _              | believe    |      16 |\n",
      "|  18 | individual     | _          |      15 |\n",
      "|  19 | snowflake      | _          |      15 |\n",
      "|  20 | stone          | _          |      13 |\n",
      "|  21 | company        | _          |      13 |\n",
      "|  22 | long           | _          |      12 |\n",
      "|  23 | hank           | _          |      12 |\n",
      "|  24 | journey        | _          |      12 |\n",
      "|  25 | product        | _          |      11 |\n",
      "|  26 | style          | _          |      11 |\n",
      "|  27 | home           | _          |      11 |\n",
      "|  28 | client         | _          |      10 |\n",
      "|  29 | rug            | _          |      10 |\n",
      "|  30 | forget         | _          |      10 |\n",
      "|  31 | day            | _          |       9 |\n",
      "|  32 | _              | expect     |       8 |\n",
      "|  33 | event          | _          |       8 |\n",
      "|  34 | student        | _          |       7 |\n",
      "|  35 | plan           | _          |       7 |\n",
      "|  36 | program        | _          |       7 |\n",
      "|  37 | property       | _          |       7 |\n",
      "|  38 | skein          | _          |       7 |\n",
      "|  39 | shade          | _          |       7 |\n",
      "|  40 | tile           | _          |       7 |\n",
      "|  41 | bag            | _          |       7 |\n",
      "|  42 | session        | _          |       7 |\n",
      "|  43 | doubt          | _          |       6 |\n",
      "|  44 | thing          | _          |       6 |\n",
      "|  45 | diamond        | _          |       6 |\n",
      "|  46 | person         | _          |       6 |\n",
      "|  47 | design         | _          |       6 |\n",
      "|  48 | plaque         | _          |       6 |\n",
      "|  49 | injury         | _          |       6 |\n",
      "|  50 | room           | _          |       6 |\n",
      "|  51 | relationship   | _          |       6 |\n",
      "|  52 | game           | _          |       6 |\n",
      "|  53 | brain          | _          |       5 |\n",
      "|  54 | bar            | _          |       5 |\n",
      "|  55 | family         | _          |       5 |\n",
      "|  56 | _              | appear     |       5 |\n",
      "|  57 | dog            | _          |       5 |\n",
      "|  58 | 2              | _          |       5 |\n",
      "|  59 | customer       | _          |       5 |\n",
      "|  60 | tree           | _          |       5 |\n",
      "|  61 | body           | _          |       5 |\n",
      "|  62 | fan            | _          |       5 |\n",
      "|  63 | artist         | _          |       5 |\n",
      "|  64 | face           | _          |       5 |\n",
      "|  65 | experience     | _          |       5 |\n",
      "|  66 | garment        | _          |       5 |\n",
      "|  67 | organization   | _          |       5 |\n",
      "|  68 | matter         | _          |       5 |\n",
      "|  69 | exception      | _          |       5 |\n",
      "|  70 | problem        | _          |       5 |\n",
      "|  71 | card           | _          |       5 |\n",
      "|  72 | bracelet       | _          |       4 |\n",
      "|  73 | woman          | _          |       4 |\n",
      "|  74 | fingerprint    | _          |       4 |\n",
      "|  75 | child          | _          |       4 |\n",
      "|  76 | pen            | _          |       4 |\n",
      "|  77 | school         | _          |       4 |\n",
      "|  78 | be             | _          |       4 |\n",
      "|  79 | man            | _          |       4 |\n",
      "|  80 | show           | _          |       4 |\n",
      "|  81 | decanter       | _          |       4 |\n",
      "|  82 | divorce        | _          |       4 |\n",
      "|  83 | system         | _          |       4 |\n",
      "|  84 | set            | _          |       4 |\n",
      "|  85 | egg            | _          |       3 |\n",
      "|  86 | strategy       | _          |       3 |\n",
      "|  87 | necklace       | _          |       3 |\n",
      "|  88 | sheet          | _          |       3 |\n",
      "|  89 | earring        | _          |       3 |\n",
      "|  90 | group          | _          |       3 |\n",
      "|  91 | position       | _          |       3 |\n",
      "|  92 | pattern        | _          |       3 |\n",
      "|  93 | have           | _          |       3 |\n",
      "|  94 | he             | _          |       3 |\n",
      "|  95 | hide           | _          |       3 |\n",
      "|  96 | car            | _          |       3 |\n",
      "|  97 | story          | _          |       3 |\n",
      "|  98 | community      | _          |       3 |\n",
      "|  99 | crate          | _          |       3 |\n",
      "| 100 | play           | _          |       3 |\n",
      "| 101 | shopper        | _          |       3 |\n",
      "| 102 | skin           | _          |       3 |\n",
      "| 103 | country        | _          |       3 |\n",
      "| 104 | job            | _          |       3 |\n",
      "| 105 | ring           | _          |       3 |\n",
      "| 106 | invitation     | _          |       3 |\n",
      "| 107 | figure         | _          |       3 |\n",
      "| 108 | deal           | _          |       3 |\n",
      "| 109 | state          | _          |       3 |\n",
      "| 110 | shoe           | _          |       3 |\n",
      "| 111 | performance    | _          |       3 |\n",
      "| 112 | dad            | _          |       3 |\n",
      "| 113 | facility       | _          |       3 |\n",
      "| 114 | playthrough    | _          |       3 |\n",
      "| 115 | call           | _          |       3 |\n",
      "| 116 | city           | _          |       3 |\n",
      "| 117 | board          | _          |       3 |\n",
      "| 118 | work           | _          |       3 |\n",
      "| 119 | athlete        | _          |       3 |\n",
      "| 120 | learner        | _          |       3 |\n",
      "| 121 | ball           | _          |       3 |\n",
      "| 122 | batch          | _          |       3 |\n",
      "| 123 | tour           | _          |       3 |\n",
      "| 124 | makeover       | _          |       3 |\n",
      "| 125 | being          | _          |       3 |\n",
      "| 126 | watch          | _          |       3 |\n",
      "| 127 | _              | intend     |       3 |\n",
      "| 128 | _              | imagine    |       3 |\n",
      "| 129 | week           | _          |       3 |\n",
      "| 130 | branch         | _          |       3 |\n",
      "| 131 | breakup        | _          |       3 |\n",
      "| 132 | tee            | _          |       3 |\n",
      "| 133 | map            | _          |       3 |\n",
      "| 134 | Drupalist      | _          |       3 |\n",
      "| 135 | network        | _          |       3 |\n",
      "| 136 | music          | _          |       3 |\n",
      "| 137 | building       | _          |       3 |\n",
      "| 138 | doorway        | _          |       2 |\n",
      "| 139 | environment    | _          |       2 |\n",
      "| 140 | say            | _          |       2 |\n",
      "| 141 | planet         | _          |       2 |\n",
      "| 142 | walk           | _          |       2 |\n",
      "| 143 | agency         | _          |       2 |\n",
      "| 144 | sculpture      | _          |       2 |\n",
      "| 145 | season         | _          |       2 |\n",
      "| 146 | menus          | _          |       2 |\n",
      "| 147 | Depression     | _          |       2 |\n",
      "| 148 | hat            | _          |       2 |\n",
      "| 149 | human          | _          |       2 |\n",
      "| 150 | instrument     | _          |       2 |\n",
      "| 151 | Gonzales       | _          |       2 |\n",
      "| 152 | member         | _          |       2 |\n",
      "| 153 | display        | _          |       2 |\n",
      "| 154 | analogy        | _          |       2 |\n",
      "| 155 | vehicle        | _          |       2 |\n",
      "| 156 | meal           | _          |       2 |\n",
      "| 157 | vagina         | _          |       2 |\n",
      "| 158 | neuron         | _          |       2 |\n",
      "| 159 | overlook       | _          |       2 |\n",
      "| 160 | shipment       | _          |       2 |\n",
      "| 161 | definition     | _          |       2 |\n",
      "| 162 | do             | _          |       2 |\n",
      "| 163 | mind           | _          |       2 |\n",
      "| 164 | house          | _          |       2 |\n",
      "| 165 | era            | _          |       2 |\n",
      "| 166 | frame          | _          |       2 |\n",
      "| 167 | form           | _          |       2 |\n",
      "| 168 | Lake           | _          |       2 |\n",
      "| 169 | of             | _          |       2 |\n",
      "| 170 | officer        | _          |       2 |\n",
      "| 171 | residence      | _          |       2 |\n",
      "| 172 | five-stand     | _          |       2 |\n",
      "| 173 | Kippins        | _          |       2 |\n",
      "| 174 | player         | _          |       2 |\n",
      "| 175 | print          | _          |       2 |\n",
      "| 176 | restaurant     | _          |       2 |\n",
      "| 177 | month          | _          |       2 |\n",
      "| 178 | rock           | _          |       2 |\n",
      "| 179 | Texas          | _          |       2 |\n",
      "| 180 | mirror         | _          |       2 |\n",
      "| 181 | water          | _          |       2 |\n",
      "| 182 | planter        | _          |       2 |\n",
      "| 183 | deck           | _          |       2 |\n",
      "| 184 | odd            | _          |       2 |\n",
      "| 185 | _              | likely     |       2 |\n",
      "| 186 | eye            | _          |       2 |\n",
      "| 187 | grain          | _          |       2 |\n",
      "| 188 | plant          | _          |       2 |\n",
      "| 189 | example        | _          |       2 |\n",
      "| 190 | _              | suggest    |       2 |\n",
      "| 191 | table          | _          |       2 |\n",
      "| 192 | investor       | _          |       2 |\n",
      "| 193 | truck          | _          |       2 |\n",
      "| 194 | sound          | _          |       2 |\n",
      "| 195 | spot           | _          |       2 |\n",
      "| 196 | time           | _          |       2 |\n",
      "| 197 | kilim          | _          |       2 |\n",
      "| 198 | longer         | _          |       2 |\n",
      "| 199 | step           | _          |       2 |\n",
      "| 200 | local          | _          |       2 |\n",
      "| 201 | coin           | _          |       2 |\n",
      "| 202 | store          | _          |       2 |\n",
      "| 203 | storm          | _          |       2 |\n",
      "| 204 | box            | _          |       2 |\n",
      "| 205 | term           | _          |       2 |\n",
      "| 206 | process        | _          |       2 |\n",
      "| 207 | chest          | _          |       2 |\n",
      "| 208 | breast         | _          |       2 |\n",
      "| 209 | pearl          | _          |       2 |\n",
      "| 210 | supplement     | _          |       2 |\n",
      "| 211 | line           | _          |       2 |\n",
      "| 212 | chair          | _          |       2 |\n",
      "| 213 | ceremony       | _          |       2 |\n",
      "| 214 | surgery        | _          |       2 |\n",
      "| 215 | level          | _          |       2 |\n",
      "| 216 | cart           | _          |       2 |\n",
      "| 217 | tattoo         | _          |       2 |\n",
      "| 218 | leave          | _          |       2 |\n",
      "| 219 | lawyer         | _          |       2 |\n",
      "| 220 | cancer         | _          |       2 |\n",
      "| 221 | campaign       | _          |       2 |\n",
      "| 222 | loss           | _          |       2 |\n",
      "| 223 | zebras         | _          |       2 |\n",
      "| 224 | slab           | _          |       2 |\n",
      "| 225 | becaufe        | _          |       2 |\n",
      "| 226 | side           | _          |       2 |\n",
      "| 227 | beach          | _          |       2 |\n",
      "| 228 | tourmaline     | _          |       2 |\n",
      "| 229 | crystal        | _          |       2 |\n",
      "| 230 | trainer        | _          |       2 |\n",
      "| 231 | crime          | _          |       2 |\n",
      "| 232 | site           | _          |       2 |\n",
      "| 233 | sign           | _          |       2 |\n",
      "| 234 | tray           | _          |       2 |\n",
      "| 235 | treatment      | _          |       2 |\n",
      "| 236 | manufacturer   | _          |       2 |\n",
      "| 237 | top            | _          |       2 |\n",
      "| 238 | skull          | _          |       2 |\n",
      "| 239 | pregnancy      | _          |       1 |\n",
      "| 240 | possibility    | _          |       1 |\n",
      "| 241 | patina         | _          |       1 |\n",
      "| 242 | post           | _          |       1 |\n",
      "| 243 | photo          | _          |       1 |\n",
      "| 244 | presentation   | _          |       1 |\n",
      "| 245 | picture        | _          |       1 |\n",
      "| 246 | polish         | _          |       1 |\n",
      "| 247 | organism       | _          |       1 |\n",
      "| 248 | prison         | _          |       1 |\n",
      "| 249 | note           | _          |       1 |\n",
      "| 250 | painting       | _          |       1 |\n",
      "| 251 | nonprofit      | _          |       1 |\n",
      "| 252 | outfit         | _          |       1 |\n",
      "| 253 | news           | _          |       1 |\n",
      "| 254 | patch          | _          |       1 |\n",
      "| 255 | pairing        | _          |       1 |\n",
      "| 256 | offer          | _          |       1 |\n",
      "| 257 | organisation   | _          |       1 |\n",
      "| 258 | pipe           | _          |       1 |\n",
      "| 259 | plane          | _          |       1 |\n",
      "| 260 | phenomenon     | _          |       1 |\n",
      "| 261 | option         | _          |       1 |\n",
      "| 262 | personality    | _          |       1 |\n",
      "| 263 | physiognomy    | _          |       1 |\n",
      "| 264 | order          | _          |       1 |\n",
      "| 265 | period         | _          |       1 |\n",
      "| 266 | piano          | _          |       1 |\n",
      "| 267 | organ          | _          |       1 |\n",
      "| 268 | parent         | _          |       1 |\n",
      "| 269 | package        | _          |       1 |\n",
      "| 270 | opinion        | _          |       1 |\n",
      "| 271 | perception     | _          |       1 |\n",
      "| 272 | place          | _          |       1 |\n",
      "| 273 | operation      | _          |       1 |\n",
      "| 274 | opal           | _          |       1 |\n",
      "| 275 | offseason      | _          |       1 |\n",
      "| 276 | pizza          | _          |       1 |\n",
      "| 277 | partnership    | _          |       1 |\n",
      "| 278 | pendant        | _          |       1 |\n",
      "| 279 | pass           | _          |       1 |\n",
      "| 280 | parish         | _          |       1 |\n",
      "| 281 | shirt          | _          |       1 |\n",
      "| 282 | professor      | _          |       1 |\n",
      "| 283 | vampire        | _          |       1 |\n",
      "| 284 | unit           | _          |       1 |\n",
      "| 285 | type           | _          |       1 |\n",
      "| 286 | tumor          | _          |       1 |\n",
      "| 287 | tube           | _          |       1 |\n",
      "| 288 | trousers       | _          |       1 |\n",
      "| 289 | tribe          | _          |       1 |\n",
      "| 290 | translation    | _          |       1 |\n",
      "| 291 | transaction    | _          |       1 |\n",
      "| 292 | trade          | _          |       1 |\n",
      "| 293 | toy            | _          |       1 |\n",
      "| 294 | tool           | _          |       1 |\n",
      "| 295 | title          | _          |       1 |\n",
      "| 296 | timber         | _          |       1 |\n",
      "| 297 | tiger          | _          |       1 |\n",
      "| 298 | tidepool       | _          |       1 |\n",
      "| 299 | they           | _          |       1 |\n",
      "| 300 | textile        | _          |       1 |\n",
      "| 301 | tent           | _          |       1 |\n",
      "| 302 | tell           | _          |       1 |\n",
      "| 303 | team           | _          |       1 |\n",
      "| 304 | tavern         | _          |       1 |\n",
      "| 305 | tallitot       | _          |       1 |\n",
      "| 306 | tail           | _          |       1 |\n",
      "| 307 | urn            | _          |       1 |\n",
      "| 308 | variety        | _          |       1 |\n",
      "| 309 | prosecution    | _          |       1 |\n",
      "| 310 | vessel         | _          |       1 |\n",
      "| 311 | ys             | _          |       1 |\n",
      "| 312 | year           | _          |       1 |\n",
      "| 313 | yard           | _          |       1 |\n",
      "| 314 | yacht          | _          |       1 |\n",
      "| 315 | writer         | _          |       1 |\n",
      "| 316 | workout        | _          |       1 |\n",
      "| 317 | worker         | _          |       1 |\n",
      "| 318 | wonder         | _          |       1 |\n",
      "| 319 | with           | _          |       1 |\n",
      "| 320 | witch          | _          |       1 |\n",
      "| 321 | wedding        | _          |       1 |\n",
      "| 322 | website        | _          |       1 |\n",
      "| 323 | web            | _          |       1 |\n",
      "| 324 | weapon         | _          |       1 |\n",
      "| 325 | way            | _          |       1 |\n",
      "| 326 | wave           | _          |       1 |\n",
      "| 327 | watershed      | _          |       1 |\n",
      "| 328 | want           | _          |       1 |\n",
      "| 329 | wallet         | _          |       1 |\n",
      "| 330 | volcano        | _          |       1 |\n",
      "| 331 | voice          | _          |       1 |\n",
      "| 332 | vision         | _          |       1 |\n",
      "| 333 | video          | _          |       1 |\n",
      "| 334 | syllable       | _          |       1 |\n",
      "| 335 | swing          | _          |       1 |\n",
      "| 336 | surface        | _          |       1 |\n",
      "| 337 | sure           | _          |       1 |\n",
      "| 338 | search         | _          |       1 |\n",
      "| 339 | scholar        | _          |       1 |\n",
      "| 340 | scenario       | _          |       1 |\n",
      "| 341 | scarve         | _          |       1 |\n",
      "| 342 | saddler        | _          |       1 |\n",
      "| 343 | run            | _          |       1 |\n",
      "| 344 | round          | _          |       1 |\n",
      "| 345 | romance        | _          |       1 |\n",
      "| 346 | road           | _          |       1 |\n",
      "| 347 | retailer       | _          |       1 |\n",
      "| 348 | response       | _          |       1 |\n",
      "| 349 | regime         | _          |       1 |\n",
      "| 350 | record         | _          |       1 |\n",
      "| 351 | recipe         | _          |       1 |\n",
      "| 352 | reception      | _          |       1 |\n",
      "| 353 | quill          | _          |       1 |\n",
      "| 354 | question       | _          |       1 |\n",
      "| 355 | puzzle         | _          |       1 |\n",
      "| 356 | puppy          | _          |       1 |\n",
      "| 357 | puberty        | _          |       1 |\n",
      "| 358 | psalm          | _          |       1 |\n",
      "| 359 | provider       | _          |       1 |\n",
      "| 360 | prosecutor     | _          |       1 |\n",
      "| 361 | seat           | _          |       1 |\n",
      "| 362 | seed           | _          |       1 |\n",
      "| 363 | segment        | _          |       1 |\n",
      "| 364 | stab           | _          |       1 |\n",
      "| 365 | support        | _          |       1 |\n",
      "| 366 | sunset         | _          |       1 |\n",
      "| 367 | sunscreen      | _          |       1 |\n",
      "| 368 | struggle       | _          |       1 |\n",
      "| 369 | stroke         | _          |       1 |\n",
      "| 370 | stereotype     | _          |       1 |\n",
      "| 371 | stave          | _          |       1 |\n",
      "| 372 | star           | _          |       1 |\n",
      "| 373 | standoff       | _          |       1 |\n",
      "| 374 | stadium        | _          |       1 |\n",
      "| 375 | spruce         | _          |       1 |\n",
      "| 376 | setting        | _          |       1 |\n",
      "| 377 | spoon          | _          |       1 |\n",
      "| 378 | song           | _          |       1 |\n",
      "| 379 | smuggler       | _          |       1 |\n",
      "| 380 | smile          | _          |       1 |\n",
      "| 381 | sleeper        | _          |       1 |\n",
      "| 382 | slate          | _          |       1 |\n",
      "| 383 | sink           | _          |       1 |\n",
      "| 384 | shutter        | _          |       1 |\n",
      "| 385 | shop           | _          |       1 |\n",
      "| 386 | she            | _          |       1 |\n",
      "| 387 | need           | _          |       1 |\n",
      "| 388 | 135            | _          |       1 |\n",
      "| 389 | nebulae        | _          |       1 |\n",
      "| 390 | captain        | _          |       1 |\n",
      "| 391 | club           | _          |       1 |\n",
      "| 392 | cloud          | _          |       1 |\n",
      "| 393 | clinic         | _          |       1 |\n",
      "| 394 | claim          | _          |       1 |\n",
      "| 395 | church         | _          |       1 |\n",
      "| 396 | charter        | _          |       1 |\n",
      "| 397 | charm          | _          |       1 |\n",
      "| 398 | character      | _          |       1 |\n",
      "| 399 | change         | _          |       1 |\n",
      "| 400 | chandelier     | _          |       1 |\n",
      "| 401 | chance         | _          |       1 |\n",
      "| 402 | cell           | _          |       1 |\n",
      "| 403 | cat            | _          |       1 |\n",
      "| 404 | candle         | _          |       1 |\n",
      "| 405 | culture        | _          |       1 |\n",
      "| 406 | cake           | _          |       1 |\n",
      "| 407 | cafe           | _          |       1 |\n",
      "| 408 | cabochon       | _          |       1 |\n",
      "| 409 | cabin          | _          |       1 |\n",
      "| 410 | button         | _          |       1 |\n",
      "| 411 | businesspeople | _          |       1 |\n",
      "| 412 | bride          | _          |       1 |\n",
      "| 413 | briarpipe      | _          |       1 |\n",
      "| 414 | break          | _          |       1 |\n",
      "| 415 | brand          | _          |       1 |\n",
      "| 416 | bracket        | _          |       1 |\n",
      "| 417 | bowl           | _          |       1 |\n",
      "| 418 | bouquet        | _          |       1 |\n",
      "| 419 | co-worker      | _          |       1 |\n",
      "| 420 | coaster        | _          |       1 |\n",
      "| 421 | collar         | _          |       1 |\n",
      "| 422 | college        | _          |       1 |\n",
      "| 423 | crown          | _          |       1 |\n",
      "| 424 | criticize      | _          |       1 |\n",
      "| 425 | crest          | _          |       1 |\n",
      "| 426 | creature       | _          |       1 |\n",
      "| 427 | creation       | _          |       1 |\n",
      "| 428 | create         | _          |       1 |\n",
      "| 429 | crash          | _          |       1 |\n",
      "| 430 | coxcomb        | _          |       1 |\n",
      "| 431 | cowhide        | _          |       1 |\n",
      "| 432 | cover          | _          |       1 |\n",
      "| 433 | course         | _          |       1 |\n",
      "| 434 | couple         | _          |       1 |\n",
      "| 435 | countertop     | _          |       1 |\n",
      "| 436 | cornflake      | _          |       1 |\n",
      "| 437 | cork           | _          |       1 |\n",
      "| 438 | copy           | _          |       1 |\n",
      "| 439 | cooler         | _          |       1 |\n",
      "| 440 | cooker         | _          |       1 |\n",
      "| 441 | cook           | _          |       1 |\n",
      "| 442 | conversation   | _          |       1 |\n",
      "| 443 | contest        | _          |       1 |\n",
      "| 444 | contender      | _          |       1 |\n",
      "| 445 | connection     | _          |       1 |\n",
      "| 446 | congregation   | _          |       1 |\n",
      "| 447 | condition      | _          |       1 |\n",
      "| 448 | concept        | _          |       1 |\n",
      "| 449 | component      | _          |       1 |\n",
      "| 450 | boudin         | _          |       1 |\n",
      "| 451 | bottle         | _          |       1 |\n",
      "| 452 | borrower       | _          |       1 |\n",
      "| 453 | album          | _          |       1 |\n",
      "| 454 | agent          | _          |       1 |\n",
      "| 455 | actor          | _          |       1 |\n",
      "| 456 | accessory      | _          |       1 |\n",
      "| 457 | _              | wish       |       1 |\n",
      "| 458 | _              | plan       |       1 |\n",
      "| 459 | _              | most       |       1 |\n",
      "| 460 | _              | choose     |       1 |\n",
      "| 461 | _              | advisable  |       1 |\n",
      "| 462 | Y              | _          |       1 |\n",
      "| 463 | Steamers       | _          |       1 |\n",
      "| 464 | Software       | _          |       1 |\n",
      "| 465 | Servals        | _          |       1 |\n",
      "| 466 | Scout          | _          |       1 |\n",
      "| 467 | Said           | _          |       1 |\n",
      "| 468 | RESPECT        | _          |       1 |\n",
      "| 469 | Originals      | _          |       1 |\n",
      "| 470 | Ninos          | _          |       1 |\n",
      "| 471 | NBC            | _          |       1 |\n",
      "| 472 | Michel         | _          |       1 |\n",
      "| 473 | Lee            | _          |       1 |\n",
      "| 474 | Jo             | _          |       1 |\n",
      "| 475 | Hathaway       | _          |       1 |\n",
      "| 476 | Donald         | _          |       1 |\n",
      "| 477 | Darryl         | _          |       1 |\n",
      "| 478 | Curls          | _          |       1 |\n",
      "| 479 | Animas         | _          |       1 |\n",
      "| 480 | 597            | _          |       1 |\n",
      "| 481 | airline        | _          |       1 |\n",
      "| 482 | alcoholic      | _          |       1 |\n",
      "| 483 | bluebird       | _          |       1 |\n",
      "| 484 | answer         | _          |       1 |\n",
      "| 485 | blaze          | _          |       1 |\n",
      "| 486 | birth          | _          |       1 |\n",
      "| 487 | belt           | _          |       1 |\n",
      "| 488 | beer           | _          |       1 |\n",
      "| 489 | bear           | _          |       1 |\n",
      "| 490 | battery        | _          |       1 |\n",
      "| 491 | base           | _          |       1 |\n",
      "| 492 | bank           | _          |       1 |\n",
      "| 493 | ballpark       | _          |       1 |\n",
      "| 494 | awakening      | _          |       1 |\n",
      "| 495 | autumn         | _          |       1 |\n",
      "| 496 | author         | _          |       1 |\n",
      "| 497 | audience       | _          |       1 |\n",
      "| 498 | atrocity       | _          |       1 |\n",
      "| 499 | atom           | _          |       1 |\n",
      "| 500 | atmosphere     | _          |       1 |\n",
      "| 501 | asteroid       | _          |       1 |\n",
      "| 502 | association    | _          |       1 |\n",
      "| 503 | aspect         | _          |       1 |\n",
      "| 504 | artifact       | _          |       1 |\n",
      "| 505 | area           | _          |       1 |\n",
      "| 506 | apron          | _          |       1 |\n",
      "| 507 | application    | _          |       1 |\n",
      "| 508 | appearance     | _          |       1 |\n",
      "| 509 | api            | _          |       1 |\n",
      "| 510 | apartment      | _          |       1 |\n",
      "| 511 | anything       | _          |       1 |\n",
      "| 512 | cufflink       | _          |       1 |\n",
      "| 513 | custom         | _          |       1 |\n",
      "| 514 | nde            | _          |       1 |\n",
      "| 515 | jar            | _          |       1 |\n",
      "| 516 | lamp           | _          |       1 |\n",
      "| 517 | lake           | _          |       1 |\n",
      "| 518 | kombucha       | _          |       1 |\n",
      "| 519 | koi            | _          |       1 |\n",
      "| 520 | know           | _          |       1 |\n",
      "| 521 | knob           | _          |       1 |\n",
      "| 522 | knife          | _          |       1 |\n",
      "| 523 | kit            | _          |       1 |\n",
      "| 524 | kippin         | _          |       1 |\n",
      "| 525 | key            | _          |       1 |\n",
      "| 526 | keeper         | _          |       1 |\n",
      "| 527 | jvm            | _          |       1 |\n",
      "| 528 | joint          | _          |       1 |\n",
      "| 529 | iteration      | _          |       1 |\n",
      "| 530 | cycle          | _          |       1 |\n",
      "| 531 | it             | _          |       1 |\n",
      "| 532 | introduction   | _          |       1 |\n",
      "| 533 | interrogation  | _          |       1 |\n",
      "| 534 | institution    | _          |       1 |\n",
      "| 535 | instance       | _          |       1 |\n",
      "| 536 | installation   | _          |       1 |\n",
      "| 537 | install        | _          |       1 |\n",
      "| 538 | insertion      | _          |       1 |\n",
      "| 539 | information    | _          |       1 |\n",
      "| 540 | inflection     | _          |       1 |\n",
      "| 541 | incident       | _          |       1 |\n",
      "| 542 | implementation | _          |       1 |\n",
      "| 543 | image          | _          |       1 |\n",
      "| 544 | language       | _          |       1 |\n",
      "| 545 | law            | _          |       1 |\n",
      "| 546 | list           | _          |       1 |\n",
      "| 547 | listening      | _          |       1 |\n",
      "| 548 | name           | _          |       1 |\n",
      "| 549 | municipality   | _          |       1 |\n",
      "| 550 | mug            | _          |       1 |\n",
      "| 551 | msp            | _          |       1 |\n",
      "| 552 | movie          | _          |       1 |\n",
      "| 553 | mouth          | _          |       1 |\n",
      "| 554 | mouse          | _          |       1 |\n",
      "| 555 | mount          | _          |       1 |\n",
      "| 556 | motorcycle     | _          |       1 |\n",
      "| 557 | motor          | _          |       1 |\n",
      "| 558 | money          | _          |       1 |\n",
      "| 559 | model          | _          |       1 |\n",
      "| 560 | minute         | _          |       1 |\n",
      "| 561 | minis          | _          |       1 |\n",
      "| 562 | method         | _          |       1 |\n",
      "| 563 | merger         | _          |       1 |\n",
      "| 564 | memory         | _          |       1 |\n",
      "| 565 | meet           | _          |       1 |\n",
      "| 566 | mass           | _          |       1 |\n",
      "| 567 | mask           | _          |       1 |\n",
      "| 568 | marriage       | _          |       1 |\n",
      "| 569 | market         | _          |       1 |\n",
      "| 570 | mantel         | _          |       1 |\n",
      "| 571 | manicure       | _          |       1 |\n",
      "| 572 | manager        | _          |       1 |\n",
      "| 573 | lockout        | _          |       1 |\n",
      "| 574 | location       | _          |       1 |\n",
      "| 575 | iguana         | _          |       1 |\n",
      "| 576 | hurricane      | _          |       1 |\n",
      "| 577 | hunter         | _          |       1 |\n",
      "| 578 | fest           | _          |       1 |\n",
      "| 579 | feature        | _          |       1 |\n",
      "| 580 | fawn           | _          |       1 |\n",
      "| 581 | farm           | _          |       1 |\n",
      "| 582 | fair           | _          |       1 |\n",
      "| 583 | facilitator    | _          |       1 |\n",
      "| 584 | facelift       | _          |       1 |\n",
      "| 585 | exorcism       | _          |       1 |\n",
      "| 586 | episode        | _          |       1 |\n",
      "| 587 | employer       | _          |       1 |\n",
      "| 588 | elephant       | _          |       1 |\n",
      "| 589 | election       | _          |       1 |\n",
      "| 590 | effect         | _          |       1 |\n",
      "| 591 | duis           | _          |       1 |\n",
      "| 592 | drone          | _          |       1 |\n",
      "| 593 | driveway       | _          |       1 |\n",
      "| 594 | dress          | _          |       1 |\n",
      "| 595 | dream          | _          |       1 |\n",
      "| 596 | donor          | _          |       1 |\n",
      "| 597 | doll           | _          |       1 |\n",
      "| 598 | dice           | _          |       1 |\n",
      "| 599 | diagnose       | _          |       1 |\n",
      "| 600 | development    | _          |       1 |\n",
      "| 601 | designer       | _          |       1 |\n",
      "| 602 | designation    | _          |       1 |\n",
      "| 603 | description    | _          |       1 |\n",
      "| 604 | department     | _          |       1 |\n",
      "| 605 | defendant      | _          |       1 |\n",
      "| 606 | feeling        | _          |       1 |\n",
      "| 607 | find           | _          |       1 |\n",
      "| 608 | hundredth      | _          |       1 |\n",
      "| 609 | firm           | _          |       1 |\n",
      "| 610 | hotel          | _          |       1 |\n",
      "| 611 | horse          | _          |       1 |\n",
      "| 612 | horn           | _          |       1 |\n",
      "| 613 | honey          | _          |       1 |\n",
      "| 614 | hole           | _          |       1 |\n",
      "| 615 | hoarder        | _          |       1 |\n",
      "| 616 | helmet         | _          |       1 |\n",
      "| 617 | heart          | _          |       1 |\n",
      "| 618 | head           | _          |       1 |\n",
      "| 619 | hassle         | _          |       1 |\n",
      "| 620 | hand           | _          |       1 |\n",
      "| 621 | habitat        | _          |       1 |\n",
      "| 622 | grip           | _          |       1 |\n",
      "| 623 | golfer         | _          |       1 |\n",
      "| 624 | goaly          | _          |       1 |\n",
      "| 625 | goal           | _          |       1 |\n",
      "| 626 | glasses        | _          |       1 |\n",
      "| 627 | gemstone       | _          |       1 |\n",
      "| 628 | garden         | _          |       1 |\n",
      "| 629 | galls          | _          |       1 |\n",
      "| 630 | fursuit        | _          |       1 |\n",
      "| 631 | funnel         | _          |       1 |\n",
      "| 632 | fund           | _          |       1 |\n",
      "| 633 | framework      | _          |       1 |\n",
      "| 634 | foot           | _          |       1 |\n",
      "| 635 | flower         | _          |       1 |\n",
      "| 636 | floor          | _          |       1 |\n",
      "| 637 | journal        | _          |       1 |\n"
     ]
    }
   ],
   "source": [
    "print(show_counts(tdf, ['relay_lemma', 'nr_lemma']).reset_index().to_markdown())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚩 **zero overlap of defined relay and defined negraiser** so the patterns probably do not allow for this 🤔:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any((tdf.relay_lemma!='_') & (tdf.nr_lemma!='_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odf.loc[odf.hit_text=='not every purchase is exactly right', :].sample(1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>hit_text</th>\n",
       "      <th>text_window</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apw_eng_19950325_0192_28:3-4-6-7</th>\n",
       "      <td>scoped</td>\n",
       "      <td>no one is exactly sure</td>\n",
       "      <td>actually , no one is exactly sure who will take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19950809_0936_18:08-13-14</th>\n",
       "      <td>contig</td>\n",
       "      <td>no , that is n't exactly true</td>\n",
       "      <td>finding out , no , that is n't exactly true . ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19971119_0418_15:11-16-17</th>\n",
       "      <td>contig</td>\n",
       "      <td>No , I 'm not exactly sure</td>\n",
       "      <td>said , `` No , I 'm not exactly sure I 've made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_19990223_1352_7:07-08-10-11</th>\n",
       "      <td>scoped</td>\n",
       "      <td>no one was exactly sure</td>\n",
       "      <td>so far that no one was exactly sure where it l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apw_eng_20010328_0581_8:02-15-16</th>\n",
       "      <td>contig</td>\n",
       "      <td>no , the debut film from Mexican director Alej...</td>\n",
       "      <td>so no , the debut film from Mexican director A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_29_102.3735_x1638023_32:09-10-12-13</th>\n",
       "      <td>scoped</td>\n",
       "      <td>no two are exactly alike</td>\n",
       "      <td>from photos as no two are exactly alike .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_29_105.7814_x1693270_8:1-3-5-6</th>\n",
       "      <td>scoped</td>\n",
       "      <td>No two collars are exactly alike</td>\n",
       "      <td>No two collars are exactly alike !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_test_2.10082_x32416_01:19-21-23-24</th>\n",
       "      <td>scoped</td>\n",
       "      <td>no two lockouts are exactly alike</td>\n",
       "      <td>to remember that no two lockouts are exactly a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_2.05937_x25809_09:09-11-13-14</th>\n",
       "      <td>scoped</td>\n",
       "      <td>no 2 pieces are exactly alike</td>\n",
       "      <td>characteristics , and no 2 pieces are exactly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcc_eng_val_3.10294_x51165_07:15-16-18-19</th>\n",
       "      <td>scoped</td>\n",
       "      <td>no two are exactly alike</td>\n",
       "      <td>individually handmade , no two are exactly ali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2808 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            category  \\\n",
       "hit_id                                                 \n",
       "apw_eng_19950325_0192_28:3-4-6-7              scoped   \n",
       "apw_eng_19950809_0936_18:08-13-14             contig   \n",
       "apw_eng_19971119_0418_15:11-16-17             contig   \n",
       "apw_eng_19990223_1352_7:07-08-10-11           scoped   \n",
       "apw_eng_20010328_0581_8:02-15-16              contig   \n",
       "...                                              ...   \n",
       "pcc_eng_29_102.3735_x1638023_32:09-10-12-13   scoped   \n",
       "pcc_eng_29_105.7814_x1693270_8:1-3-5-6        scoped   \n",
       "pcc_eng_test_2.10082_x32416_01:19-21-23-24    scoped   \n",
       "pcc_eng_val_2.05937_x25809_09:09-11-13-14     scoped   \n",
       "pcc_eng_val_3.10294_x51165_07:15-16-18-19     scoped   \n",
       "\n",
       "                                                                                      hit_text  \\\n",
       "hit_id                                                                                           \n",
       "apw_eng_19950325_0192_28:3-4-6-7                                        no one is exactly sure   \n",
       "apw_eng_19950809_0936_18:08-13-14                                no , that is n't exactly true   \n",
       "apw_eng_19971119_0418_15:11-16-17                                   No , I 'm not exactly sure   \n",
       "apw_eng_19990223_1352_7:07-08-10-11                                    no one was exactly sure   \n",
       "apw_eng_20010328_0581_8:02-15-16             no , the debut film from Mexican director Alej...   \n",
       "...                                                                                        ...   \n",
       "pcc_eng_29_102.3735_x1638023_32:09-10-12-13                           no two are exactly alike   \n",
       "pcc_eng_29_105.7814_x1693270_8:1-3-5-6                        No two collars are exactly alike   \n",
       "pcc_eng_test_2.10082_x32416_01:19-21-23-24                   no two lockouts are exactly alike   \n",
       "pcc_eng_val_2.05937_x25809_09:09-11-13-14                        no 2 pieces are exactly alike   \n",
       "pcc_eng_val_3.10294_x51165_07:15-16-18-19                             no two are exactly alike   \n",
       "\n",
       "                                                                                   text_window  \n",
       "hit_id                                                                                          \n",
       "apw_eng_19950325_0192_28:3-4-6-7               actually , no one is exactly sure who will take  \n",
       "apw_eng_19950809_0936_18:08-13-14             finding out , no , that is n't exactly true . ''  \n",
       "apw_eng_19971119_0418_15:11-16-17              said , `` No , I 'm not exactly sure I 've made  \n",
       "apw_eng_19990223_1352_7:07-08-10-11          so far that no one was exactly sure where it l...  \n",
       "apw_eng_20010328_0581_8:02-15-16             so no , the debut film from Mexican director A...  \n",
       "...                                                                                        ...  \n",
       "pcc_eng_29_102.3735_x1638023_32:09-10-12-13          from photos as no two are exactly alike .  \n",
       "pcc_eng_29_105.7814_x1693270_8:1-3-5-6                      No two collars are exactly alike !  \n",
       "pcc_eng_test_2.10082_x32416_01:19-21-23-24   to remember that no two lockouts are exactly a...  \n",
       "pcc_eng_val_2.05937_x25809_09:09-11-13-14    characteristics , and no 2 pieces are exactly ...  \n",
       "pcc_eng_val_3.10294_x51165_07:15-16-18-19    individually handmade , no two are exactly ali...  \n",
       "\n",
       "[2808 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.loc[tdf.neg_lemma == 'no', ['category', 'hit_text', 'text_window']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-sanpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
